{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "# Parsing and Recombining Inputs\n",
    "\n",
    "In the chapter on [Grammars](Grammars.ipynb), we discussed how grammars can be\n",
    "used to represent various languages. We also saw how grammars can be used to\n",
    "generate strings of the corresponding language. Grammars can also perform the\n",
    "reverse. That is, given a string, one can decompose the string into its\n",
    "constituent parts that correspond to the parts grammar used to generate it\n",
    "-- the derivation tree of that string. These parts (and parts from other similar\n",
    "strings) can later be recombined using the same grammar to produce new strings.\n",
    "\n",
    "In this chapter, we use grammars to parse and decompose inputs to\n",
    "their corresponding derivation trees, allowing us to recombine them\n",
    "arbitrarily."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on grammars](Grammars.ipynb).\n",
    "* An understanding of derivation trees from the [chapter on grammar fuzzer](GrammarFuzzer.ipynb)\n",
    "  is also required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import fuzzingbook_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from Grammars import EXPR_GRAMMAR, START_SYMBOL, RE_NONTERMINAL, is_valid_grammar\n",
    "from Fuzzer import Fuzzer\n",
    "from GrammarFuzzer import GrammarFuzzer\n",
    "from ExpectError import ExpectError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We define extended versions of `display_tree` and `all_terminals` here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "from graphviz import Digraph\n",
    "\n",
    "\n",
    "def annotated_symbol(s, a):\n",
    "    return re.sub(r'([^a-zA-Z0-9\" ])', r\"\\\\\\1\", s) + (\" (%s)\" % a if a else '')\n",
    "\n",
    "\n",
    "def annotated_node(node, id):\n",
    "    symbol, children, *annotation = node\n",
    "    return symbol, children, ''.join(str(a) for a in annotation)\n",
    "\n",
    "\n",
    "def display_tree(derivation_tree,\n",
    "                           annotated_node=annotated_node,\n",
    "                           annotate=annotated_symbol):\n",
    "    counter = 0\n",
    "\n",
    "    def traverse_tree(dot, tree, id=0):\n",
    "        (symbol, children, annotation) = annotated_node(tree, id)\n",
    "        dot.node(repr(id), annotate(symbol, annotation))\n",
    "\n",
    "        if children is not None:\n",
    "            for child in children:\n",
    "                nonlocal counter\n",
    "                counter += 1\n",
    "                child_id = counter\n",
    "                dot.edge(repr(id), repr(child_id))\n",
    "                traverse_tree(dot, child, child_id)\n",
    "\n",
    "    dot = Digraph(comment=\"Derivation Tree\")\n",
    "    dot.attr('node', shape='plain')\n",
    "    traverse_tree(dot, derivation_tree)\n",
    "    display(dot)\n",
    "    \n",
    "def all_terminals(tree):\n",
    "    symbol, children, *_ = tree\n",
    "    return ''.join(all_terminals(c) for c in children) if children else symbol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "In order to parse a string, one needs to identify the language, and the\n",
    "corresponding grammar. For example, here is a string that we would like to parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mystring = '1+2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "This string is an arithmetic expression for addition, which may be specified using a grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Grammars\n",
    "\n",
    "A grammar, as you have read from the [chapter on grammars](Grammars.ipynb) is a set of rules the explain how the start symbol can be expanded, and each rule has a name, also called a nonterminal, and a set of alternative choices in how it can be expanded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A1_GRAMMAR = {\n",
    "    \"<start>\": [\"<expr>\"],\n",
    "    \"<expr>\": [\"<expr>+<expr>\", \"<expr>-<expr>\", \"<integer>\"],\n",
    "    \"<integer>\": [\"<digit><integer>\", \"<digit>\"],\n",
    "    \"<digit>\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "In the above expression, the rule `<expr> : <expr>+<expr>|<expr>-<expr>|<int>` corresponds to how the nonterminal `<expr>` might be expanded. We expression `<expr>+<expr>` corresponds to one of the alternative choices. We call this an _alternative_ expansion for the nonterminal `<expr>`. Finally, in an expression `<expr>+<expr>`, each of `<expr>`, `+`, and `<expr>` are _symbols_ in that expansion. A symbol could be either a nonterminal or a terminal symbol based on whether its expansion is available in the grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The _derivation tree_ for our expression from this grammar is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "tree = ('<start>', [('<expr>',\n",
    "                     [('<expr>', [('<integer>', [('<digit>', [('1', [])])])]),\n",
    "                      ('+', []),\n",
    "                      ('<expr>', [('<integer>', [('<digit>', [('2',\n",
    "                                                               [])])])])])])\n",
    "assert mystring == all_terminals(tree)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While a grammar can be used to specify a given language, there could be multiple\n",
    "grammars that correspond to the same language. For example, here is another \n",
    "grammar to describe the same addition expression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A2_GRAMMAR = {\n",
    "    \"<start>\": [\"<expr>\"],\n",
    "    \"<expr>\": [\"<integer><expr_>\"],\n",
    "    \"<expr_>\": [\"+<expr>\", \"-<expr>\", \"\"],\n",
    "    \"<integer>\": [\"<digit><integer_>\"],\n",
    "    \"<integer_>\": [\"<integer>\", \"\"],\n",
    "    \"<digit>\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The corresponding derivation tree is given by:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tree = ('<start>', [('<expr>', [('<integer>', [('<digit>', [('1', [])]),\n",
    "                                               ('<integer_>', [('', [])])]),\n",
    "                                ('<expr_>', [('+', []),\n",
    "                                             ('<expr>',\n",
    "                                              [('<integer>',\n",
    "                                                [('<digit>', [('2', [])]),\n",
    "                                                 ('<integer_>', [('', [])])]),\n",
    "                                               ('<expr_>', [('', [])])])])])])\n",
    "assert mystring == all_terminals(tree)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Indeed, there could be different classes of grammars that\n",
    "describe the same language. For example, the first grammar `A1_GRAMMAR`\n",
    "is a grammar that sports both _right_ and _left_ recursion, while the\n",
    "second grammar `A2_GRAMMAR` does not have left recursion in the\n",
    "nonterminals in any of its productions, but contains _epsilon_ productions.\n",
    "(An epsilon production is a production that has empty string in its right\n",
    "hand side.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Recursion\n",
    "\n",
    "A grammar is _left recursive_ if any of its nonterminals are left recursive,\n",
    "and a nonterminal is directly left-recursive if the left-most symbol of\n",
    "any of its productions is itself. It is indirectly left-recursive if any\n",
    "of the left-most symbols can be expanded using their definitions to\n",
    "produce the nonterminal as the left-most symbol of the expansion. The left\n",
    "recursion is called a _hidden-left-recursion_ if during the series of\n",
    "expansions of  a nonterminal, one reaches a rule where the rule contains\n",
    "the same nonterminal after a prefix of other symbols, and these symbols can\n",
    "dervive the empty string. For example, in `A1_GRAMMAR`, `<integer>` will be\n",
    "considered hidden-left recursive if `<digit>` could derive an empty string.\n",
    "\n",
    "Right recursive grammars are defined similarly.\n",
    "\n",
    "For example, in `A1_GRAMMAR`, the definition of `<expr>` is\n",
    "left-recursive, and right recursive directly. However in `A2_GRAMMAR`,\n",
    "`<expr>` is right recursive indirectly through the expansion of `<expr_>`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Ambiguity\n",
    "\n",
    "To complicate matters further, there could be\n",
    "multiple derivation trees -- also called _parses_ -- corresponding to the\n",
    "same string from the same grammar. For example, a string `1+2+3` can be parsed\n",
    "in two ways as we see below using the `A1_GRAMMAR`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mystring = '1+2+3'\n",
    "tree = ('<start>',\n",
    "        [('<expr>',\n",
    "          [('<expr>', [('<expr>', [('<integer>', [('<digit>', [('1', [])])])]),\n",
    "                       ('+', []),\n",
    "                       ('<expr>', [('<integer>',\n",
    "                                    [('<digit>', [('2', [])])])])]), ('+', []),\n",
    "           ('<expr>', [('<integer>', [('<digit>', [('3', [])])])])])])\n",
    "assert mystring == all_terminals(tree)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "tree = ('<start>',\n",
    "        [('<expr>', [('<expr>', [('<integer>', [('<digit>', [('1', [])])])]),\n",
    "                     ('+', []),\n",
    "                     ('<expr>',\n",
    "                      [('<expr>', [('<integer>', [('<digit>', [('2', [])])])]),\n",
    "                       ('+', []),\n",
    "                       ('<expr>', [('<integer>', [('<digit>', [('3',\n",
    "                                                                [])])])])])])])\n",
    "assert all_terminals(tree) == mystring\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Numerous parsing techniques exist that can parse a given string using a\n",
    "given grammar, and produce corresponding derivation tree or trees. However,\n",
    "some of these techniques work only on specific classes of grammars.\n",
    "These classes of grammars are named after the specific kind of parser\n",
    "that can accept grammars of that category. That is, the upper bound for\n",
    "the capabilities of the parser defines the grammar class named after that\n",
    "parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Different classes of grammars differ in the features that are available to\n",
    "the user for writing a grammar of that class. That is, the corresponding\n",
    "kind of parser will be unable to parse a grammar that makes use of more\n",
    "features than is allowed. For example, the `A2_GRAMMAR` is an *LL*\n",
    "grammar because it lacks left recursion, while `A1_GRAMMAR` is not an\n",
    "*LL* grammar. This is because an *LL* parser is a parser that parses\n",
    "its input from left to right, and constructs a leftmost derivation of its\n",
    "input by expanding the nonterminals it encounters. If there is a left\n",
    "recursion in one of these rules, an *LL* parser will enter an infinite loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Next, we develop different parsers. To do that, we define a minimal interface for parsing, that is obeyed by all parsers. There are two approaches to parsing a string using a grammar.\n",
    "\n",
    "The traditional approach is to use a *lexer* (also called a *tokenizer* or a *scanner*) to first tokenize the incoming string, and feed the grammar one token at a time. The lexer is typically a smaller parser that accepts a *regular language*. The advantage of this approach is that the grammar used by the parser can eschew the details of tokenization. Further, one gets a shallow derivation tree at the end of the parsing which can be directly used for generating the *Abstract Syntax Tree*.\n",
    "\n",
    "The second approach is to use a tree pruner after the complete parse. With this approach, one uses a grammar that has the complete details of the syntax. Next, the nodes corresponding to tokens are pruned and replaced with their corresponding strings as leaf nodes. The utility of this approach is that the parser is more powerful, and further there is no artificial distinction between *lexing* and *parsing*.\n",
    "\n",
    "We use the second approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(object):\n",
    "    def __init__(self, grammar, **kwargs):\n",
    "        self._grammar = grammar\n",
    "        self.start_symbol=kwargs.get('start_symbol') or START_SYMBOL \n",
    "        self.log = kwargs.get('log') or False\n",
    "        self.tokens = kwargs.get('tokens') or set()\n",
    "\n",
    "    def grammar(self):\n",
    "        return self._grammar\n",
    "\n",
    "    def parse_prefix(self, text):\n",
    "        \"\"\"Return pair (cursor, forest) for longest prefix of text\"\"\"\n",
    "        raise NotImplemented()\n",
    "\n",
    "    def parse(self, text):\n",
    "        cursor, forest = self.parse_prefix(text)\n",
    "        if cursor < len(text):\n",
    "            raise SyntaxError(\"at \" + repr(text[cursor:]))\n",
    "        return [self.prune_tree(tree) for tree in forest]\n",
    "\n",
    "    def prune_tree(self, tree):\n",
    "        name, children = tree\n",
    "        if name in self.tokens:\n",
    "            return (name, [(all_terminals(tree), [])])\n",
    "        else:\n",
    "            return (name, [self.prune_tree(c) for c in children]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Parsing Expression Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A _[Parsing Expression Grammar](http://bford.info/pub/lang/peg)_ (*PEG*)~\\cite{Ford2004} is a type of _recognition based formal grammar_ that specifies the sequence of steps to take to parse a given string.\n",
    "A _Parsing Expression Grammar_ is very similar to a _Context Free Grammar_. As in a _Context Free Grammar_, the grammar is represented by a set of nonterminals and corresponding alternatives representing how to match each. For example, here is a PEG that matches `a` or `b`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "PEG1 = {\n",
    "    '<start>': ['a','b']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "However, unlike the _CFG_, the alternatives represent **ordered choice**. That is, rather than choosing all rules that can potentially match, we stop at the first match that succeed. For example, the below _PEG_ can match `ab` but not `abc` unlike a _CFG_ which will match both. (We call the sequence of ordered choice expressions **choice expressions**  rather than alternatives to make the distinction from _CFG_ clear.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "PEG2 = {\n",
    "    '<start>': ['ab','abc']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Each choice in a _choice expression_ represents a rule on how to satisfy that particular choice. The choice is a sequence of symbols (terminals and nonterminals) that are matched against a given text as in a _CFG_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Beyond the syntax of grammar definitions we have seen so far, a _PEG_ can can also contain a few additional elements. See the exercises at the end of the chapter for additional information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The PEGs model the typical practice in handwritten recursive descent parsers, and hence it may be considered more intuitive to understand.\n",
    "\n",
    "We look at parsers for PEGs next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Packrat Parser for _PEGs_\n",
    "\n",
    "Short of hand rolling a parser, _Packrat_ parsing is one of the simplest parsing techniques, and is one of the techniques for parsing PEGs.\n",
    "The _Packrat_ parser is so named because it tries to cache all results from simpler problems in the hope that these solutions can be used to avoid re-computation later. We develop a minimal _Packrat_ parser next.\n",
    "\n",
    "But before that, we need to implement a few supporting tools."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from functools import reduce, lru_cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The  `EXPR_GRAMMAR` we import from the [chapter on grammars](Grammars.ipynb) is oriented towards generation. In particular, the production rules are stored as strings. We need to massage this representation a little to conform to a canonical representation where each token in a rule is represented separately. The `canonical` format uses separate tokens to represent each symbol in an expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def canonical(grammar, letters=False):\n",
    "    def split(rule):\n",
    "        return [token for token in re.split(RE_NONTERMINAL, rule) if token]\n",
    "\n",
    "    def tokenize(word):\n",
    "        return list(word) if letters else [word]\n",
    "\n",
    "    def canonical_expr(expression):\n",
    "        return [\n",
    "            token for word in split(expression)\n",
    "            for token in ([word] if word in grammar else tokenize(word))\n",
    "        ]\n",
    "\n",
    "    return {\n",
    "        k: [canonical_expr(expression) for expression in alternatives]\n",
    "        for k, alternatives in grammar.items()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "canonical(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We define a function to pretty print the grammars we define:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def pp_grammar(grammar):\n",
    "    def show_symbol(s):\n",
    "        return s if s in grammar else \"'%s'\" % s\n",
    "\n",
    "    def show_alternative(alt):\n",
    "        return \" + \".join(show_symbol(symbol) for symbol in alt)\n",
    "\n",
    "    def show_alternative_set(alts):\n",
    "        return \"\\n\\t| \".join([show_alternative(alt) for alt in alts])\n",
    "\n",
    "    for key in grammar:\n",
    "        print(\"%s = %s\" % (key, show_alternative_set(grammar[key])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pp_grammar(canonical(EXPR_GRAMMAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We update our parser class to store the `canonical` representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Parser(Parser):\n",
    "    def __init__(self, grammar, **kwargs):\n",
    "        self._grammar = grammar\n",
    "        self.start_symbol=kwargs.get('start_symbol') or START_SYMBOL \n",
    "        self.log = kwargs.get('log') or False\n",
    "        self.tokens = kwargs.get('tokens') or set()        \n",
    "        self.cgrammar = canonical(grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The Parser\n",
    "We derive from the `Parser` base class first, and we accept the text to be parsed in the `parse` method, which in turn calls `unify_key` with the `start_symbol`.\n",
    "\n",
    "__Note.__ While our PEG parser can produce only a single unambiguous parse tree, other parsers can produce multiple parses for ambiguous grammars. Hence, we return a list of trees (in this case with a single element)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PEGParser(Parser):\n",
    "    def parse_prefix(self, text):\n",
    "        cursor, tree = self.unify_key(self.start_symbol, text, 0)\n",
    "        return cursor, [tree]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Unify Key\n",
    "The `unify_key` algorithm is simple. If given a terminal symbol, it tries to match the symbol with the current position in the text. If the symbol and text match, it returns successfully with the new parse index `at`.\n",
    "\n",
    "If on the other hand, it was given a nonterminal, it retrieves the choice expression corresponding to the key, and tries to match each choice in order using `unify_rule`. If **any** of the rules succeed in being unified with the given text, the parse is considered a success, and we return with the new parse index returned by `unify_rule`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PEGParser(PEGParser):\n",
    "    def unify_key(self, key, text, at=0):\n",
    "        if key not in self.cgrammar:\n",
    "            if text[at:].startswith(key):\n",
    "                return at + len(key), (key, [])\n",
    "            else:\n",
    "                return at, None\n",
    "        for rule in self.cgrammar[key]:\n",
    "            to, res = self.unify_rule(rule, text, at)\n",
    "            if res:\n",
    "                return (to, (key, res))\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Unify Rule\n",
    "The `unify_rule` is similar. It retrieves the tokens corresponding to the rule that it needs to unify with the text, and calls `unify_key` on them in sequence. If **all** tokens are successfully unified with the text, the parse is a success."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PEGParser(PEGParser):\n",
    "    def unify_rule(self, rule, text, at):\n",
    "        results = []\n",
    "        for token in rule:\n",
    "            at, res = self.unify_key(token, text, at)\n",
    "            if res is None:\n",
    "                return at, None\n",
    "            results.append(res)\n",
    "        return at, results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The two methods are mutually recursive, and given that `unify_key` tries each alternative until it succeeds, `unify_key` can be called multiple times with the same arguments. Hence, it is import to memoize the results of `unify_key`. Python provides a simple decorator `lru_cache` for memoizing any function call that has hashable arguments. We add that to our implementation so that repeated calls to `unify_key` with same arguments get cached results.\n",
    "\n",
    "This memoization gives the algorithm its name -- _Packrat_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class PEGParser(PEGParser):\n",
    "    @lru_cache(maxsize=None)\n",
    "    def unify_key(self, key, text, at=0):\n",
    "        if key not in self.cgrammar:\n",
    "            if text[at:].startswith(key): return at + len(key), (key, [])\n",
    "            else: return at, None\n",
    "        for rule in self.cgrammar[key]:\n",
    "            to, res = self.unify_rule(rule, text, at)\n",
    "            if res: return (to, (key, res))\n",
    "        return 0, None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We wrap initialization and calling of `PEGParser` in a method `parse` that accepts the text to be parsed along with the grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here are a few examples of our parser in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mystring = \"1 + (2 * 3)\"\n",
    "peg = PEGParser(EXPR_GRAMMAR)\n",
    "for tree in peg.parse(mystring):\n",
    "    assert all_terminals(tree) == mystring\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mystring = \"1 * (2 + 3.35)\"\n",
    "for tree in peg.parse(mystring):\n",
    "    assert all_terminals(tree) == mystring\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "One should be aware that while the grammar looks like a _Context Free Grammar_, the language described by a PEG may be different. Indeed, only _LL(1)_ grammars are guaranteed to represent the same language for both PEGs and other parsers, while other behaviors could be surprising~\\cite{redziejowski2008}. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recombining Parsed Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Recombining parsed inputs was pioneered by _Langfuzz_~\\cite{Holler2012}. The basic idea is that program inputs often carry additional constraints beyond what is described by the syntax. For example, in Java, one needs to declare a variable --using s specific format for declaration -- before it can be used in an expression. This restriction is not captured in the _Java CFG_. Checking for type correctness is another example for additional restrictions carried by program definitions.\n",
    "\n",
    "When fuzzing compilers and interpreters, naive generation of programs using the language *CFG* often fails to achieve significant deeper coverage due to these kinds checks external to the grammar. Holler et al. suggests using pre-existing valid code fragments to get around these restrictions. The idea is that the pre-existing valid code fragments already conform to the restrictions external to the grammar, and can often provide a means to evade validity checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A Simple Fuzzer\n",
    "\n",
    "A simple idea is that one can treat the derivation tree of a preexisting program as the scaffolding, poke holes in it, and patch it with generated inputs from our grammar. Given below is a grammar for a language that allows definition and assignment of variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "VAR_GRAMMAR = {\n",
    "    '<start>': ['<statements>'],\n",
    "    '<statements>': ['<statement>;<statements>', '<statement>'],\n",
    "    '<statement>': ['<declaration>', '<assignment>'],\n",
    "    '<declaration>': ['<def> <identifier>'],\n",
    "    '<assignment>': ['<identifier>=<expr>'],\n",
    "    '<def>': ['def'],\n",
    "    '<identifier>': ['<word>'],\n",
    "    '<word>': ['<alpha><word>', '<alpha>'],\n",
    "    '<alpha>':\n",
    "    list(string.ascii_letters),\n",
    "    '<expr>': ['<term>+<expr>', '<term>-<expr>', '<term>'],\n",
    "    '<term>': ['<factor>*<term>', '<factor>/<term>', '<factor>'],\n",
    "    '<factor>':\n",
    "    ['+<factor>', '-<factor>', '(<expr>)', '<identifier>', '<number>'],\n",
    "    '<number>': ['<integer>.<integer>', '<integer>'],\n",
    "    '<integer>': ['<digit><integer>', '<digit>'],\n",
    "    '<digit>':\n",
    "    list(string.digits),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Let us use our new grammar to parse a program. But before that, our grammar is rather detailed, and we need to define our token nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "VAR_TOKENS = {'<def>', '<number>', '<identifier>'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mystring = 'def avar;def bvar;avar=1.3;bvar=avar-3*(4+300)'\n",
    "parser = PEGParser(VAR_GRAMMAR, tokens=VAR_TOKENS)\n",
    "for tree in parser.parse(mystring):\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We develop a `LangFuzzer` class that generates recombined inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To apply the _Langfuzz_ approach, we need a few parsed strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "mystrings = [\n",
    "    'def abc;abc=12+(3+3.3)',\n",
    "    'def a;def b;def c;a=1;b=2;c=a+b',\n",
    "    'def avar;def bvar;avar=1.3;bvar=avar-3*(4+300)',\n",
    "    'def a;def b;a=1.3;b=a-1*(4+3+(2/a))',\n",
    "    'def a;def b;def c;def d;a=10;b=20;c=34;d=-b+(b*b-4*a*c)/(2*a)',\n",
    "    'def x;def y;def z;x=10;y=20;z=(x+y)*(x-y)',\n",
    "    'def x;def y;def z;x=23;y=51;z=x*x-y*y',\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We recurse through any given tree, collecting parsed fragments corresponding to each nonterminal. Further, we also name each node so that we can address each node separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class LangFuzzer(Fuzzer):\n",
    "    def __init__(self, parser):\n",
    "        self.parser = parser\n",
    "        self.fragments = {k: [] for k in self.parser.cgrammar}\n",
    "\n",
    "    def traverse_tree(self, node):\n",
    "        counter = 1\n",
    "        nodes = {}\n",
    "\n",
    "        def helper(node, id):\n",
    "            nonlocal counter\n",
    "            name, children = node\n",
    "            new_children = []\n",
    "            nodes[id] = node\n",
    "            for child in children:\n",
    "                counter += 1\n",
    "                new_children.append(helper(child, counter))\n",
    "            return name, new_children, id\n",
    "\n",
    "        return helper(node, counter), nodes\n",
    "\n",
    "    def fragment(self, strings):\n",
    "        self.trees = []\n",
    "        for string in strings:\n",
    "            for tree in self.parser.parse(string):\n",
    "                tree, nodes = self.traverse_tree(tree)\n",
    "                self.trees.append((tree, nodes))\n",
    "                for node in nodes:\n",
    "                    symbol = nodes[node][0]\n",
    "                    if symbol in self.fragments:\n",
    "                        self.fragments[symbol].append(nodes[node])\n",
    "        return self.fragments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We thus obtain all valid fragments from our parsed strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lf = LangFuzzer(PEGParser(VAR_GRAMMAR, tokens=VAR_TOKENS))\n",
    "fragments = lf.fragment(mystrings)\n",
    "for key in fragments:\n",
    "    print(\"%s: %d\" % (key, len(fragments[key])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "All that remains is to actually find a place to poke a hole using `candidate`, and patch that hole using `generate_new_tree`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "class LangFuzzer(LangFuzzer):\n",
    "    def __init__(self, parser, strings):\n",
    "        self.parser = parser\n",
    "        self.fragments = {k: [] for k in self.parser.cgrammar}\n",
    "        self.fragment(strings)\n",
    "    \n",
    "    def generate_new_tree(self, node, choice):\n",
    "        name, children, id = node\n",
    "        if id == choice:\n",
    "            return random.choice(self.fragments[name])\n",
    "        else:\n",
    "            return (name, [self.generate_new_tree(c, choice) for c in children])\n",
    "        \n",
    "    def candidate(self):\n",
    "        tree, nodes = random.choice(self.trees)\n",
    "        interesting_nodes = [\n",
    "            n for n in nodes if nodes[n][0] in self.fragments\n",
    "            and len(self.fragments[nodes[n][0]]) > 1\n",
    "        ]\n",
    "        node = random.choice(interesting_nodes)\n",
    "        return tree, node\n",
    "    \n",
    "    def fuzz(self):\n",
    "        tree, node = self.candidate()\n",
    "        modified = self.generate_new_tree(tree, node)\n",
    "        return all_terminals(modified)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Here is our fuzzer in action."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "lf = LangFuzzer(PEGParser(VAR_GRAMMAR, tokens=VAR_TOKENS), mystrings)\n",
    "for i in range(100):\n",
    "    print(lf.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "###  Problems with PEG\n",
    "While _PEGs_ are simple at first sight, their behavior in some cases might be a bit unintuitive. For example, here is an example~\\cite{redziejowski}:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "PEG_SURPRISE = {\n",
    "    \"<A>\": [\"a<A>a\",\"aa\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "When interpreted as a context free grammar and used as a string generator, it will produce strings of the form `a, aa, aaaa, aaaaaa` that is, it produces strings where the number of `a` is $2*n$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "strings = []\n",
    "for e in range(4):\n",
    "    f = GrammarFuzzer(PEG_SURPRISE, '<A>')\n",
    "    tree = ('<A>', None)\n",
    "    for _ in range(e):\n",
    "        tree = f.expand_tree_once(tree)\n",
    "    tree = f.expand_tree_with_strategy(tree, f.expand_node_min_cost)\n",
    "    strings.append(all_terminals(tree))\n",
    "    display_tree(tree)\n",
    "strings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, the _PEG_ parser can only recognize strings of the form $2^n$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "peg = PEGParser(PEG_SURPRISE, start_symbol='<A>')\n",
    "for s in strings:\n",
    "    with ExpectError():\n",
    "        for tree in peg.parse(s):\n",
    "            display_tree(tree)\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We will see how we can overcome these problems with general purpose parsers next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Context Free Grammars"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "While the _Parsing Expression Grammars_ are expressive and the _packrat_ parser for parsing them is simple and intuitive, PEGs suffer from a major deficiency for our purposes. PEGs are oriented towards language recognition, and it is not clear how to translate an arbitrary PEG to a _CFG_. As we mentioned earlier, a naive re-interpretation of a PEG as a CFG does not work very well. Further, it is not clear what is the exact relation between the class of languages represented by _PEG_ and the the class of languages represented by _CFG_. Since our primary focus is _fuzzing_ -- that is _generation_ of strings, we next look at parsers that can accept _CFG_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `rules` method takes in a grammar and returns `(key,production)` pairs for each production in the grammar. This is an alternative representation of a grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def rules(grammar):\n",
    "    return [(key, choice)\n",
    "            for key, choices in grammar.items()\n",
    "            for choice in choices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We require an additional step to massage our `EXPR_GRAMMAR`. Some of the symbols in `EXPR_GRAMMAR` contains extraneous spaces. We need to remove these spaces from the grammar definition. While the spaces are reasonable when one is generating strings, they make our parses needlessly complicated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "EXPR_GRAMMAR_NS = {}\n",
    "for key, alts in canonical(EXPR_GRAMMAR).items():\n",
    "    EXPR_GRAMMAR_NS[key] = [''.join(sym.strip() for sym in alt) for alt in alts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "rules(EXPR_GRAMMAR_NS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "The `terminals` method extracts all terminal symbols from a `canonical` grammar representation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "def terminals(grammar):\n",
    "    return set(token\n",
    "               for key, choice in rules(grammar)\n",
    "               for token in choice if token not in grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "terminals(canonical(EXPR_GRAMMAR_NS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "pp_grammar(canonical(EXPR_GRAMMAR_NS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "There is a choice being made here. Traditionally, obtaining the derivation tree involved two phases: *lexing* and *parsing*. Lexing involves splitting of incoming input stream into tokens, and is often accomplished with a _Regular Grammar_ or a _Regular Expression_. However, a separate lexing stage is not a requirement because _CFGs_ can be easily extended with the _Regular Grammar_ one uses for lexing, and both lexing and parsing can be accomplished in one stage. If one is inclined to do so, it is easy to recover the derivation tree using tokens from the derivation tree using individual letters as we show later."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Nearley Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The Earley parser is a general parser that is able to parse arbitrary _Context Free Grammars_. It was invented by Jay Earley~\\cite{Earley1970} for use in computational linguistics. While its computational complexity is $O(n^3)$ for parsing strings with arbitrary grammars, it can parse strings with unambiguous grammars in $O(n^2)$ time, and all *[LR(k)](https://en.wikipedia.org/wiki/LR_parser)* grammars in linear time ($O(n)$~\\cite{Leo1991}). Further improvements -- notably handling epsilon rules -- were invented by Aycock et al.~\\cite{Aycock2002}.\n",
    "\n",
    "Note that one restriction of our implementation is that the start symbol should have only one choice in its choice expression. This is not a restriction in practice because any grammar with multiple choices in its start symbol can be extended with a new start symbol that has the original start symbol as its only choice. That is, given a grammar as below,\n",
    "\n",
    "```\n",
    "grammar = {\n",
    "    '<start>': ['<A>', '<B>'],\n",
    "    '<A>': ['a'],\n",
    "    '<B>': ['b'],    \n",
    "}\n",
    "```\n",
    "One may rewrite it as below to conform to the single choice rule.\n",
    "```\n",
    "grammar = {\n",
    "    '<start>': ['<start_>'],\n",
    "    '<start_>': ['<A>', '<B>'],\n",
    "    '<A>': ['a'],\n",
    "    '<B>': ['b'],    \n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We first implement a simpler parser that is nearly an _Earley_ parser, but not quite. In particular, our parser does not understand epsilon rules -- rules that derive empty string. Further, it sacrifices some performance for simplicity in that it stores redundant information to make retrieving derivation trees easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The parser uses dynamic programming to generate a table containing a forest of possible parses at each letter index -- the table contains as many columns as there are letters in the input, and each column contains different parsing rules at various stages of the parse.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Columns\n",
    "We define the `Column` first. The `Column` is initialized by its own `index` in the input string, and the `letter` at that index. Internally, we also keep track of the states that are added to the column as the parsing progresses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Column(object):\n",
    "    def __init__(self, index, letter):\n",
    "        self.index, self.letter = index, letter\n",
    "        self.states, self._unique = [], {}\n",
    "\n",
    "    def __str__(self):\n",
    "        return \"%s chart[%d]\\n%s\" % (self.letter, self.index, \"\\n\".join(\n",
    "            str(state) for state in self.states if state.finished()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The `Column` only stores unique `states`. Hence, when a new `state` is `added` to our `Column`, we check whether it is already known."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Column(Column):\n",
    "    def add(self, state):\n",
    "        if state in self._unique:\n",
    "            return self._unique[state]\n",
    "        self._unique[state] = state\n",
    "        self.states.append(state)\n",
    "        state.e_col = self\n",
    "        return self._unique[state]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### Item\n",
    "An item represents a parse in progress for a specific rule. Hence the item contains the name of the nonterminal, and the corresponding alternative expression (`expr`) which together form the rule, and the current position of parsing in this expression -- `dot`.\n",
    "\n",
    "\n",
    "**Note.** If you are familiar with [LR parsing](https://en.wikipedia.org/wiki/LR_parser), you will notice that an item is simply an `LR0` item."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Item(object):\n",
    "    def __init__(self, name, expr, dot):\n",
    "        self.name, self.expr, self.dot = name, expr, dot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We also provide a few convenience methods. The method `finished` checks if the `dot` has moved beyond the last element in `expr`. The method `advance` produces a new `Item` with the `dot` advanced one token, and represents an advance of the parsing. and `at_dot` returns the current symbol being parsed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class Item(Item):\n",
    "    def finished(self):\n",
    "        return self.dot >= len(self.expr)\n",
    "\n",
    "    def advance(self):\n",
    "        return Item(self.name, self.expr, self.dot + 1)\n",
    "\n",
    "    def at_dot(self):\n",
    "        return self.expr[self.dot] if self.dot < len(self.expr) else None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### States\n",
    "\n",
    "For `Earley` parsing, the state of the parsing is simply `Item` along with some meta information such as the starting `s_col`  and ending column `e_col` for each state. Hence we inherit from `Item` to create a `State`.\n",
    "We also store a references to all the child states (that is, all states that originates from nonterminals in the `expr`). Since we are interested in comparing states, we define `hash` and `eq` with the corresponding methods.\n",
    "\n",
    "Note that our hash and equality operators include comparison with `<state>.children`. This means that we store more states per column than strictly necessary by the canonical Earley algorithm -- The canonical Earley algorithm only stores states that differ by `name`, `expr` and `dot`. However, these also make retrieving the parse tree simpler as we will demonstrate later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class State(Item):\n",
    "    def __init__(self, name, expr, dot, s_col, children=[]):\n",
    "        super().__init__(name, expr, dot)\n",
    "        self.s_col, self.e_col, self.children = s_col, None, children[:]\n",
    "\n",
    "    def __str__(self):\n",
    "        return self.name + ':= ' + ' '.join([\n",
    "            str(p)\n",
    "            for p in [*self.expr[:self.dot], '|', *self.expr[self.dot:]]\n",
    "        ]) + \"(%d,%d)\" % (self.s_col.index, self.e_col.index)\n",
    "\n",
    "    def _t(self):\n",
    "        return (self.name, self.expr, self.dot, self.s_col.index,\n",
    "                tuple(self.children))\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self._t())\n",
    "\n",
    "    def __eq__(self, other):\n",
    "        return self._t() == other._t()\n",
    "\n",
    "    def advance(self):\n",
    "        return State(self.name, self.expr, self.dot + 1, self.s_col,\n",
    "                     self.children)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### The Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We use the following grammar in our examples below.\n",
    "```\n",
    "grammar = {\n",
    "    '<start>': ['<A><B>'],\n",
    "    '<A>': ['a<B>c', 'A<A>'],\n",
    "    '<B>': ['b<C>', '<D>'],\n",
    "    '<C>': ['c'],\n",
    "    '<D>': ['d']\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The _Earley_ algorithm starts by initializing the chart with columns (as many as there are letters in the input). We also seed the first column with a state representing the expression corresponding to the start symbol. In our case, the state corresponds to the start symbol with the `dot` at `0` is represented as below. The `|` symbol represents the paring status. In this case, we have not parsed anything.\n",
    "\n",
    "```\n",
    "<start>: | <A> <B>\n",
    "```\n",
    "We pass this partial chart to a method for filling the rest of the parse chart.\n",
    "\n",
    "We call it the `NearlyParser` because it is not yet an `Earley` parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NearleyParser(Parser):\n",
    "    def __init__(self, grammar, **kwargs):\n",
    "        super().__init__(grammar, **kwargs)\n",
    "        self.cgrammar = canonical(grammar, letters=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Before starting to parse, we seed the chart with the state representing the ongoing parse of the start symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NearleyParser(NearleyParser):\n",
    "    def chart_parse(self, words, start):\n",
    "        alt = tuple(*self.cgrammar[start])\n",
    "        chart = [Column(i, tok) for i, tok in enumerate([None, *words])]\n",
    "        chart[0].add(State(start, alt, 0, chart[0]))\n",
    "        return self.fill_chart(chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "The main loop has three fundamental operations. `predict`, `scan`, and `complete`. We discuss `predict` next."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Predict\n",
    "We have already seeded `chart[0]` with a state `[<A>,<B>]` with `dot` at `0`. Next, given that `<A>` is a nonterminal, we `predict` the possible parse continuations of this state. That is, it could be either `a <B> c` or `A <A>`.\n",
    "\n",
    "The general idea of `predict` is as follows: Say you have a state with name `<A>` from the above grammar, and expression containing `[a,<B>,c]`. Imagine that you are have seen `a` already, which means that the `dot` will be on `<B>`. Below, is a representation of our parse status. The left hand side of `|` represents the portion already parsed (`a`), and the right hand side represents the portion yet to be parsed (`<B> c`).\n",
    "\n",
    "```\n",
    "<A>: a | <B> c\n",
    "```\n",
    "\n",
    "To recognize `<B>`, we look at the definition of `<B>`, which has different alternative expressions. The `predict` step adds each of these alternatives to the set of states, with `dot` at `0`.\n",
    "\n",
    "```\n",
    "<A>: a | <B> c\n",
    "<B>: | b c\n",
    "<B>: | <D>\n",
    "```\n",
    "\n",
    "In essence, the `predict` method, when called with the current nonterminal, fetches the choices expressions corresponding to this nonterminal, and adds these as predicted _child_ states to the _current_ column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NearleyParser(NearleyParser):\n",
    "    def predict(self, col, sym):\n",
    "        for alt in self.cgrammar[sym]:\n",
    "            col.add(State(sym, tuple(alt), 0, col))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Scan\n",
    "What if rather than a nonterminal, the state contained a terminal symbol -- i.e a letter? In that case, we are ready to make some progress. For example, consider the second state above\n",
    "```\n",
    "<B>: | b c\n",
    "```\n",
    "We `scan` the next column's letter. Say the next token is `b`.\n",
    "If the letter matches what we have, then create a new state by advancing the current state by one letter.\n",
    "\n",
    "```\n",
    "<B>: b | c\n",
    "```\n",
    "This new state is added to the next column (i.e the column that has the matched letter)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NearleyParser(NearleyParser):\n",
    "    def scan(self, col, state, letter):\n",
    "        if letter == col.letter:\n",
    "            col.add(state.advance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Complete\n",
    "\n",
    "When we advance, what if we actually `complete` the processing of the current rule? If so, we want to update not just this state, but also all the _parent_ states from which this state was derived.\n",
    "For example, say we have states as below.\n",
    "```\n",
    "<A>: a | <B> c\n",
    "<B>: b c |\n",
    "```\n",
    "The state `<B>: b c |` is now complete. So, we need to advance `<A>: a | <B> c` one step forward.\n",
    "\n",
    "How do we determine the parent states? Note from `predict` that we added the predicted child states to the _same_ column as that of the inspected state. Hence, we look at the starting column of the current state, with the same symbol `at_dot` as that of the name of the completed state.\n",
    "\n",
    "For each such parent found, we advance that parent (because we have just finished parsing that non terminal for their `at_dot`) and add the new states to the current column. We also mark the current state as a child state for the advanced parent state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NearleyParser(NearleyParser):\n",
    "    def complete(self, col, state):\n",
    "        return self.nearley_complete(col, state)\n",
    "\n",
    "    def nearley_complete(self, col, state):\n",
    "        parent_states = [\n",
    "            st for st in state.s_col.states if st.at_dot() == state.name\n",
    "        ]\n",
    "        for st in parent_states:\n",
    "            col.add(st.advance()).children.append(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Fill chart\n",
    "The main driving loop in `fill_chart` essentially calls these operations in order. We loop over each column in order. For each column, fetch one state in the column at a time, and check if the state is `finished`. If it is, then we `complete` all the parent states depending on this state. If the state was not finished, we check to see if the state's current symbol `at_dot` is a nonterminal. If it is a nonterminal, we `predict` possible continuations, and update the current column with these states. If it was not, we `scan` the next column and advance the current state if it matches the next letter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NearleyParser(NearleyParser):\n",
    "    def fill_chart(self, chart):\n",
    "        for i, col in enumerate(chart):\n",
    "            for state in col.states:\n",
    "                if state.finished():\n",
    "                    self.complete(col, state)\n",
    "                else:\n",
    "                    sym = state.at_dot()\n",
    "                    if sym in self.cgrammar:\n",
    "                        self.predict(col, sym)\n",
    "                    else:\n",
    "                        if i + 1 >= len(chart):\n",
    "                            continue\n",
    "                        self.scan(chart[i + 1], state, sym)\n",
    "            if self.log:\n",
    "                print(col)\n",
    "        return chart"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "Since `chart_parse` returns the completed table, we now need to extract the derivation trees. Fortunately, this is simple given that we have kept the list of children for each state. All that we need to do is to fetch the finished states corresponding to the `start_symbol`, and retrieve the derivation trees for each child."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### Parse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class NearleyParser(NearleyParser):\n",
    "    def parse_prefix(self, text):\n",
    "        table = self.chart_parse(text, self.start_symbol)\n",
    "        for col in reversed(table):\n",
    "            states = [st for st in col.states if st.name == self.start_symbol]\n",
    "            if states:\n",
    "                return col.index, states\n",
    "        return -1, []\n",
    "\n",
    "    def parse(self, text):\n",
    "        cursor, states = self.parse_prefix(list(text))\n",
    "        if cursor != len(text):\n",
    "            return []\n",
    "        for state in states:\n",
    "            if state.finished():\n",
    "                yield self.derivation_tree(state)\n",
    "\n",
    "    def process_expr(self, expr, children):\n",
    "        terms = iter([(i, []) for i in expr if i not in self.cgrammar])\n",
    "        nts = iter([self.derivation_tree(i) for i in children])\n",
    "        return [next(terms if i not in self.cgrammar else nts) for i in expr]\n",
    "\n",
    "    def derivation_tree(self, state):\n",
    "        return (state.name, self.process_expr(state.expr, state.children))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mystring = '12*(2+2)/31'\n",
    "nearley = NearleyParser(EXPR_GRAMMAR_NS)\n",
    "for tree in nearley.parse(mystring):\n",
    "    assert mystring == all_terminals(tree)\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### Ambiguous parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Ambiguous grammars are grammars that can produce multiple derivation trees for some given string. For example, the grammar below can parse `1+2+3` in two different ways -- `[1+2]+3` and `1+[2+3]`. Both the parse trees can be retrieved using the `Earley` parser. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mystring = '1+2+3'\n",
    "nearley = NearleyParser(A1_GRAMMAR)\n",
    "for tree in nearley.parse(mystring):\n",
    "    assert mystring == all_terminals(tree)\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Earley Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "We mentioned previously that our implementation took certain liberties with how states are stored in the `NearleyParser`. We pay for that in the performance penalty incurred. What if we do not want to pay that penalty? We next modify our parser so that it is a true Earley parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "#### States\n",
    "The first modification is how states are compared. We no longer want to distinguish states with different children. Instead, we will obtain a parse forest at the end of the parse, from which derivation trees can be obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class State(State):\n",
    "    def __init__(self, name, expr, dot, s_col):\n",
    "        self.name, self.expr, self.dot = name, expr, dot\n",
    "        self.s_col, self.e_col = s_col, None\n",
    "\n",
    "    def _t(self):\n",
    "        return (self.name, self.expr, self.dot, self.s_col.index)\n",
    "\n",
    "    def advance(self):\n",
    "        return State(self.name, self.expr, self.dot + 1, self.s_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### The Parser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Similarly, we no longer track the children during `complete`.\n",
    "##### Complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EarleyParser(NearleyParser):\n",
    "    def complete(self, col, state):\n",
    "        return self.earley_complete(col, state)\n",
    "\n",
    "    def earley_complete(self, col, state):\n",
    "        parent_states = [\n",
    "            st for st in state.s_col.states if st.at_dot() == state.name\n",
    "        ]\n",
    "        for st in parent_states:\n",
    "            col.add(st.advance())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Parse\n",
    "\n",
    "The following is adapted from the excellent reference on Earley parsing by [Loup Vaillant](http://loup-vaillant.fr/tutorials/earley-parsing/).\n",
    "\n",
    "The new `parse` procedure has to construct the parsed forest. Our original chart is a table of states that contains production rules that end at that index. For easier parsing, we switch to a table that contains rules that begins at that index."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def reverse(self, table):\n",
    "        f_table = [Column(c.index, c.letter) for c in table]\n",
    "        for col in table:\n",
    "            finished = [s for s in col.states if s.finished()]\n",
    "            for s in finished:\n",
    "                f_table[s.s_col.index].states.append(s)\n",
    "        return f_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "For now, we return the first available derivation tree. To do that, we need to extract the parse forest from the state corresponding to `start`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def extract_trees(self, forest):\n",
    "        return [self.extract_a_tree(forest)]\n",
    "\n",
    "    def parse(self, text):\n",
    "        cursor, states = self.parse_prefix(text)\n",
    "        if cursor != len(text):\n",
    "            return []\n",
    "        table = self.chart_parse(text, self.start_symbol)\n",
    "        f_table = self.reverse(table)\n",
    "        start = next(s for s in states if s.finished())\n",
    "        return self.extract_trees(self.parse_forest(f_table, start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Parse Forests\n",
    "\n",
    "The `parse_forest` takes the state which represents the completed parse, and determines the possible ways that its expressions corresponded to the parsed expression. For example, say we are parsing `1+2+3`, and the state has `[<expr>,+,<expr>]` in `expr`. It could have been parsed either `[{<expr>:1+2},+,{<expr>:3}]` or `[{<expr>:1},+,{<expr>:2+3}]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def parse_forest(self, chart, state):\n",
    "        if not state.expr:\n",
    "            return (state.name, [])\n",
    "        pathexprs = self.parse_paths(state.expr, chart, state.s_col.index,\n",
    "                                     state.e_col.index)\n",
    "        paths_ = []\n",
    "        for pathexpr in pathexprs:\n",
    "            pathexpr_ = []\n",
    "            for varexpr in pathexpr:\n",
    "                completion = (self.parse_forest(chart, varexpr) if isinstance(\n",
    "                    varexpr, State) else (varexpr, []))\n",
    "                pathexpr_.append(completion)\n",
    "            paths_.append(pathexpr_)\n",
    "        return (state.name, paths_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### Parse Paths\n",
    "\n",
    "The parse paths tries to unify the given expression in `expr_` with the parsed string. For that, it extracts the first symbol in `expr_` and checks if it is a terminal symbol. If it is, then it checks the chart to see if the letter corresponding to the position matches the terminal symbol. If it does, advance our start index by the length of the symbol.\n",
    "\n",
    "If the symbol was a nonterminal symbol, then we retrieve the parsed states at the current start index that correspond to the nonterminal symbol, and collect the end index. These are the start indexes for the remaining expression.\n",
    "\n",
    "Given our list of start index, we obtain the parse paths from the remaining expression. If we can obtain any, then we return the parse paths. If not, we had an error. We return `None` in that case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def parse_paths(self, expr_, chart, frm, til):\n",
    "        var, *expr = expr_\n",
    "        starts = None\n",
    "        if var not in self.cgrammar:\n",
    "            starts = ([(var, frm + len(var))]\n",
    "                      if frm < til and chart[frm + 1].letter == var else [])\n",
    "        else:\n",
    "            starts = [(s, s.e_col.index) for s in chart[frm].states\n",
    "                      if s.name == var]\n",
    "\n",
    "        paths = []\n",
    "        for state, start in starts:\n",
    "            if not expr:\n",
    "                paths.extend([[state]] if start == til else [])\n",
    "            else:\n",
    "                res = self.parse_paths(expr, chart, start, til)\n",
    "                paths.extend([[state] + r for r in res])\n",
    "        return paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "##### extract_a_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "What we have from `parse_forest` is a forest of trees. We need to extract a single tree from that forest. That is accomplished as follows. (We adapt the solution given by [Loup Vaillant](http://loup-vaillant.fr/tutorials/earley-parsing/parser))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def extract_a_tree(self, forest_node):\n",
    "        name, paths = forest_node\n",
    "        if not paths:\n",
    "            return (name, [])\n",
    "        return (name, [self.extract_a_tree(p) for p in paths[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We now verify that our modifications work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "A3_GRAMMAR = {\n",
    "        \"<start>\":\n",
    "        [\"<expr>\"],\n",
    "        \"<expr>\":\n",
    "        [\"<expr>+<expr>\", \"<expr>-<expr>\",\"(<expr>)\", \"<integer>\"],\n",
    "        \"<integer>\":\n",
    "        [\"<digit><integer>\", \"<digit>\"],\n",
    "        \"<digit>\":\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mystring = '(1+24)-33'\n",
    "earley = EarleyParser(A3_GRAMMAR)\n",
    "for tree in earley.parse(mystring):\n",
    "    assert all_terminals(tree) == mystring\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "##### extract_trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Extracting a single tree might be reasonable for unambiguous parses. However, what if the given grammar produces amgiguity when given a string? We need to extract all derivation trees in that case. We enhance our `extract_trees` to extract multiple derivation trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def extract_trees(self, forest_node):\n",
    "        name, paths = forest_node\n",
    "        if not paths:\n",
    "            return [(name, [])]\n",
    "        results = []\n",
    "        for path in paths:\n",
    "            ptrees = zip(*[self.extract_trees(p) for p in path])\n",
    "            results.extend([(name, p) for p in ptrees])\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As before, we verify that everything works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "mystring = '12+23-34'\n",
    "earley = EarleyParser(A1_GRAMMAR)\n",
    "for tree in earley.parse(mystring):\n",
    "    assert mystring == all_terminals(tree)\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "#### The Aycock Epsilon fix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "While parsing, one often requires to know whether a given nonterminal can derive an empty string. For example, in the following grammar A can derive an empty string, while B can not. The nonterminals that can derive an empty string is called a _nullable_ nonterminal. For example, in the below grammar `E_GRARRMA_1`,  `<A>` is _nullable_, and since `<A>` is one of the alternatives of `<start>`, `<start>` is also _nullable_. But `<B>` is not _nullable_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "E_GRAMMAR_1 = {\n",
    "    '<start>': ['<A>','<B>'],\n",
    "    '<A>': ['a',''],\n",
    "    '<B>': ['b']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "One of the problems with the original Earley implementation is that it does not handle rules that can derive empty strings very well. For example, the given grammar should match `a`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "EPSILON = ''\n",
    "E_GRAMMAR = {\n",
    "        '<start>': ['<S>'],\n",
    "        '<S>': ['<A><A><A><A>'],\n",
    "        '<A>': ['a', '<E>'],\n",
    "        '<E>': [EPSILON]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mystring = 'a'\n",
    "earley = EarleyParser(E_GRAMMAR)\n",
    "trees = earley.parse(mystring)\n",
    "print(trees)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Aycock et al. suggests a simple fix. Their idea is to pre-compute the `nullable` set and use it to advance the `nullable` states. However, before we do that, we need to compute the `nullable` set. The `nullable` set consists of all nonterminals that can derive an empty string."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Computing the `nullable` set requires expanding each production rule in the grammar iteratively and inspecting whether a given rule can derive the empty string. Each iteration needs to take into account new terminals that have been found to be `nullable`. The procedure stops when we obtain a stable result. This procedure can be abstracted into a more general method `fixpoint`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Fixpoint\n",
    "\n",
    "A `fixpoint` of a function is an element in the function's domain such that it is mapped to itself. For example, 1 is a `fixpoint` of square root because `squareroot(1) == 1`.\n",
    "\n",
    "(We use `str` rather than `hash` to check for equality in `fixpoint` because the data structure `set`, which we would like to use as an argument has a good string representation but is not hashable)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def fixpoint(f):\n",
    "    def helper(arg):\n",
    "        while True:\n",
    "            sarg = str(arg)\n",
    "            arg_ = f(arg)\n",
    "            if str(arg_) == sarg:\n",
    "                return arg\n",
    "            arg = arg_\n",
    "\n",
    "    return helper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "Remember `my_sqrt` from the first chapter? We can define my_sqrt using fixpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def my_sqrt(x):\n",
    "    @fixpoint\n",
    "    def _my_sqrt(approx):\n",
    "        return (approx + x / approx) / 2\n",
    "\n",
    "    return _my_sqrt(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "my_sqrt(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Nullable\n",
    "\n",
    "Similarly, we can define `nullable` using `fixpoint`. We essentially provide the definition of a single intermediate step. That is, assuming that `nullables` contain the current `nullable` nonterminals, we iterate over the grammar looking for productions which are `nullable` -- that is, productions where the entire sequence can yield an empty string on some expansion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def nullable_expr(expr, nullables):\n",
    "    return all(token in nullables for token in expr)\n",
    "\n",
    "\n",
    "def nullable(grammar):\n",
    "    productions = rules(grammar)\n",
    "\n",
    "    @fixpoint\n",
    "    def nullable_(nullables):\n",
    "        for A, expr in productions:\n",
    "            if nullable_expr(expr, nullables):\n",
    "                nullables |= {A}\n",
    "        return (nullables)\n",
    "\n",
    "    return nullable_({EPSILON})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "for key, grammar in {\n",
    "        'E_GRAMMAR': E_GRAMMAR,\n",
    "        'E_GRAMMAR_1': E_GRAMMAR_1\n",
    "}.items():\n",
    "    print(key, nullable(canonical(grammar)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "So, once we have the `nullable` set, all that we need to do is, after we have called `predict` on a state corresponding to a nonterminal, check if it is `nullable` and if it is, advance and add the state to the current column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class EarleyParser(EarleyParser):\n",
    "    def fill_chart(self, chart):\n",
    "        epsilon = nullable(self.cgrammar)\n",
    "        for i, col in enumerate(chart):\n",
    "            for state in col.states:\n",
    "                if state.finished():\n",
    "                    self.complete(col, state)\n",
    "                else:\n",
    "                    sym = state.at_dot()\n",
    "                    if sym in self.cgrammar:\n",
    "                        self.predict(col, sym)\n",
    "                        if sym in epsilon:\n",
    "                            col.add(state.advance())\n",
    "                    else:\n",
    "                        if i + 1 >= len(chart):\n",
    "                            continue\n",
    "                        self.scan(chart[i + 1], state, sym)\n",
    "            if self.log:\n",
    "                print(col)\n",
    "        return chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "mystring = 'a'\n",
    "earley = EarleyParser(E_GRAMMAR)\n",
    "for tree in earley.parse(mystring):\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Further Information\n",
    "* Parsing with arbitrary context free grammars is reducible to boolean matrix multiplication~\\cite{Valiant1975} (and the reverse~\\cite{Lee2002}) which is at present bounded by $O(2^{23728639}$) ~\\cite{LeGall2014}\n",
    "* The actual class of languages that is expressible in PEG is currently unknown. In particular, we know that PEGs can express certain languages such as $a^n b^n c^n$. However, we do not know if there exist Context-Free languages that are not expressible with PEGs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* Grammars can be used to generate derivation trees for a given string.\n",
    "* Parsing Expression Grammars are intuitive, and easy to implement, but require care to write.\n",
    "* Earley Parsers can parse arbitrary Context Free Grammars.\n",
    "* How to generate a pool of fragments using the Langfuzz approach, and use it to generate nearly valid strings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "* [Use the automatic grammar miner for obtaining grammar](GrammarMiner.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 1 An alternative _Packrat_\n",
    "\n",
    "\n",
    "In the _Packrat_ parser, we showed how one could implement a simple _PEG_ parser. That parser kept track of the current location in the text using an index. Can you modify the parser so that it simply uses the current substring rather than tracking the index? That is, it should no longer have the `at` parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class PackratParser(Parser):\n",
    "    def parse_prefix(self, text):\n",
    "        txt, res = self.unify_key(self.start_symbol, text)\n",
    "        return len(txt), [res]\n",
    "\n",
    "    def parse(self, text):\n",
    "        remain, res = self.parse_prefix(text)\n",
    "        if remain:\n",
    "            raise SyntaxError(\"at \" + res)\n",
    "        return res\n",
    "\n",
    "    def unify_rule(self, rule, text):\n",
    "        results = []\n",
    "        for token in rule:\n",
    "            text, res = self.unify_key(token, text)\n",
    "            if res is None:\n",
    "                return text, None\n",
    "            results.append(res)\n",
    "        return text, results\n",
    "\n",
    "    def unify_key(self, key, text):\n",
    "        if key not in self.cgrammar:\n",
    "            if text.startswith(key):\n",
    "                return text[len(key):], (key, [])\n",
    "            else:\n",
    "                return text, None\n",
    "        for rule in self.cgrammar[key]:\n",
    "            text_, res = self.unify_rule(rule, text)\n",
    "            if res:\n",
    "                return (text_, (key, res))\n",
    "        return text, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "mystring = \"1+(2*3)\"\n",
    "for tree in PackratParser(EXPR_GRAMMAR_NS).parse(mystring):\n",
    "    assert all_terminals(tree) == mystring\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise 2 _More PEG Syntax_\n",
    "\n",
    "The _PEG_ syntax provides a few notational conveniences reminiscent of regular expressions. For example, it supports the following operators (letters `T` and `A` represents tokens that can be either terminal or nonterminal. `` is an empty string, and `/` is the ordered choice operator similar to the non-ordered choice operator `|`):\n",
    "\n",
    "* `T?` represents an optional greedy match of T and `A := T?` is equivalent to `A := T/`.\n",
    "* `T*` represents zero or more greedy matches of `T` and `A := T*` is equivalent to `A := T A/`.\n",
    "* `T+` represents one or more greedy matches -- equivalent to `TT*`\n",
    "\n",
    "If you look at the three notations above, each can be represented in the grammar in terms of basic syntax.\n",
    "Remember the exercise from [the chapter on grammars](Grammars.ipynb) that developed `define_ex_grammar` that can represent grammars as Python code? extend `define_ex_grammar` to `define_peg` to support the above notational conveniences. The decorator should rewrite a given grammar that contain these notations to an equivalent grammar in basic syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "heading_collapsed": true,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "solution": "hidden",
    "solution_first": true
   },
   "source": [
    "### Exercise 3 _PEG Predicates_\n",
    "\n",
    "Beyond these notational conveniences, it also supports two predicates that can provide a powerful lookahead facility that does not consume any input.\n",
    "\n",
    "* `T&A` represents an _And-predicate_ that matches `T` if `T` is matched, and it is immediately followed by `A`\n",
    "* `T!A` represents a _Not-predicate_ that matches `T` if `T` is matched, and it is *not* immediately followed by `A`\n",
    "\n",
    "Implement these predicates in our _PEG_ parser."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 4 _Earley Fill Chart_\n",
    "\n",
    "In the `Earley Parser`, `Column` class, we keep the states both as a `list` and also as a `dict` even though `dict` is ordered. Can you explain why?\n",
    "\n",
    "**Hint**: see the `fill_chart` method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Python allows us to append to a list in flight, while a dict, eventhough it is ordered does not allow that facility.\n",
    "\n",
    "That is, the following will work\n",
    "\n",
    "```python\n",
    "values = [1]\n",
    "for v in values:\n",
    "   values.append(v*2)\n",
    "```\n",
    "\n",
    "However, the following will result in an error\n",
    "```python\n",
    "values = {1:1}\n",
    "for v in values:\n",
    "    values[v*2] = v*2\n",
    "```\n",
    "\n",
    "In the `fill_chart`, we make use of this facility to modify the set of states we are iterating on, on the fly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise 5 _Leo Parser_\n",
    "\n",
    "One of the problems with the original Earley parser is that while it can parse strings using arbitrary _Context Free Gramamrs_, its performance on right-recursive grammars is quadratic. That is, it takes $O(n^2)$ runtime and space for parsing with right-recursive grammars. For example, consider these grammars:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "LR_GRAMMAR = {\n",
    "    '<start>': ['<A>'],\n",
    "    '<A>': ['<A>a', ''],\n",
    "}\n",
    "\n",
    "RR_GRAMMAR = {\n",
    "    '<start>': ['<A>'],\n",
    "    '<A>': ['a<A>', ''],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "We enable logging to see the parse progress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "result = EarleyParser(LR_GRAMMAR, log=True).parse('aaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "result = EarleyParser(RR_GRAMMAR, log=True).parse('aaaaaa')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "As can be seen from the parsing log for each letter, the number of states with representation `<A>:= a <A> | (i, j)` increases at each stage, and these are simply a left over from the previous letter. They do not contribute anything more to the parse other than to simply complete these entries. However, they take up space, and require resources for inspection, contributing a factor of `n` in analysis.\n",
    "\n",
    "Joop Leo~\\cite{Leo1991} found that this inefficiency can be avoided by detecting right recursion. The idea is that before starting the `completion` step, check whether the current item has a _deterministic reduction path_. If such a path exists, add a copy of the topmost element of the _deteministic reduction path_ to the current column, and return. If not, perform the original `completion` step.\n",
    "\n",
    "Finding a _deterministic reduction path_ is as follows:\n",
    "\n",
    "Given a complete state, represented by `<A> := ... | (i)` where `(i)` is the starting column, there is a _deterministic reduction path_ above it if two criteria are satisfied.\n",
    "* There exist an item `<B> := ... <A> | (k)` in the current column.\n",
    "* It should have a _single_ predecessor of the form `<B> := ... | <A> (k)` in the current column.\n",
    "Following this chain (looking for the link above `<B> := ... <A> | (k)`), the topmost item is the item that does not have a parent.\n",
    "\n",
    "See [Loup Vaillant](http://loup-vaillant.fr/tutorials/earley-parsing/right-recursion) for further information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class State(State):\n",
    "    def __init__(self, name, expr, dot, s_col, tag=None):\n",
    "        super().__init__(name, expr, dot, s_col)\n",
    "        self.tag = tag\n",
    "\n",
    "    def copy(self, tag=None):\n",
    "        return State(self.name, self.expr, self.dot, self.s_col, tag)\n",
    "\n",
    "    def __str__(self):\n",
    "        init = \"%s %s\" % (self.tag or \"   \", self.name)\n",
    "        return init + ':= ' + ' '.join([\n",
    "            str(p)\n",
    "            for p in [*self.expr[:self.dot], '|', *self.expr[self.dot:]]\n",
    "        ]) + \"(%d,%d)\" % (self.s_col.index, self.e_col.index)\n",
    "\n",
    "\n",
    "class LeoParser(EarleyParser):\n",
    "    def complete(self, col, state):\n",
    "        return self.leo_complete(col, state)\n",
    "\n",
    "    def leo_complete(self, col, state):\n",
    "        detred = self.deterministic_reduction(state)\n",
    "        if detred:\n",
    "            col.add(detred.copy())\n",
    "        else:\n",
    "            self.earley_complete(col, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "shown",
    "solution2_first": true
   },
   "source": [
    "Can you implement the `deterministic_reduction` method to obtain the top most element?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "def splitlst(predicate, iterable):\n",
    "    return reduce(lambda res, e: res[predicate(e)].append(e) or res, iterable,\n",
    "                  ([], []))\n",
    "\n",
    "\n",
    "class LeoParser(LeoParser):\n",
    "    def check_single_item(self, st, remain):\n",
    "        res = [\n",
    "            s for s in remain if s.name == st.name and s.expr == st.expr\n",
    "            and s.s_col.index == st.s_col.index and s.dot == (st.dot - 1)\n",
    "        ]\n",
    "        return len(res) == 1\n",
    "\n",
    "    @lru_cache(maxsize=None)\n",
    "    def get_above(self, state):\n",
    "        remain, finished = splitlst(lambda s: s.finished(), state.s_col.states)\n",
    "        res = [\n",
    "            st for st in finished\n",
    "            if len(st.expr) > 1 and state.name == st.expr[-1]\n",
    "        ]\n",
    "        vals = [st for st in res if self.check_single_item(st, remain)]\n",
    "        if vals:\n",
    "            assert len(vals) == 1\n",
    "            return vals[0]\n",
    "        return None\n",
    "\n",
    "    def deterministic_reduction(self, state):\n",
    "        st = state\n",
    "        while True:\n",
    "            _st = self.get_above(st)\n",
    "            if not _st:\n",
    "                break\n",
    "            st = _st\n",
    "        return st if st != state else None\n",
    "\n",
    "    def complete(self, col, state):\n",
    "        return self.leo_complete(col, state)\n",
    "\n",
    "    def leo_complete(self, col, state):\n",
    "        detred = self.deterministic_reduction(state)\n",
    "        if detred:\n",
    "            col.add(detred.copy(state.name))\n",
    "        else:\n",
    "            self.earley_complete(col, state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "shown"
   },
   "source": [
    "Now, both LR and RR grammars should work within $O(n)$ bounds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "result = LeoParser(RR_GRAMMAR, log=True).parse('aaaaaa')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "shown"
   },
   "outputs": [],
   "source": [
    "result = LeoParser(LR_GRAMMAR, log=True).parse('aaaaaa')\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "shown"
   },
   "source": [
    "__Advanced:__ We have fixed the complexity bounds. However, because we are saving only the top most item of a right recursion, we need to fix our `parse_forest` and `extract_a_tree` to be aware of our fix. Can you fix both?\n",
    "\n",
    "__Hint:__ When you start extracting the parse trees, any time you see a tagged state, look at its end point `e_col.index`. Until you reach this end point, you can carry along this state for each token you see, with the starting column suitably adjusted. Does your solution work for the following grammar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR_GRAMMAR = {\n",
    "    '<start>': ['<A>'],\n",
    "    '<A>': ['ab<A>', ''],\n",
    "}\n",
    "mystring = 'abababab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "shown"
   },
   "source": [
    "How about this one? (Does this obey the requirements?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RR_GRAMMAR = {\n",
    "    '<start>': ['<A>'],\n",
    "    '<A>': ['ab<B>', ''],\n",
    "    '<B>': ['<A>'],\n",
    "}\n",
    "mystring = 'abababab'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 6 _First set of a nonterminal_\n",
    "\n",
    "We previously gave a way to extract a the `nullable` (epsilon) set, which is often used for parsing.\n",
    "Along with `nullable`, parsing algorithms often use two other sets [`first` and `follow`](https://en.wikipedia.org/wiki/Canonical_LR_parser#FIRST_and_FOLLOW_sets).\n",
    "The first set of a terminal symbol is itself, and the first set of a nonterminal is composed of terminal symbols that can come at the beginning of any derivation\n",
    "of that nonterminal. The first set of any nonterminal that can derive the empty string should contain `EPSILON`. For example, using our `A1_GRAMMAR`, the first set of both `<expr>` and `<start>` is `{0,1,2,3,4,5,6,7,8,9}`. The extraction first set for any self-recursive nonterminal is simple enough. One simply has to recursively compute the first set of the first element of its choice expressions. The computation of `first` set for a self-recursive nonterminal is tricky. One has to recursively compute the first set until one is sure that no more terminals can be added to the first set.\n",
    "\n",
    "Can you implement the `first` set using our `fixpoint`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The first set of all terminals is the set containing just themselves. So we initialize that first. Then we update the first set with rules that derive empty strings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def firstset(grammar, nullable):\n",
    "    first = {i: {i} for i in terminals(grammar)}\n",
    "    for k in grammar:\n",
    "        first[k] = {EPSILON} if k in nullable else set()\n",
    "    return firstset_((rules(grammar), first, nullable))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Finally, we rely on the `fixpoint` to update the first set with the contents of the current first set until the first set stops changing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "def first_expr(expr, first, nullable):\n",
    "    tokens = set()\n",
    "    for token in expr:\n",
    "        tokens |= first[token]\n",
    "        if token not in nullable:\n",
    "            break\n",
    "    return tokens\n",
    "\n",
    "\n",
    "@fixpoint\n",
    "def firstset_(arg):\n",
    "    (rules, first, epsilon) = arg\n",
    "    for A, expression in rules:\n",
    "        first[A] |= first_expr(expression, first, epsilon)\n",
    "    return (rules, first, epsilon)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "firstset(canonical(EXPR_GRAMMAR_NS), EPSILON)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 7 _Follow set of a nonterminal_\n",
    "\n",
    "The follow set definition is similar to the first set. The follow set of a nonterminal is the set of terminals that can occur just after that nonterminal is used in any derivation. The follow set of the start symbol is `EOF`, and the follow set of any nonterminal is the super set of first sets of all symbols that come after it in any choice expression.\n",
    "\n",
    "For example, the follow set of `<expr>` in `A1_GRAMMAR` is the set `{EOF, +, -}`.\n",
    "\n",
    "As in the previous exercise, implement the `followset` using `fixpoint`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "The implementation of `followset` is similar to the `firstset`. We first initialize the follow set with `EOF`, get the epsilon and first sets, and use the `fixpoint` to iteratively compute the follow set until nothing changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "EOF = '\\0'\n",
    "\n",
    "\n",
    "def followset(grammar, start):\n",
    "    follow = {i: set() for i in grammar}\n",
    "    follow[start] = {EOF}\n",
    "\n",
    "    epsilon = nullable(grammar)\n",
    "    first = firstset(grammar, epsilon)\n",
    "    return followset_((grammar, epsilon, first, follow))[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Given the current follow set, one can update the follow set as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "@fixpoint\n",
    "def followset_(arg):\n",
    "    grammar, epsilon, first, follow = arg\n",
    "    for A, expression in rules(grammar):\n",
    "        f_B = follow[A]\n",
    "        for t in reversed(expression):\n",
    "            if t in grammar:\n",
    "                follow[t] |= f_B\n",
    "            f_B = f_B | first[t] if t in epsilon else (first[t] - {EPSILON})\n",
    "\n",
    "    return (grammar, epsilon, first, follow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "followset(canonical(A1_GRAMMAR), START_SYMBOL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "### Exercise 8 _LL(1) parse table_\n",
    "\n",
    "As we mentioned previously, there exist other kinds of parsers that operate left-to-right with right most derivation (*LR(k)*) or left-to-right with left most derivation (*LL(k)*) with _k_ signifying the amount of lookahead the parser is permitted to use.\n",
    "\n",
    "What should one do with the lookahead? That lookahead can be used to determine which rule to apply. In the case of an *LL(1)* parser, the rule to apply is determined by looking at the _first_ set of the different rules. We previously implemented `first_expr` that takes a an expression, the set of  `nullables`, and computes the first set of that rule.\n",
    "\n",
    "If a rule can derive an empty set, then that rule may also be applicable if of sees the `follow` set of the corresponding nonterminal.\n",
    "\n",
    "Can you implement the parse table that describes what action to take for an *LL(1)* parser on seeing a terminal symbol on lookahead? The table should be in the form of a dictionary such that the keys represent the nonterminal symbol, and the value should contain another dict with keys as terminal symbols and the particular rule to continue parsing as the value.\n",
    "\n",
    "The `parse_table` method populate a `self.table` data structure that should conform to the following requirements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "class LL1Parser(Parser):\n",
    "    def parse_table(self):\n",
    "        self.my_rules = rules(self.cgrammar)\n",
    "        # .. fill in here to produce\n",
    "        # self.table = ...\n",
    "    \n",
    "    def rules(self):\n",
    "        for i, rule in enumerate(self.my_rules):\n",
    "            print(i, rule)\n",
    "            \n",
    "    def show_table(self):\n",
    "        ts = list(sorted(terminals(self.cgrammar)))\n",
    "        print('Rule Name\\t| %s' % ' | '.join(t for t in ts))\n",
    "        for k in self.table:\n",
    "            pr = self.table[k]\n",
    "            actions = list(str(pr[t]) if t in pr else ' ' for t in ts)\n",
    "            print('%s  \\t| %s' % (k, ' | '.join(actions)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "On invocation of `LL1Parser(grammar).show_table()`\n",
    "It should result in the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "for i, r in enumerate(rules(canonical(A2_GRAMMAR))):\n",
    "    print(\"%d\\t %s := %s\" %(i, r[0], r[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "\n",
    "|Rule Name\t|| + | - | 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9|\n",
    "|-----------||---|---|---|---|---|---|---|---|---|---|---|--|\n",
    "|start  \t||   |   | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0|\n",
    "|expr  \t||   |   | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1 | 1|\n",
    "|expr_  \t|| 2 | 3 |   |   |   |   |   |   |   |   |   |  |\n",
    "|integer  \t||   |   | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5 | 5|\n",
    "|integer_  \t|| 7 | 7 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6 | 6|\n",
    "|digit  \t||   |   | 8 | 9 | 10 | 11 | 12 | 13 | 14 | 15 | 16 | 17|    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "We define `predict` as we explained before. Then use the predicted rules to populate the parse table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class LL1Parser(LL1Parser):\n",
    "    def predict(self, rulepair, first, follow, epsilon):\n",
    "        A, rule = rulepair\n",
    "        rf = first_expr(rule, first, epsilon)\n",
    "        if nullable_expr(rule, epsilon):\n",
    "            rf |= follow[A]\n",
    "        return rf\n",
    "\n",
    "    def parse_table(self):\n",
    "        self.my_rules = rules(self.cgrammar)\n",
    "        epsilon = nullable(self.cgrammar)\n",
    "        first = firstset(self.cgrammar, epsilon)\n",
    "        # inefficient, can combine the three.\n",
    "        follow = followset(self.cgrammar, self.start_symbol)\n",
    "\n",
    "        ptable = [(i, self.predict(rule, first, follow, epsilon))\n",
    "                  for i,rule in enumerate(self.my_rules)]\n",
    "\n",
    "        parse_tbl = {k: {} for k in self.cgrammar}\n",
    "\n",
    "        for i, pvals in ptable:\n",
    "            (k, expr) = self.my_rules[i]\n",
    "            parse_tbl[k].update({v: i for v in pvals})\n",
    "\n",
    "        self.table = parse_tbl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "ll1parser = LL1Parser(A2_GRAMMAR)\n",
    "ll1parser.parse_table()\n",
    "ll1parser.show_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 9 _LL(1) parser_\n",
    "\n",
    "Once we have the parse table, implementing the parser is as follows: Consider the first item from the sequence of tokens to parse, and seed the stack with the start symbol.\n",
    "\n",
    "While the stack is not empty, extract the first symbol from the stack, and if the symbol is a terminal, verify that the symbol matches the item from the input stream. If the symbol is a nonterminal, use the symbol and input item to lookup the next rule from the parse table. Insert the rule thus found to the top of the stack. Keep track of the expressions being parsed to build up the parse table.\n",
    "\n",
    "Use the parse table defined previously to implement the complete LL(1) parser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class LL1Parser(LL1Parser):\n",
    "    def parse_helper(self, stack, inplst):\n",
    "        inp, *inplst = inplst\n",
    "        exprs = []\n",
    "        while stack:\n",
    "            val, *stack = stack\n",
    "            if isinstance(val, tuple):\n",
    "                exprs.append(val)\n",
    "            elif val not in self.cgrammar:  # terminal\n",
    "                assert val == inp\n",
    "                exprs.append(val)\n",
    "                inp, *inplst = inplst or [None]\n",
    "            else:\n",
    "                if inp is not None:\n",
    "                    i = self.table[val][inp]\n",
    "                    _, rhs = self.my_rules[i]\n",
    "                    stack = rhs + [(val, len(rhs))] + stack\n",
    "        return self.linear_to_tree(exprs)\n",
    "\n",
    "    def parse(self, inp):\n",
    "        self.parse_table()\n",
    "        k, _ = self.my_rules[0]\n",
    "        stack = [k]\n",
    "        return self.parse_helper(stack, inp)\n",
    "\n",
    "    def linear_to_tree(self, arr):\n",
    "        stack = []\n",
    "        while arr:\n",
    "            elt = arr.pop(0)\n",
    "            if not isinstance(elt, tuple):\n",
    "                stack.append((elt, []))\n",
    "            else:\n",
    "                # get the last n\n",
    "                sym, n = elt\n",
    "                elts = stack[-n:] if n > 0 else []\n",
    "                stack = stack[0:len(stack) - n]\n",
    "                stack.append((sym, elts))\n",
    "        assert len(stack) == 1\n",
    "        return stack[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "ll1parser = LL1Parser(A2_GRAMMAR)\n",
    "tree = ll1parser.parse('1+2')\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 10 A different LangFuzzer\n",
    "\n",
    "Sometimes we do not want to use our pool of strings for various reasons -- the number of items in the pool may be inadequate, or not varied enough. Extend the `LangFuzzer` to use a separate function to check if the number of items in the pool corresponding to the selected non-terminal is large enough (say greater than 10), and if not, use the tree expansion technique from `GrammarFuzzer` to patch the hole.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden"
   },
   "source": [
    "Before we can make use of `GrammarFuzzer`, we need to change it a little bit. GrammarFuzzer relies on the grammar being in the `fuzzing` format with the expansions represented as strings. Our `LangFuzz` expects the expansions to be a list of tokens. So we fix the `GrammarFuzzer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class LangFuzzer2(LangFuzzer):\n",
    "    def __init__(self, parser, strings):\n",
    "        super().__init__(parser, strings)\n",
    "        self.gfuzz = GrammarFuzzer(parser.grammar())\n",
    "\n",
    "    def check_diversity(self, pool):\n",
    "        return len(pool) > 10\n",
    "\n",
    "    def candidate(self):\n",
    "        tree, nodes = random.choice(self.trees)\n",
    "        interesting_nodes = [\n",
    "            n for n in nodes if nodes[n][0] in self.fragments\n",
    "            and nodes[n][0] is not self.parser.start_symbol\n",
    "            and len(self.fragments[nodes[n][0]]) > 0\n",
    "        ]\n",
    "        node = random.choice(interesting_nodes)\n",
    "        return tree, node\n",
    "\n",
    "    def generate_new_tree(self, node, choice):\n",
    "        name, children, id = node\n",
    "        if id == choice:\n",
    "            pool = self.fragments[name]\n",
    "            if self.check_diversity(pool):\n",
    "                return random.choice(self.fragments[name])\n",
    "            else:\n",
    "                return None\n",
    "        else:\n",
    "            return (name,\n",
    "                    [self.generate_new_tree(c, choice) for c in children])\n",
    "\n",
    "    def fuzz(self):\n",
    "        tree, node = self.candidate()\n",
    "        tree_with_a_hole = self.generate_new_tree(tree, node)\n",
    "        modified = self.gfuzz.expand_tree(tree_with_a_hole)\n",
    "        return all_terminals(modified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "parser = EarleyParser(VAR_GRAMMAR, tokens=VAR_TOKENS)\n",
    "lf = LangFuzzer2(parser, mystrings)\n",
    "for i in range(100):\n",
    "    print(lf.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "With these changes, our `LangFuzzer2` is able to use the pool of fragments when necessary, but can rely on the grammar when the pool is not sufficient."
   ]
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
