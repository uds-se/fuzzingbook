{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "tags": [],
    "toc-hr-collapsed": false
   },
   "source": [
    "# Probabilistic Grammar Fuzzing\n",
    "\n",
    "Let us give grammars even more power by assigning _probabilities_ to individual expansions.  This allows us to control how many of each element should be produced, and thus allows us to _target_ our generated tests towards specific functionality.  We also show how to learn such probabilities from given sample inputs, and specifically direct our tests towards input features that are uncommon in these samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import YouTubeVideo\n",
    "YouTubeVideo('9htOliNwopc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You should have read the [chapter on grammars](Grammars.ipynb).\n",
    "* Our implementation hooks into the grammar-based fuzzer introduced in [\"Efficient Grammar Fuzzing\"](GrammarFuzzer.ipynb)\n",
    "* For learning probabilities from samples, we make use of [parsers](Parser.ipynb)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Synopsis\n",
    "<!-- Automatically generated. Do not edit. -->\n",
    "\n",
    "To [use the code provided in this chapter](Importing.ipynb), write\n",
    "\n",
    "```python\n",
    ">>> from fuzzingbook.ProbabilisticGrammarFuzzer import <identifier>\n",
    "```\n",
    "\n",
    "and then make use of the following features.\n",
    "\n",
    "\n",
    "A _probabilistic_ grammar allows attaching individual _probabilities_ to production rules.  To set the probability of an individual expansion `S` to the value `X` (between 0 and 1), replace it with a pair\n",
    "\n",
    "```python\n",
    "(S, opts(prob=X))\n",
    "```\n",
    "\n",
    "If we want to ensure that 90% of phone numbers generated have an area code starting with `9`, we can write:\n",
    "\n",
    "```python\n",
    ">>> from Grammars import US_PHONE_GRAMMAR, extend_grammar, opts\n",
    ">>> PROBABILISTIC_US_PHONE_GRAMMAR: Grammar = extend_grammar(US_PHONE_GRAMMAR,\n",
    ">>> {\n",
    ">>>       \"<lead-digit>\": [\n",
    ">>>                           \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\",\n",
    ">>>                           (\"9\", opts(prob=0.9))\n",
    ">>>                       ],\n",
    ">>> })\n",
    "```\n",
    "A `ProbabilisticGrammarFuzzer` will extract and interpret these options.  Here is an example:\n",
    "\n",
    "```python\n",
    ">>> probabilistic_us_phone_fuzzer = ProbabilisticGrammarFuzzer(PROBABILISTIC_US_PHONE_GRAMMAR)\n",
    ">>> [probabilistic_us_phone_fuzzer.fuzz() for i in range(5)]\n",
    "```\n",
    "As you can see, the large majority of area codes now starts with `9`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Law of Leading Digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In all our examples so far, you may have noted that inputs generated by a program differ quite a bit from \"natural\" inputs as they occur in real life.  This is true even for innocuous elements such as numbers â€“ yes, the numbers we have generated so far actually _differ_ from numbers in the real world.  This is because in real-life sets of numerical data, the _leading significant digit_ is likely to be small: Actually, on average, the leading digit `1` occurs more than _six times_ as often as the leading digit `8` or `9`.  It has been shown that this result applies to a wide variety of data sets, including electricity bills, street addresses, stock prices, house prices, population numbers, death rates, lengths of rivers, physical and mathematical constants (Wikipedia)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This law of leading digits was first observed by Newcomb \\cite{Newcomb1881} and later formalized by Benford in \\cite{Benford1938}.  Let us take a look at the conditions that determine the first digit of a number.  We can easily compute the first digit by converting the number into a string and take the first character:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_digit_via_string(x: int) -> int:\n",
    "    return ord(repr(x)[0]) - ord('0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_digit_via_string(2001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do this mathematically, though, we have to take the fractional part of their logarithm, or formally\n",
    "\n",
    "$$\n",
    "d = 10^{\\{\\log_{10}(x)\\}}\n",
    "$$\n",
    "\n",
    "where $\\{x\\}$ is the fractional part of $x$ (i.e. $\\{1.234\\} = 0.234$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def first_digit_via_log(x: int) -> int:\n",
    "    frac, whole = math.modf(math.log10(x))\n",
    "    return int(10 ** frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_digit_via_log(2001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most sets of \"naturally\" occurring numbers should not have any bias in the fractional parts of their logarithms, and hence, the fractional part $\\{\\log_{10}(x)\\}$ is typically uniformly distributed.  However, the fractional parts for the individual digits are _not_ evenly distributed.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a number to start with a digit $d$, the condition $d < 10^{\\{\\log_{10}(x)\\}} < d + 1$ must hold.  To start with the digit 1, the fractional part $\\{\\log_{10}(x)\\}$ must thus be in the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(math.log10(1), math.log10(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with the digit 2, though, it must be in the range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(math.log10(2), math.log10(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "which is much smaller.  Formally, the probability $P(d)$ for a leading digit $d$ (again, assuming uniformly distributed fractional parts) is known as Benford's law:\n",
    "$$\n",
    "P(d) = \\log_{10}(d + 1) - \\log_{10}(d)\n",
    "$$\n",
    "which gives us:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_leading_digit(d: int) -> float:\n",
    "    return math.log10(d + 1) - math.log10(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us compute these probabilities for all digits:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digit_probs = [prob_leading_digit(d) for d in range(1, 10)]\n",
    "[(d, \"%.2f\" % digit_probs[d - 1]) for d in range(1, 10)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "import matplotlib.pyplot as plt  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "labels = range(1, 10)\n",
    "fig1, ax1 = plt.subplots()\n",
    "ax1.pie(digit_probs, labels=labels, shadow=True, autopct='%1.1f%%',\n",
    "        counterclock=False, startangle=90)\n",
    "ax1.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that a leading 1 is indeed six times as probable as a leading 9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Benford's law has a number of applications.  Most notably, it can be used to detect \"non-natural\" numbers, i.e. numbers that apparently were created randomly rather than coming from a \"natural\" source.  if you write a scientific paper and fake data by putting in random numbers (for instance, [using our grammar fuzzer](GrammarFuzzer.ipynb) on integers), you will likely violate Benford's law, and this can indeed be spotted.  On the other hand, how would we proceed if we _wanted_ to create numbers that adhere to Benford's law?  To this end, we need to be able to _encode_ probabilities such as the above in our grammar, such that we can ensure that a leading digit is indeed a `1` in 30% of all cases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying Probabilities\n",
    "\n",
    "The goal of this chapter is to assign _probabilities_ to individual expansions in the grammar, such that we can express that some expansion alternatives should be favored over others.  This is not only useful to generate \"natural\"-looking numbers, but even more so to _direct_ test generation towards a specific goal.  If you recently have changed some code in your program, you would probably like to generate inputs that exercise precisely this code.  By raising the probabilities on the input elements associated with the changed code, you will get more tests that exercise the changed code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our concept for expressing probabilities is to _annotate_ individual expansions with attributes such as probabilities, using the annotation mechanism introduced in [the chapter on grammars](Grammars.ipynb).  To this end, we allow that an expansion cannot only be a string, but also a _pair_ of a string and a set of attributes, as in\n",
    "\n",
    "```python\n",
    "    \"<expr>\":\n",
    "        [(\"<term> + <expr>\", opts(prob=0.1)),\n",
    "         (\"<term> - <expr>\", opts(prob=0.2)),\n",
    "         \"<term>\"]\n",
    "```\n",
    "\n",
    "Here, the `opts()` function would allow us to express probabilities for choosing the individual expansions.  The addition would have a probability of 10%, the subtraction of 20%.  The remaining probability (in this case 70%) is equally distributed over the non-attributed expansions (in this case the single last one)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now use pairs with `opts()` to assign probabilities to our expression grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bookutils.setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Fuzzer import Fuzzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from GrammarFuzzer import GrammarFuzzer, all_terminals, display_tree, DerivationTree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "code_folding": [],
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "from Grammars import is_valid_grammar, EXPR_GRAMMAR, START_SYMBOL, crange\n",
    "from Grammars import opts, exp_string, exp_opt, set_opts\n",
    "from Grammars import Grammar, Expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Dict, Set, Optional, cast, Any, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "PROBABILISTIC_EXPR_GRAMMAR: Grammar = {\n",
    "    \"<start>\":\n",
    "        [\"<expr>\"],\n",
    "\n",
    "    \"<expr>\":\n",
    "        [(\"<term> + <expr>\", opts(prob=0.1)),\n",
    "         (\"<term> - <expr>\", opts(prob=0.2)),\n",
    "         \"<term>\"],\n",
    "\n",
    "    \"<term>\":\n",
    "        [(\"<factor> * <term>\", opts(prob=0.1)),\n",
    "         (\"<factor> / <term>\", opts(prob=0.1)),\n",
    "         \"<factor>\"\n",
    "         ],\n",
    "\n",
    "    \"<factor>\":\n",
    "        [\"+<factor>\", \"-<factor>\", \"(<expr>)\",\n",
    "            \"<leadinteger>\", \"<leadinteger>.<integer>\"],\n",
    "\n",
    "    \"<leadinteger>\":\n",
    "        [\"<leaddigit><integer>\", \"<leaddigit>\"],\n",
    "\n",
    "    # Benford's law: frequency distribution of leading digits\n",
    "    \"<leaddigit>\":\n",
    "        [(\"1\", opts(prob=0.301)),\n",
    "         (\"2\", opts(prob=0.176)),\n",
    "         (\"3\", opts(prob=0.125)),\n",
    "         (\"4\", opts(prob=0.097)),\n",
    "         (\"5\", opts(prob=0.079)),\n",
    "         (\"6\", opts(prob=0.067)),\n",
    "         (\"7\", opts(prob=0.058)),\n",
    "         (\"8\", opts(prob=0.051)),\n",
    "         (\"9\", opts(prob=0.046)),\n",
    "         ],\n",
    "\n",
    "    # Remaining digits are equally distributed\n",
    "    \"<integer>\":\n",
    "        [\"<digit><integer>\", \"<digit>\"],\n",
    "\n",
    "    \"<digit>\":\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PROBABILISTIC_EXPR_GRAMMAR, supported_opts={'prob'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how the grammar expansions are represented internally:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaddigits: List[Expansion] = PROBABILISTIC_EXPR_GRAMMAR[\"<leaddigit>\"]\n",
    "leaddigits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, we typically access the expansion string and the associated probability via designated helper functions, `exp_string()` (from the [chapter on Grammars](Grammars.ipynb)) and `exp_prob()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaddigit_expansion = leaddigits[0]\n",
    "leaddigit_expansion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_string(leaddigit_expansion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_prob(expansion: Expansion) -> float:\n",
    "    \"\"\"Return the options of an expansion\"\"\"\n",
    "    return exp_opt(expansion, 'prob')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_prob(leaddigit_expansion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our existing fuzzers are all set up to work with grammars annotated this way.  They simply ignore all annotations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = GrammarFuzzer(PROBABILISTIC_EXPR_GRAMMAR)\n",
    "f.fuzz()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GrammarCoverageFuzzer import GrammarCoverageFuzzer  # minor dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = GrammarCoverageFuzzer(PROBABILISTIC_EXPR_GRAMMAR)\n",
    "f.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Computing Probabilities\n",
    "\n",
    "Let us define functions that access probabilities for given expansions.  While doing so, they also check for inconsistencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributing Probabilities\n",
    "\n",
    "Here is how we distribute probabilities for expansions without specified probabilities. Given an expansion rule\n",
    "\n",
    "$$S ::= a_1\\:|\\: a_2 \\:|\\: \\dots \\:|\\: a_n \\:|\\: u_1 \\:|\\: u_2 \\:|\\: \\dots u_m$$\n",
    "\n",
    "with $n \\ge 0$ alternatives $a_i$ for which the probability $p(a_i)$ is _specified_ and\n",
    "$m \\ge 0$ alternatives $u_j$ for which the probability $p(u_j)$ is _unspecified_, \n",
    "the \"remaining\" probability is distributed equally over all $u_j$; in other words,\n",
    "\n",
    "$$p(u_j) = \\frac{1 - \\sum_{i = 1}^{n}p(a_i)}{m}$$\n",
    "\n",
    "If no probabilities are specified ($n = 0$), then all expansions have the same probability.\n",
    "\n",
    "The overall sum of probabilities must be 1:\n",
    "\n",
    "$$\\sum_{i = 1}^{n} p(a_i) + \\sum_{j = 1}^{m} p(u_i) = 1$$\n",
    "\n",
    "We check these properties while distributing probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `exp_probabilities()` returns a mapping of all expansions in a rule to their respective probabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exp_probabilities(expansions: List[Expansion],\n",
    "                      nonterminal: str =\"<symbol>\") \\\n",
    "        -> Dict[Expansion, float]:\n",
    "    probabilities = [exp_prob(expansion) for expansion in expansions]\n",
    "    prob_dist = prob_distribution(probabilities, nonterminal)  # type: ignore\n",
    "\n",
    "    prob_mapping: Dict[Expansion, float] = {}\n",
    "    for i in range(len(expansions)):\n",
    "        expansion = exp_string(expansions[i])\n",
    "        prob_mapping[expansion] = prob_dist[i]\n",
    "\n",
    "    return prob_mapping"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The gist of `exp_probabilities()` is handled in `prob_distribution()`, which does the actual checking and computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prob_distribution(probabilities: List[Optional[float]],\n",
    "                      nonterminal: str = \"<symbol>\"):\n",
    "    epsilon = 0.00001\n",
    "\n",
    "    number_of_unspecified_probabilities = probabilities.count(None)\n",
    "    if number_of_unspecified_probabilities == 0:\n",
    "        sum_probabilities = cast(float, sum(probabilities))\n",
    "        assert abs(sum_probabilities - 1.0) < epsilon, \\\n",
    "            nonterminal + \": sum of probabilities must be 1.0\"\n",
    "        return probabilities\n",
    "\n",
    "    sum_of_specified_probabilities = 0.0\n",
    "    for p in probabilities:\n",
    "        if p is not None:\n",
    "            sum_of_specified_probabilities += p\n",
    "    assert 0 <= sum_of_specified_probabilities <= 1.0, \\\n",
    "        nonterminal + \": sum of specified probabilities must be between 0.0 and 1.0\"\n",
    "\n",
    "    default_probability = ((1.0 - sum_of_specified_probabilities)\n",
    "                           / number_of_unspecified_probabilities)\n",
    "    all_probabilities = []\n",
    "    for p in probabilities:\n",
    "        if p is None:\n",
    "            p = default_probability\n",
    "        all_probabilities.append(p)\n",
    "\n",
    "    assert abs(sum(all_probabilities) - 1.0) < epsilon\n",
    "    return all_probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the mapping `exp_probabilities()` returns for the annotated `<leaddigit>` element:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_probabilities(PROBABILISTIC_EXPR_GRAMMAR[\"<leaddigit>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If no expansion is annotated, all expansions have the same likelihood of being selected, as in our previous grammar fuzzers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(exp_probabilities(PROBABILISTIC_EXPR_GRAMMAR[\"<digit>\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how `exp_probabilities()` distributes any remaining probability across non-annotated expansions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_probabilities(PROBABILISTIC_EXPR_GRAMMAR[\"<expr>\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking Probabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use the checking capabilities of `exp_probabilities()` to check a probabilistic grammar for consistency:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_valid_probabilistic_grammar(grammar: Grammar,\n",
    "                                   start_symbol: str = START_SYMBOL) -> bool:\n",
    "    if not is_valid_grammar(grammar, start_symbol):\n",
    "        return False\n",
    "\n",
    "    for nonterminal in grammar:\n",
    "        expansions = grammar[nonterminal]\n",
    "        _ = exp_probabilities(expansions, nonterminal)\n",
    "\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_probabilistic_grammar(PROBABILISTIC_EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_probabilistic_grammar(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ExpectError import ExpectError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ExpectError():\n",
    "    assert is_valid_probabilistic_grammar({\"<start>\": [(\"1\", opts(prob=0.5))]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with ExpectError():\n",
    "    assert is_valid_probabilistic_grammar(\n",
    "        {\"<start>\": [(\"1\", opts(prob=1.5)), \"2\"]})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expanding by Probability\n",
    "\n",
    "Now that we have seen how to specify probabilities for a grammar, we can actually implement probabilistic expansion.  In our `ProbabilisticGrammarFuzzer`, it suffices to overload one method, namely `choose_node_expansion()`.  For each of the children we can choose from (typically all expansions of a symbol), we determine their probability (using `exp_probabilities()` defined above), and make a weighted choice using `random.choices()` with a `weight` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticGrammarFuzzer(GrammarFuzzer):\n",
    "    \"\"\"A grammar-based fuzzer respecting probabilities in grammars.\"\"\"\n",
    "\n",
    "    def check_grammar(self) -> None:\n",
    "        super().check_grammar()\n",
    "        assert is_valid_probabilistic_grammar(self.grammar)\n",
    "\n",
    "    def supported_opts(self) -> Set[str]:\n",
    "        return super().supported_opts() | {'prob'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticGrammarFuzzer(ProbabilisticGrammarFuzzer):\n",
    "    def choose_node_expansion(self, node: DerivationTree,\n",
    "                              children_alternatives: List[Any]) -> int:\n",
    "        (symbol, tree) = node\n",
    "        expansions = self.grammar[symbol]\n",
    "        probabilities = exp_probabilities(expansions)\n",
    "\n",
    "        weights: List[float] = []\n",
    "        for children in children_alternatives:\n",
    "            expansion = all_terminals((symbol, children))\n",
    "            children_weight = probabilities[expansion]\n",
    "            if self.log:\n",
    "                print(repr(expansion), \"p =\", children_weight)\n",
    "            weights.append(children_weight)\n",
    "\n",
    "        if sum(weights) == 0:\n",
    "            # No alternative (probably expanding at minimum cost)\n",
    "            return random.choices(\n",
    "                range(len(children_alternatives)))[0]\n",
    "        else:\n",
    "            return random.choices(\n",
    "                range(len(children_alternatives)), weights=weights)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our probabilistic grammar fuzzer works just like the non-probabilistic grammar fuzzer, except that it actually respects probability annotations.  Let us generate a couple of \"natural\" numbers that respect Benford's law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "natural_fuzzer = ProbabilisticGrammarFuzzer(\n",
    "    PROBABILISTIC_EXPR_GRAMMAR, start_symbol=\"<leadinteger>\")\n",
    "print([natural_fuzzer.fuzz() for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, these numbers are pure random:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "integer_fuzzer = GrammarFuzzer(\n",
    "    PROBABILISTIC_EXPR_GRAMMAR, start_symbol=\"<leadinteger>\")\n",
    "print([integer_fuzzer.fuzz() for i in range(20)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Are the \"natural\" numbers really more \"natural\" than the random ones?  To show that `ProbabilisticGrammarFuzzer` indeed respects  the probabilistic annotations, let us create a specific fuzzer for the lead digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "leaddigit_fuzzer = ProbabilisticGrammarFuzzer(\n",
    "    PROBABILISTIC_EXPR_GRAMMAR, start_symbol=\"<leaddigit>\")\n",
    "leaddigit_fuzzer.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we generate thousands of lead digits, their distribution should again follow Benford's law:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 10000\n",
    "\n",
    "count = {}\n",
    "for c in crange('0', '9'):\n",
    "    count[c] = 0\n",
    "\n",
    "for i in range(trials):\n",
    "    count[leaddigit_fuzzer.fuzz()] += 1\n",
    "\n",
    "print([(digit, count[digit] / trials) for digit in count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quod erat demonstrandum! The distribution is pretty much exactly as originally specified.  We now have a fuzzer where we can exercise control by specifying probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Directed Fuzzing\n",
    "\n",
    "Assigning probabilities to individual expansions gives us great control over which inputs should be generated.  By choosing probabilities wisely, we can _direct_ fuzzing towards specific functions and features â€“Â for instance, towards functions that are particularly critical, prone to failures, or that have been recently changed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, consider the URL grammar from the [chapter on grammars](Grammars.ipynb).  Let us assume we have just made a change to our implementation of the secure FTP protocol.  By assigning a higher probability to the `ftps` scheme, we can generate more URLs that will specifically test this functionality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let us define a helper function that sets a particular option:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a specialization just for probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_prob(grammar: Grammar, symbol: str, \n",
    "             expansion: Expansion, prob: Optional[float]) -> None:\n",
    "    \"\"\"Set the probability of the given expansion of grammar[symbol]\"\"\"\n",
    "    set_opts(grammar, symbol, expansion, opts(prob=prob))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use `set_prob()` to give the `ftps` expansion a probability of 80%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Grammars import URL_GRAMMAR, extend_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_url_grammar = extend_grammar(URL_GRAMMAR)\n",
    "set_prob(probabilistic_url_grammar, \"<scheme>\", \"ftps\", 0.8)\n",
    "assert is_valid_probabilistic_grammar(probabilistic_url_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_url_grammar[\"<scheme>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use this grammar for fuzzing, we will get plenty of `ftps:` prefixes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_url_fuzzer = ProbabilisticGrammarFuzzer(probabilistic_url_grammar)\n",
    "for i in range(10):\n",
    "    print(prob_url_fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a similar vein, we can direct URL generation towards specific hosts or ports; we can favor URLs with queries, fragments, or logins â€“ or URLs without these.  All it takes is to set appropriate probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By setting the probability of an expansion to zero, we can effectively disable specific expansions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_prob(probabilistic_url_grammar, \"<scheme>\", \"ftps\", 0.0)\n",
    "assert is_valid_probabilistic_grammar(probabilistic_url_grammar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_url_fuzzer = ProbabilisticGrammarFuzzer(probabilistic_url_grammar)\n",
    "for i in range(10):\n",
    "    print(prob_url_fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that even if we set the probability of an expansion to zero, we may still see the expansion taken. This can happen during the \"closing\" phase of [our grammar fuzzer](GrammarFuzzer.ipynb), when the expansion is closed at minimum cost.  At this stage, even expansions with \"zero\" probability will be taken if this is necessary for closing the expansion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us illustrate this feature using the `<expr>` rule from our expression grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Grammars import EXPR_GRAMMAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_expr_grammar = extend_grammar(EXPR_GRAMMAR)\n",
    "probabilistic_expr_grammar[\"<expr>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we set the probability of the `<term>` expansion to zero, the string should expand again and again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_prob(probabilistic_expr_grammar, \"<expr>\", \"<term>\", 0.0)\n",
    "assert is_valid_probabilistic_grammar(probabilistic_expr_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Still, in the \"closing\" phase, subexpressions will eventually expand into `<term>`, as it is the only way to close the expansion.  Tracking `choose_node_expansion()` shows that it is invoked with only one possible expansion `<term>`, which has to be taken even though its specified probability is zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prob_expr_fuzzer = ProbabilisticGrammarFuzzer(probabilistic_expr_grammar)\n",
    "prob_expr_fuzzer.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Probabilities in Context\n",
    "\n",
    "While specified probabilities give us a means to control which expansions are taken how often, this control by itself may not be enough.  As an example, consider the following grammar for IPv4 addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decrange(start: int, end: int) -> List[Expansion]:\n",
    "    \"\"\"Return a list with string representations of numbers in the range [start, end)\"\"\"\n",
    "    return [repr(n) for n in range(start, end)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_ADDRESS_GRAMMAR: Grammar = {\n",
    "    \"<start>\": [\"<address>\"],\n",
    "    \"<address>\": [\"<octet>.<octet>.<octet>.<octet>\"],\n",
    "    # [\"0\", \"1\", \"2\", ..., \"255\"]\n",
    "    \"<octet>\": decrange(0, 256)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(IP_ADDRESS_GRAMMAR[\"<octet>\"][:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(IP_ADDRESS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can easily use this grammar to create IP addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ip_fuzzer = ProbabilisticGrammarFuzzer(IP_ADDRESS_GRAMMAR)\n",
    "ip_fuzzer.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if we want to assign a specific probability to one of the four octets, we are out of luck.  All we can do is to assign the same probability distribution for all four octets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_ip_address_grammar = extend_grammar(IP_ADDRESS_GRAMMAR)\n",
    "set_prob(probabilistic_ip_address_grammar, \"<octet>\", \"127\", 0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_ip_fuzzer = ProbabilisticGrammarFuzzer(\n",
    "    probabilistic_ip_address_grammar)\n",
    "probabilistic_ip_fuzzer.fuzz()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to assign _different_ probabilities to each of the four octets, what do we do?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The answer lies in the concept of _context_, which we already have seen [while discussing coverage-driven fuzzers](GrammarCoverageFuzzer.ipynb).  As with coverage-driven fuzzing, the idea is to _duplicate_ the element whose probability we want to set dependent on its context.  In our case, this means to duplicate the `<octet>` element to four individual ones, each of which can then get an individual probability distribution.  We can do this programmatically, using the `duplicate_context()` method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GrammarCoverageFuzzer import duplicate_context  # minor dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_ip_address_grammar = extend_grammar(IP_ADDRESS_GRAMMAR)\n",
    "duplicate_context(probabilistic_ip_address_grammar, \"<address>\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_ip_address_grammar[\"<address>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now assign different probabilities to each of the `<octet>` symbols.  For instance, we can force specific expansions by setting their probability to 100%:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_prob(probabilistic_ip_address_grammar, \"<octet-1>\", \"127\", 1.0)\n",
    "set_prob(probabilistic_ip_address_grammar, \"<octet-2>\", \"0\", 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_probabilistic_grammar(probabilistic_ip_address_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The remaining two octets `<octet-3>` and `<octet-4>` have no specific probabilities set.  During fuzzing, all their expansions (all octets) are thus still available:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_ip_fuzzer = ProbabilisticGrammarFuzzer(\n",
    "    probabilistic_ip_address_grammar)\n",
    "[probabilistic_ip_fuzzer.fuzz() for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as with coverage, we can duplicate grammar rules arbitrarily often to get more and more finer-grained control over probabilities.  However, this finer-grained control also comes at the cost of having to maintain these probabilities.  In the next section, we will therefore discuss means to assign and tune such probabilities automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": true
   },
   "source": [
    "## Learning Probabilities from Samples\n",
    "\n",
    "Probabilities need not be set manually all the time.  They can also be _learned_ from other sources, notably by counting _how frequently individual expansions occur in a given set of inputs_.  This is useful in a number of situations, including:\n",
    "\n",
    "1. Test _common_ features.  The idea is that during testing, one may want to focus on frequently occurring (or frequently used) features first, to ensure correct functionality for the most common usages.\n",
    "2. Test _uncommon_ features.  Here, the idea is to have test generation focus on features that are rarely seen (or not seen at all) in inputs.  This is the same motivation as with [grammar coverage](GrammarCoverageFuzzer.ipynb), but from a probabilistic standpoint.\n",
    "3. Focus on specific _slices_.  One may have a set of inputs that is of particular interest (for instance, because they exercise a critical functionality, or recently have discovered bugs).  Using this learned distribution for fuzzing allows us to _focus_ on precisely these functionalities of interest.\n",
    "\n",
    "Let us first introduce counting expansions and learning probabilities, and then detail these scenarios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Counting Expansions\n",
    "\n",
    "We start with implementing a means to take a set of inputs and determine the number of expansions in that set.  To this end, we need the _parsers_ introduced [in the previous chapter](Parser.ipynb) to transform a string input into a derivation tree.  For our IP address grammar, this is how this works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser import Parser, EarleyParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IP_ADDRESS_TOKENS = {\"<octet>\"}  # EarleyParser needs explicit tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = EarleyParser(IP_ADDRESS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree, *_ = parser.parse(\"127.0.0.1\")\n",
    "display_tree(tree)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a tree such as this one, we can now _count_ individual expansions.  In the above tree, for instance, we have two expansions of `<octet>` into `0`, one into `1`, and one into `127`.  The expansion `<octet>` into `0` makes up 50% of all expansions seen; the expansions into `127` and `1` make up 25% each, and the other ones 0%.  These are the probabilities we'd like to assign to our \"learned\" grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduce a class `ExpansionCountMiner` which allows us to count how frequently individual expansions take place.  Its initialization method takes a parser (say, an `EarleyParser`) that would be initialized with the appropriate grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GrammarCoverageFuzzer import expansion_key  # minor dependency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Grammars import is_nonterminal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpansionCountMiner:\n",
    "    def __init__(self, parser: Parser, log: bool = False) -> None:\n",
    "        assert isinstance(parser, Parser)\n",
    "        self.grammar = extend_grammar(parser.grammar())\n",
    "        self.parser = parser\n",
    "        self.log = log\n",
    "        self.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The attribute `expansion_counts` holds the expansions seen; adding a tree with `add_tree()` traverses the given tree and adds all expansions seen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpansionCountMiner(ExpansionCountMiner):\n",
    "    def reset(self) -> None:\n",
    "        self.expansion_counts: Dict[str, int] = {}\n",
    "\n",
    "    def add_coverage(self, symbol: str, children: List[DerivationTree]) -> None:\n",
    "        key = expansion_key(symbol, children)\n",
    "\n",
    "        if self.log:\n",
    "            print(\"Found\", key)\n",
    "\n",
    "        if key not in self.expansion_counts:\n",
    "            self.expansion_counts[key] = 0\n",
    "        self.expansion_counts[key] += 1\n",
    "\n",
    "    def add_tree(self, tree: DerivationTree) -> None:\n",
    "        (symbol, children) = tree\n",
    "        if not is_nonterminal(symbol):\n",
    "            return\n",
    "        assert children is not None\n",
    "\n",
    "        direct_children: List[DerivationTree] = [\n",
    "            (symbol, None) if is_nonterminal(symbol) \n",
    "            else (symbol, []) for symbol, c in children]\n",
    "        self.add_coverage(symbol, direct_children)\n",
    "\n",
    "        for c in children:\n",
    "            self.add_tree(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method `count_expansions()` is the one facing the public; it takes a list of inputs, parses them, and processes the resulting trees.  The method ` counts()` returns the counts found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExpansionCountMiner(ExpansionCountMiner):\n",
    "    def count_expansions(self, inputs: List[str]) -> None:\n",
    "        for inp in inputs:\n",
    "            tree, *_ = self.parser.parse(inp)\n",
    "            self.add_tree(tree)\n",
    "\n",
    "    def counts(self) -> Dict[str, int]:\n",
    "        return self.expansion_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try this out on our IP address grammar.  We create an `ExpansionCountMiner` for our IP address grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_count_miner = ExpansionCountMiner(EarleyParser(IP_ADDRESS_GRAMMAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We parse a (small) set of IP addresses and count the expansions occurring:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expansion_count_miner.count_expansions([\"127.0.0.1\", \"1.2.3.4\"])\n",
    "expansion_count_miner.counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You see that we have one expansion into `127`, and two into `0`.  These are the counts we can use to assign probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Assigning Probabilities\n",
    "\n",
    "The distribution of counts, as determined by `ExpansionCountMiner` is what we can use to assign probabilities to our grammar.  To this end, we introduce a subclass `ProbabilisticGrammarMiner` whose method `set_expansion_probabilities()` processes all expansions of a given symbol, checks whether it occurs in a given count distribution, and assigns probabilities using the following formula. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given a set $T$ of derivation trees (as mined from samples), we determine the probabilities $p_i$ for each alternative $a_i$ of a symbol $S \\rightarrow a_1 | \\dots | a_n$ as\n",
    "\n",
    "$$p_i = \\frac{\\text{Expansions of $S \\rightarrow a_i$ in $T$}}{\\text{Expansions of $S$ in $T$}}$$\n",
    "\n",
    "Should $S$ not occur at all in $T$, then $p_i$ is _unspecified_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is the implementation of `set_expansion_probabilities()`, implementing the above formula:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticGrammarMiner(ExpansionCountMiner):\n",
    "    def set_probabilities(self, counts: Dict[str, int]):\n",
    "        for symbol in self.grammar:\n",
    "            self.set_expansion_probabilities(symbol, counts)\n",
    "\n",
    "    def set_expansion_probabilities(self, symbol: str, counts: Dict[str, int]):\n",
    "        expansions = self.grammar[symbol]\n",
    "        if len(expansions) == 1:\n",
    "            set_prob(self.grammar, symbol, expansions[0], None)\n",
    "            return\n",
    "\n",
    "        expansion_counts = [\n",
    "            counts.get(\n",
    "                expansion_key(\n",
    "                    symbol,\n",
    "                    expansion),\n",
    "                0) for expansion in expansions]\n",
    "        total = sum(expansion_counts)\n",
    "        for i, expansion in enumerate(expansions):\n",
    "            p = expansion_counts[i] / total if total > 0 else None\n",
    "            # if self.log:\n",
    "            #     print(\"Setting\", expansion_key(symbol, expansion), p)\n",
    "            set_prob(self.grammar, symbol, expansion, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The typical use of `ProbabilisticGrammarMiner` is through `mine_probabilistic_grammar()`, which first determines a distribution from a set of inputs, and then sets the probabilities accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProbabilisticGrammarMiner(ProbabilisticGrammarMiner):\n",
    "    def mine_probabilistic_grammar(self, inputs: List[str]) -> Grammar:\n",
    "        self.count_expansions(inputs)\n",
    "        self.set_probabilities(self.counts())\n",
    "        return self.grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us put this to use.  We create a grammar miner for IP addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_grammar_miner = ProbabilisticGrammarMiner(\n",
    "    EarleyParser(IP_ADDRESS_GRAMMAR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now use `mine_probabilistic_grammar()` to mine the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_ip_address_grammar = probabilistic_grammar_miner.mine_probabilistic_grammar([\n",
    "                                                                                          \"127.0.0.1\", \"1.2.3.4\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_probabilistic_grammar(probabilistic_ip_address_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the resulting distribution of octets in our grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[expansion for expansion in probabilistic_ip_address_grammar['<octet>']\n",
    "    if exp_prob(expansion) > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we use these probabilities for fuzzing, we will get the same distribution of octets as in our sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_ip_fuzzer = ProbabilisticGrammarFuzzer(\n",
    "    probabilistic_ip_address_grammar)\n",
    "[probabilistic_ip_fuzzer.fuzz() for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By learning from a sample, we can thus adjust our fuzzing towards the (syntactic) properties of this very sample."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Common Features\n",
    "\n",
    "Let us now get to our three usage scenarios.  The first scenario is to create probability distributions right out of a sample, and to use these very distributions during test generation.  This helps to focus test generation on those features that are _most commonly used_, which thus minimizes the risk of customers encountering failures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate testing of common features, we choose the URL domain.  Let us assume that we are running some Web-related service, and this is a sample of the URLs our customers access most:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_SAMPLE: List[str] = [\n",
    "    \"https://user:password@cispa.saarland:80/\",\n",
    "    \"https://fuzzingbook.com?def=56&x89=3&x46=48&def=def\",\n",
    "    \"https://cispa.saarland:80/def?def=7&x23=abc\",\n",
    "    \"https://fuzzingbook.com:80/\",\n",
    "    \"https://fuzzingbook.com:80/abc?def=abc&abc=x14&def=abc&abc=2&def=38\",\n",
    "    \"ftps://fuzzingbook.com/x87\",\n",
    "    \"https://user:password@fuzzingbook.com:6?def=54&x44=abc\",\n",
    "    \"http://fuzzingbook.com:80?x33=25&def=8\",\n",
    "    \"http://fuzzingbook.com:8080/def\",\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Earley parser from the [chapter on parsers](Parser.ipynb), we can parse any of these inputs into a parse tree; we have to specify a token set, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL_TOKENS: Set[str] = {\"<scheme>\", \"<userinfo>\", \"<host>\", \"<port>\", \"<id>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url_parser = EarleyParser(URL_GRAMMAR, tokens=URL_TOKENS)\n",
    "url_input = URL_SAMPLE[2]\n",
    "print(url_input)\n",
    "tree, *_ = url_parser.parse(url_input)\n",
    "display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us apply our `ProbabilisticGrammarMiner` class on these inputs, using the above `url_parser` parser, and obtain a probabilistic URL grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_grammar_miner = ProbabilisticGrammarMiner(url_parser)\n",
    "probabilistic_url_grammar = probabilistic_grammar_miner.mine_probabilistic_grammar(\n",
    "    URL_SAMPLE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are the counts we obtained during parsing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(probabilistic_grammar_miner.counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These counts translate into individual probabilities.  We see that in our sample, most URLs use the `https:` scheme, whereas there is no input using the `ftp:` scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_url_grammar['<scheme>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise, we see that most given URLs have multiple parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_url_grammar['<params>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we use this probabilistic grammar for fuzzing, these distributions are reflected in our generated inputs â€“Â no `ftp:` schemes either, and most inputs have multiple parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ProbabilisticGrammarFuzzer(probabilistic_url_grammar)\n",
    "[g.fuzz() for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Being able to replicate a probability distribution learned from a sample is not only important for focusing on commonly used features.  It can also help in achieving _valid inputs_, in particular if one learns probabilities _in context_, as discussed above: If within a given context, some elements are more likely than others (because they depend on each other), a learned probability distribution will reflect this; and hence, inputs generated from this learned probability distribution will have a higher chance to be valid, too.  We will explore this further in the [exercises](#Exercises), below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Uncommon Features\n",
    "\n",
    "So far, we have focused on _common_ features; but from a testing perspective, one may just as well test _uncommon_ features â€“Â that is, features that rarely occur in our usage samples and therefore would be less exercised in practice.  This is a common scenario in security testing, where one focuses on uncommon (and possibly lesser-known) features, as fewer users means fewer bugs reported, and thus more bugs left to be found and exploited."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To have our probabilistic grammar fuzzer focus on _uncommon_ features, we _change the learned probabilities_ such that commonly occuring features (i.e., those with a high learned probability) get a low probability, and vice versa: The last shall be first, and the first last.  A particularly simple way to achieve such an _inversion_ of probabilities is to _swap_ them: The alternatives with the highest and lowest probability swaps their probabilities, as so the alternatives with the second-highest and second-lowest probability, the alternatives with the third highest and lowest, and so on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `invert_expansion()` takes an expansion (a list of alternatives) from a grammar and returns a new inverted expansion in which the probabilities have been swapped according to the rule above.  It creates a list of indexes, sorts it by increasing probability, and then for each $n$-th element, assigns it the probability of the $n$-th last element in the indexes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_expansion(expansion: List[Expansion]) -> List[Expansion]:\n",
    "    def sort_by_prob(x: Tuple[int, float]) -> float:\n",
    "        index, prob = x\n",
    "        return prob if prob is not None else 0.0\n",
    "\n",
    "    inverted_expansion: List[Expansion] = copy.deepcopy(expansion)\n",
    "    indexes_and_probs = [(index, exp_prob(alternative))\n",
    "                         for index, alternative in enumerate(expansion)]\n",
    "    indexes_and_probs.sort(key=sort_by_prob)\n",
    "    indexes = [i for (i, _) in indexes_and_probs]\n",
    "\n",
    "    for j in range(len(indexes)):\n",
    "        k = len(indexes) - 1 - j\n",
    "        # print(indexes[j], \"gets\", indexes[k])\n",
    "        inverted_expansion[indexes[j]][1]['prob'] = expansion[indexes[k]][1]['prob']  # type: ignore\n",
    "\n",
    "    return inverted_expansion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's `invert_expansion()` in action.  This is our original probability distribution for URL schemes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_url_grammar['<scheme>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And this is the \"inverted\" distribution.  We see that the `ftp:` scheme, which previously had a probability of zero, now has the highest probability, whereas the most common scheme, `https:`, now has the previous zero probability of the `ftp:` scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_expansion(probabilistic_url_grammar['<scheme>'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One nice feature of this swapping of probabilities is that the sum of probabilities stays unchanged; no normalization is needed.  Another nice feature is that the inversion of the inversion returns the original distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invert_expansion(invert_expansion(probabilistic_url_grammar['<scheme>']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our implementation does not universally satisfy this property: If two alternatives $a_1$ and $a_2$ in the expansion share the same probability, then the second inversion may assign different probabilities to $a_1$ and $a_2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can apply this inversion of expansions across the entire grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_probs(grammar: Grammar) -> Grammar:\n",
    "    inverted_grammar = extend_grammar(grammar)\n",
    "    for symbol in grammar:\n",
    "        inverted_grammar[symbol] = invert_expansion(grammar[symbol])\n",
    "    return inverted_grammar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This means that probabilities would be swapped for each and every expansion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_url_grammar[\"<digit>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverted_probabilistic_url_grammar = invert_probs(probabilistic_url_grammar)\n",
    "inverted_probabilistic_url_grammar[\"<digit>\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we now use this \"inverted\" grammar for fuzzing, the generated inputs will focus on the *complement of the input samples*.  We will get plenty of tests of user/password features, as well as `ftp:` schemes â€“ in essence, all the features present in our language, but rarely used (if at all) in our input samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = ProbabilisticGrammarFuzzer(inverted_probabilistic_url_grammar)\n",
    "[g.fuzz() for i in range(10)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides having _only_ common or _only_ uncommon features, one can also create mixed forms â€“Â for instance, testing uncommon features in a common context.  This can be helpful for security testing, where one may want an innocuous (common) \"envelope\" combined with an (uncommon) \"payload\".  It all depends on where and how we tune the probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Probabilities from Input Slices\n",
    "\n",
    "In our previous examples, we have learned from _all_ inputs to generate common or uncommon inputs.  However, we can also learn from a _subset_ of inputs to focus on the features present in that subset (or, conversely, to _avoid_ its features).  If we know, for instance, that there is some subset of inputs that covers a functionality of interest (say, because it is particularly critical or because it has been recently changed), we can learn from this very subset and focus our test generation on its features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To illustrate this approach, let us use the CGI grammar introduced in the [chapter on coverage](Coverage.ipynb).  We have a special interest in Line 25 in our CGI decoder â€“Â that is, the line that processes a `%` character followed by two valid hexadecimal digits:\n",
    "\n",
    "```python\n",
    "        ...\n",
    "        elif c == '%':\n",
    "            digit_high, digit_low = s[i + 1], s[i + 2]\n",
    "            i += 2\n",
    "            if digit_high in hex_values and digit_low in hex_values:\n",
    "                v = hex_values[digit_high] * 16 + hex_values[digit_low] ### Line 25\n",
    "                t += chr(v)\n",
    "        ...\n",
    "\n",
    "```\n",
    "Let us assume that we do not know precisely under which conditions Line 25 is executed â€“ but still, we'd like to test it thoroughly.  With our probability learning tools, we can learn these conditions, though.  We start with a set of random inputs and consider the subset that covers Line 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Coverage import Coverage, cgi_decode\n",
    "from Grammars import CGI_GRAMMAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cgi_fuzzer = GrammarFuzzer(CGI_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "coverage = {}\n",
    "\n",
    "for i in range(trials):\n",
    "    cgi_input = cgi_fuzzer.fuzz()\n",
    "    with Coverage() as cov:\n",
    "        cgi_decode(cgi_input)\n",
    "    coverage[cgi_input] = cov.coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are all the random inputs that cover Line 25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_slice = [cgi_input for cgi_input in coverage\n",
    "                  if ('cgi_decode', 25) in coverage[cgi_input]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(coverage_slice)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Actually, about half of the inputs cover Line 25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coverage_slice) / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us now learn a probabilistic grammar from this slice of inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_grammar_miner = ProbabilisticGrammarMiner(\n",
    "    EarleyParser(CGI_GRAMMAR))\n",
    "probabilistic_cgi_grammar = probabilistic_grammar_miner.mine_probabilistic_grammar(\n",
    "    coverage_slice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_probabilistic_grammar(probabilistic_cgi_grammar)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that percentage signs are very likely to occur:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_cgi_grammar['<letter>']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using this grammar, we can now generate tests that specifically target Line 25:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_cgi_fuzzer = ProbabilisticGrammarFuzzer(\n",
    "    probabilistic_cgi_grammar)\n",
    "print([probabilistic_cgi_fuzzer.fuzz() for i in range(20)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trials = 100\n",
    "coverage = {}\n",
    "\n",
    "for i in range(trials):\n",
    "    cgi_input = probabilistic_cgi_fuzzer.fuzz()\n",
    "    with Coverage() as cov:\n",
    "        cgi_decode(cgi_input)\n",
    "    coverage[cgi_input] = cov.coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that the fraction of inputs that cover Line 25 is much higher already, showing that our focusing works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_slice: List[str] = [cgi_input for cgi_input in coverage\n",
    "                             if ('cgi_decode', 25) in coverage[cgi_input]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coverage_slice) / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Repeating this one more time yields an even higher focusing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for run in range(3):\n",
    "    probabilistic_cgi_grammar = probabilistic_grammar_miner.mine_probabilistic_grammar(\n",
    "        coverage_slice)\n",
    "    probabilistic_cgi_fuzzer = ProbabilisticGrammarFuzzer(\n",
    "        probabilistic_cgi_grammar)\n",
    "\n",
    "    trials = 100\n",
    "    coverage = {}\n",
    "\n",
    "    for i in range(trials):\n",
    "        cgi_input = probabilistic_cgi_fuzzer.fuzz()\n",
    "        with Coverage() as cov:\n",
    "            cgi_decode(cgi_input)\n",
    "        coverage[cgi_input] = cov.coverage()\n",
    "\n",
    "    coverage_slice = [cgi_input for cgi_input in coverage\n",
    "                      if ('cgi_decode', 25) in coverage[cgi_input]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coverage_slice) / trials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By learning (and re-learning) probabilities from a subset of sample inputs, we can _specialize_ fuzzers towards the properties of that subset â€“Â in our case, inputs that contain percentage signs and valid hexadecimal letters.  The degree to which we can specialize things is induced by the number of variables we can control â€“ in our case, the probabilities for the individual rules.  Adding more context to the grammar, as discussed above, will increase the number of variables, and thus the amount of specialization."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A high degree of specialization, however, limits our possibilities to explore combinations that fall _outside_ the selected scope, and limit our possibilities to find bugs induced by these combinations.  This trade-off is known as *exploration vs. exploitation* in machine learning â€“ shall one try to explore as many (possibly shallow) combinations as possible, or focus (exploit) specific areas?  In the end, it all depends on where the bugs are, and where we are most likely to find them.  Assigning and learning probabilities allows us to control the search strategies â€“ from the common to the uncommon to specific subsets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecting Unnatural Numbers\n",
    "\n",
    "Let us close this chapter by getting back to our introductory example.  We said that Benford's law allows us not only to produce, but also to detect \"unnatural\" lead digit distributions such as the ones produced by simple random choices.\n",
    "\n",
    "If we use the regular `GrammarFuzzer` class (which ignores probabilities) to generate (random) lead digits, this is the distribution we get for each leading digit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 1000\n",
    "random_integer_fuzzer = GrammarFuzzer(\n",
    "    PROBABILISTIC_EXPR_GRAMMAR,\n",
    "    start_symbol=\"<leaddigit>\")\n",
    "random_integers = [random_integer_fuzzer.fuzz() for i in range(sample_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_counts = [random_integers.count(str(c)) for c in crange('1', '9')]\n",
    "random_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(For simplicity, we use the simple list `count()` method here rather than deploying the full-fledged `ProbabilisticGrammarMiner`.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we had a natural distribution of lead digits, this is what we would expect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_prob_counts = [\n",
    "    exp_prob(\n",
    "        PROBABILISTIC_EXPR_GRAMMAR[\"<leaddigit>\"][i]) *\n",
    "    sample_size for i in range(9)]\n",
    "print(expected_prob_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And if we had a random distribution, we would expect an equal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "expected_random_counts = [sample_size / 9 for i in range(9)]\n",
    "print(expected_random_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which distribution better matches our `random_counts` lead digits?  To this end, we run a $\\chi^2$-test to compare the distribution we found (`random_counts`) against the \"natural\" lead digit distribution `expected_prob_counts` and the random distribution `expected_random_counts`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import chisquare  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It turns out that there is a zero chance (`pvalue` = 0.0) that the observed distribution follows a \"natural\" distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisquare(random_counts, expected_prob_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, there is a 97% chance that the observed behavior follows a random distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chisquare(random_counts, expected_random_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, if you find some numbers published and doubt their validity, you can run the above test to check whether they are likely to be natural.  Better yet, insist that authors use Jupyter notebooks to produce their results, such that you can check every step of the calculation :-)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synopsis\n",
    "\n",
    "A _probabilistic_ grammar allows attaching individual _probabilities_ to production rules.  To set the probability of an individual expansion `S` to the value `X` (between 0 and 1), replace it with a pair\n",
    "\n",
    "```python\n",
    "(S, opts(prob=X))\n",
    "```\n",
    "\n",
    "If we want to ensure that 90% of phone numbers generated have an area code starting with `9`, we can write:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Grammars import US_PHONE_GRAMMAR, extend_grammar, opts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROBABILISTIC_US_PHONE_GRAMMAR: Grammar = extend_grammar(US_PHONE_GRAMMAR,\n",
    "{\n",
    "      \"<lead-digit>\": [\n",
    "                          \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\",\n",
    "                          (\"9\", opts(prob=0.9))\n",
    "                      ],\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `ProbabilisticGrammarFuzzer` will extract and interpret these options.  Here is an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "probabilistic_us_phone_fuzzer = ProbabilisticGrammarFuzzer(PROBABILISTIC_US_PHONE_GRAMMAR)\n",
    "[probabilistic_us_phone_fuzzer.fuzz() for i in range(5)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the large majority of area codes now starts with `9`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "from ClassDiagram import display_class_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ignore\n",
    "display_class_hierarchy([ProbabilisticGrammarFuzzer],\n",
    "                        public_methods=[\n",
    "                            Fuzzer.run,\n",
    "                            Fuzzer.runs,\n",
    "                            GrammarFuzzer.__init__,\n",
    "                            GrammarFuzzer.fuzz,\n",
    "                            GrammarFuzzer.fuzz_tree,\n",
    "                            ProbabilisticGrammarFuzzer.__init__,\n",
    "                        ],\n",
    "                        types={\n",
    "                            'DerivationTree': DerivationTree,\n",
    "                            'Expansion': Expansion,\n",
    "                            'Grammar': Grammar\n",
    "                        },\n",
    "                        project='fuzzingbook')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* By specifying probabilities, one can steer fuzzing towards input features of interest.\n",
    "* Learning probabilities from samples allows one to focus on features that are common or uncommon in input samples.\n",
    "* Learning probabilities from a subset of samples allows one to produce more similar inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "Now that we have brought together probabilities and grammars (and revisited parsers and grammars), we have created a foundation for many applications.  Our next chapters will focus on\n",
    "\n",
    "* how to [_reduce_ failing inputs to a minimum](Reducer.ipynb)\n",
    "* how to [carve](Carver.ipynb) and [produce](APIFuzzer.ipynb) tests at the function level \n",
    "* how to [automatically test (Web) user interfaces](WebFuzzer.ipynb)\n",
    "\n",
    "Enjoy!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The idea of mining probabilities by parsing a corpus of data was first covered in \"Learning to Fuzz: Application-Independent Fuzz Testing with Probabilistic, Generative Models of Input Data\" \\cite{Patra2016} which also learns and applies probabilistic rules for derivation trees.  Applying this idea on probabilistic grammars as well as inverting probabilities or learning from slices was first executed in the work \"Inputs from Hell: Generating Uncommon Inputs from Common Samples\" \\cite{Pavese2018}.\n",
    "\n",
    "Our exposition of Benford's law follows [this article](https://brilliant.org/wiki/benfords-law/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    },
    "toc-hr-collapsed": true
   },
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 1: Probabilistic Fuzzing with Coverage\n",
    "\n",
    "Create a class `ProbabilisticGrammarCoverageFuzzer` that extends `GrammarCoverageFuzzer` with probabilistic capabilities.  The idea is to first cover all uncovered expansions (like `GrammarCoverageFuzzer`) and once all expansions are covered, to proceed by probabilities (like `ProbabilisticGrammarFuzzer`).\n",
    "\n",
    "To this end, define new instances of the `choose_covered_node_expansion()` and `choose_uncovered_node_expansion()` methods that choose an expansion based on the given weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "If you are an advanced programmer, realize the class via *multiple inheritance* from `GrammarCoverageFuzzer` and `ProbabilisticGrammarFuzzer` to achieve this.  \n",
    "\n",
    "Multiple inheritance is a tricky thing.  If you have two classes $A'$ and $A''$ which both inherit from $A$, the same method $m()$ of $A$ may be overloaded in both $A'$ and $A''$.  If one now inherits from _both_ $A'$ and $A''$, and calls $m()$, which of the $m()$ implementations should be called?  Python \"resolves\" this conflict by simply invoking the one $m()$ method in the class one inherits from first.\n",
    "\n",
    "To avoid such conflicts, one can check whether the order in which one inherits makes a difference.  The method `inheritance_conflicts()` compares the attributes with each other; if they refer to different code, you have to resolve the conflict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import inheritance_conflicts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inheritance_conflicts(GrammarCoverageFuzzer, ProbabilisticGrammarFuzzer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "This is a method you _have_ to implement for multiple inheritance besides `choose_covered_node_expansion()` and `choose_uncovered_node_expansion()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "**Solution**.  With multiple inheritance, this is fairly easy; we just need to point the three methods to the right places:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "class ProbabilisticGrammarCoverageFuzzer(\n",
    "        GrammarCoverageFuzzer, ProbabilisticGrammarFuzzer):\n",
    "    # Choose uncovered expansions first\n",
    "    def choose_node_expansion(self, node, children_alternatives):\n",
    "        return GrammarCoverageFuzzer.choose_node_expansion(\n",
    "            self, node, children_alternatives)\n",
    "\n",
    "    # Among uncovered expansions, pick by (relative) probability\n",
    "    def choose_uncovered_node_expansion(self, node, children_alternatives):\n",
    "        return ProbabilisticGrammarFuzzer.choose_node_expansion(\n",
    "            self, node, children_alternatives)\n",
    "\n",
    "    # For covered nodes, pick by probability, too\n",
    "    def choose_covered_node_expansion(self, node, children_alternatives):\n",
    "        return ProbabilisticGrammarFuzzer.choose_node_expansion(\n",
    "            self, node, children_alternatives)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "In the first nine invocations, our fuzzer covers one digit after another:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "cov_leaddigit_fuzzer = ProbabilisticGrammarCoverageFuzzer(\n",
    "    PROBABILISTIC_EXPR_GRAMMAR, start_symbol=\"<leaddigit>\")\n",
    "print([cov_leaddigit_fuzzer.fuzz() for i in range(9)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "After these, we again proceed by probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "outputs": [],
   "source": [
    "trials = 10000\n",
    "\n",
    "count = {}\n",
    "for c in crange('0', '9'):\n",
    "    count[c] = 0\n",
    "\n",
    "for i in range(trials):\n",
    "    count[cov_leaddigit_fuzzer.fuzz()] += 1\n",
    "\n",
    "print([(digit, count[digit] / trials) for digit in count])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "solution": "hidden",
    "solution2": "hidden",
    "solution2_first": true,
    "solution_first": true
   },
   "source": [
    "### Exercise 2: Learning from Past Bugs\n",
    "\n",
    "Learning from a set of inputs can be extremely valuable if one learns from _inputs that are known to have caused failures before._  In this exercise, you will go and learn distributions from past vulnerabilities.\n",
    "\n",
    "1. Download [`js-vuln-db`](https://github.com/tunz/js-vuln-db), a set of JavaScript engine vulnerabilities.  Each vulnerability comes with code that exercises it.\n",
    "2. Extract all _number literals_ from the code, using `re.findall()` with appropriate regular expressions.\n",
    "3. Convert these literals to (decimal) _numeric values_ and count their respective occurrences.\n",
    "4. Create a grammar `RISKY_NUMBERS` that produces these numbers with probabilities reflecting the above counts.\n",
    "\n",
    "Of course, there is more to vulnerabilities than just a specific numbers, but some numbers are more likely to induce errors than others.  The next time you fuzz a system, do not generate numbers randomly; instead, pick one from `RISKY_NUMBERS` :-)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "skip"
    },
    "solution": "hidden",
    "solution2": "hidden"
   },
   "source": [
    "**Solution.** _Solution for the exercise_"
   ]
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
