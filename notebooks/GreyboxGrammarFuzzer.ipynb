{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Greybox Fuzzing with Grammars\n",
    "\n",
    "<!--\n",
    "Previously, we have learned about [mutational fuzzing](GreyboxFuzzer.ipynb), which generates new inputs by mutating seed inputs. Most mutational fuzzers represent inputs as a sequence of bytes and apply byte-level mutations to this byte sequence. Such byte-level mutations work great for compact file formats with a small number of structural constraints. However, most file formats impose a high-level structure on these byte sequences.\n",
    "\n",
    "Common components of a regular file are file header, data chunks, checksums, data fields, and meta data. Only if this file structure is correctly reflected will the file be accepted by the parser. Otherwise, the file is quickly rejected before reaching interesting parts in the program. It is not easy to generate valid files by [random fuzzing](Fuzzer.ipynb). For instance, only a tiniest proportion of random strings are valid PDF files or valid JPEG image files.\n",
    "-->\n",
    "\n",
    "<!--\n",
    "Maybe we can start with a valid file and generate new valid files by small mutations applied to the original file? Indeed, this is the main insight of ([blackbox](MutationFuzzer.ipynb) and [greybox](GreyboxFuzzer.ipynb)) mutational fuzzing. However, many file formats are so complex that even small modifications lead to invalid inputs that are quickly rejected by the parser.\n",
    "-->\n",
    "\n",
    "In this chapter, we introduce two important extensions to our syntactic fuzzing techniques:\n",
    "\n",
    "1. We show how to combine [parsing](Parser.ipynb) and [fuzzing](GrammarFuzzer.ipynb) with grammars.  This allows to _mutate_ existing inputs while preserving syntactical correctness, and to _reuse_ fragments from existing inputs while generating new ones.  The combination of parsing and fuzzing, as demonstrated in this chapter, has been highly successful in practice: The _LangFuzz_ fuzzer for JavaScript has found more than 2,600 bugs in JavaScript interpreters this way.\n",
    "\n",
    "2. In the previous chapters, we have used grammars in a _black-box_ manner – that is, we have used them to generate inputs regardless of the program being tested.  In this chapter, we introduce mutational _greybox fuzzing with grammars_: Techniques that make use of _feedback from the program under test_ to guide test generations towards specific goals.  As in [lexical greybox fuzzing](GreyboxFuzzer.ipynb), this feedback is mostly _coverage_, allowing us to direct grammar-based testing towards uncovered code parts.  \n",
    "\n",
    "\n",
    "<!--\n",
    "In this chapter, we encode file formats as [grammars](Grammars.ipynb) and make the mutational fuzzer input-structure-aware. We investigate opportunities to inform the fuzzer about the validity of the generated inputs. Specifically, we explore dictionaries, grammars, structural mutators, and validity-based power schedules\n",
    "-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    }
   },
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* We build on several concepts from [the chapter on greybox fuzzing (without grammars)](GreyboxFuzzer.ipynb).\n",
    "* As the title suggests, you should know how to fuzz with grammars [from the chapter on grammars](Grammars.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "First, we [recall](GreyboxFuzzer.ipynb#Ingredients-for-Greybox-Fuzzing) a few basic ingredients for mutational fuzzers.\n",
    "* **Seed**. A _seed_ is an input that is used by the fuzzer to generate new inputs by applying a sequence of mutations.\n",
    "* **Mutator**. A _mutator_ implements a set of mutation operators that applied to an input produce a slightly modified input.\n",
    "* **PowerSchedule**. A _power schedule_ assigns _energy_ to a seed. A seed with higher energy is fuzzed more often throughout the fuzzing campaign.\n",
    "* **MutationFuzzer**. Our _mutational blackbox fuzzer_ generates inputs by mutating seeds in an initial population of inputs.\n",
    "* **GreyboxFuzzer**. Our _greybox fuzzer_ dynamically adds inputs to the population of seeds that increased coverage.\n",
    "* **FunctionCoverageRunner**. Our _function coverage runner_ collects coverage information for the execution of a given Python function.\n",
    "\n",
    "Let's try to get a feeling for these concepts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import bookutils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GreyboxFuzzer import Mutator, Seed, PowerSchedule, MutationFuzzer, GreyboxFuzzer\n",
    "from MutationFuzzer import FunctionCoverageRunner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following command applies a mutation to the input \"Hello World\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Mutator().mutate(\"Hello World\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default power schedule assigns energy uniformly across all seeds. Let's check whether this works.\n",
    "\n",
    "We choose 10k times from a population of three seeds.  As we see in the `hits` counter, each seed is chosen about a third of the time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "population = [Seed(\"A\"), Seed(\"B\"), Seed(\"C\")]\n",
    "schedule = PowerSchedule()\n",
    "hits = {\n",
    "    \"A\" : 0,\n",
    "    \"B\" : 0,\n",
    "    \"C\" : 0\n",
    "}\n",
    "\n",
    "for i in range(10000):\n",
    "    seed = schedule.choose(population)\n",
    "    hits[seed.data] += 1\n",
    "\n",
    "hits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before explaining the function coverage runner, lets import Python's HTML parser as example..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from html.parser import HTMLParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and create a _wrapper function_ that passes each input into a new parser object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_parser(inp):\n",
    "    parser = HTMLParser()\n",
    "    parser.feed(inp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FunctionCoverageRunner` constructor takes a Python `function` to execute. The function `run()` takes an input, passes it on to the Python `function`, and collects the coverage information for this execution. The function `coverage()` returns a list of tuples `(function name, line number)` for each statement that has been covered in the Python `function`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = FunctionCoverageRunner(my_parser)\n",
    "runner.run(\"Hello World\")\n",
    "cov = runner.coverage()\n",
    "\n",
    "list(cov)[:5] # Print 5 statements covered in HTMLParser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our greybox fuzzer takes a seed population, mutator, and power schedule. Let's generate 5000 fuzz inputs starting with an \"empty\" seed corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 5000\n",
    "seed_input = \" \" # empty seed\n",
    "runner = FunctionCoverageRunner(my_parser)\n",
    "fuzzer = GreyboxFuzzer([seed_input], Mutator(), PowerSchedule())\n",
    "\n",
    "start = time.time()\n",
    "fuzzer.runs(runner, trials=n)\n",
    "end = time.time()\n",
    "\n",
    "\"It took the fuzzer %0.2f seconds to generate and execute %d inputs.\" % (end - start, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"During this fuzzing campaign, we covered %d statements.\" % len(runner.coverage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Keyword Dictionary\n",
    "\n",
    "To fuzz our HTML parser, it may be useful to inform a mutational fuzzer about important keywords in the input – that is, important HTML keywords.  To this end, we extend our mutator to consider keywords from a _dictionary_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DictMutator(Mutator):\n",
    "    def __init__(self, dictionary):\n",
    "        super().__init__()\n",
    "        self.dictionary = dictionary\n",
    "        self.mutators.append(self.insert_from_dictionary)\n",
    "        \n",
    "    def insert_from_dictionary(self,s):\n",
    "        \"\"\"Returns s with a keyword from the dictionary inserted\"\"\"\n",
    "        pos = random.randint(0, len(s))\n",
    "        random_keyword = random.choice(self.dictionary)\n",
    "        return s[:pos] + random_keyword + s[pos:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to add a few HTML tags and attributes and see whether the coverage with `DictMutator` increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = FunctionCoverageRunner(my_parser)\n",
    "dict_mutator = DictMutator([\"<a>\",\"</a>\",\"<a/>\", \"='a'\"])\n",
    "dict_fuzzer = GreyboxFuzzer([seed_input], dict_mutator, PowerSchedule())\n",
    "\n",
    "start = time.time()\n",
    "dict_fuzzer.runs(runner, trials = n)\n",
    "end = time.time()\n",
    "\n",
    "\"It took the fuzzer %0.2f seconds to generate and execute %d inputs.\" % (end - start, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly, it takes longer. In our experience, this means more code is covered:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"During this fuzzing campaign, we covered %d statements.\" % len(runner.coverage())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the fuzzers compare in terms of coverage over time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Coverage import population_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, dict_cov = population_coverage(dict_fuzzer.inputs, my_parser)\n",
    "_, fuzz_cov = population_coverage(fuzzer.inputs, my_parser)\n",
    "line_dict, = plt.plot(dict_cov, label=\"With Dictionary\")\n",
    "line_fuzz, = plt.plot(fuzz_cov, label=\"Without Dictionary\")\n",
    "plt.legend(handles=[line_dict, line_fuzz])\n",
    "plt.xlim(0,n)\n",
    "plt.title('Coverage over time')\n",
    "plt.xlabel('# of inputs')\n",
    "plt.ylabel('lines covered');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- \\todo{Andreas: Section on mining keywords using parser-directed fuzzing or AUTOGRAM?} -->\n",
    "\n",
    "***Summary.*** Informing the fuzzer about important keywords already goes a long way towards achieving lots of coverage quickly.\n",
    "\n",
    "***Try it.*** Open this chapter as Jupyter notebook and add other HTML-related keywords to the dictionary in order to see whether the difference in coverage actually increases (given the same budget of 5k generated test inputs).\n",
    "\n",
    "***Read up.*** Michał Zalewski, author of AFL, wrote several great blog posts on [making up grammars with a dictionary in hand](https://lcamtuf.blogspot.com/2015/01/afl-fuzz-making-up-grammar-with.html) and [pulling JPEGs out of thin air](https://lcamtuf.blogspot.com/2014/11/pulling-jpegs-out-of-thin-air.html)!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Fuzzing with Input Fragments\n",
    "\n",
    "While dictionaries are helpful to inject important keywords into seed inputs, they do not allow to maintain the structural integrity of the generated inputs. Instead, we need to make the fuzzer aware of the _input structure_. We can do this using [grammars](Grammars.ipynb). Our first approach \n",
    "\n",
    "1. [parses](Parser.ipynb) the seed inputs, \n",
    "2. disassembles them into input fragments, and \n",
    "3. generates new files by reassembling these fragments according to the rules of the grammar.\n",
    "\n",
    "This combination of _parsing_ and _fuzzing_ can be very powerful, as we will see in an instant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Recombining JavaScript, or How to Make 50,000 USD in Four Weeks\n",
    "\n",
    "In \"Fuzzing with Code Fragments\" \\cite{Holler2012}, Holler, Herzig, and Zeller apply these steps to fuzz a JavaScript interpreter.  They use a JavaScript grammar to\n",
    "\n",
    "1. _parse_ (valid) JavaScript inputs into parse trees,\n",
    "2. _disassemble_ them into fragments (subtrees),\n",
    "3. _recombine_ these fragments into valid JavaScript programs again, and\n",
    "4. _feed_ these programs into a JavaScript interpreter for execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in most fuzzing scenarios, the aim is to cause the JavaScript interpreter to crash.  Here is an example of LangFuzz-generated JavaScript code (from \\cite{Holler2012}) that caused a crash in the Mozilla JavaScript interpreter:\n",
    "\n",
    "```javascript\n",
    "var haystack = \"foo\";\n",
    "var re_text = \"^foo\";\n",
    "haystack += \"x\";\n",
    "re_text += \"(x)\";\n",
    "var re = new RegExp(re_text);\n",
    "re.test(haystack);\n",
    "RegExp.input = Number();\n",
    "print(RegExp.$1);\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a crash of the JavaScript interpreter, it is frequently possible to construct an *exploit* that will not only crash the interpreter, but instead have it execute code under the attacker's control.  Therefore, such crashes are serious flaws, which is why you get a bug bounty if you report them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the first four weeks of running his _LangFuzz_ tool, Christian Holler, first author of that paper, netted _more than USD 50,000 in bug bounties_.  To date, LangFuzz has found more than 2,600 bugs in the JavaScript browsers of Mozilla Firefox, Google Chrome, and Microsoft Edge.  If you use any of these browsers (say, on your Android phone), the combination of parsing and fuzzing has contributed significantly in making your browsing session secure.\n",
    "\n",
    "(Note that these are the same Holler and Zeller who are co-authors of this book.  If you ever wondered why we devote a couple of chapters on grammar-based fuzzing, that's because we have had some great experience with it.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and Recombining HTML\n",
    "\n",
    "In this book, let us stay with HTML input for a while.  To generate valid HTML inputs for our Python `HTMLParser`, we should first define a simple grammar. It allows to define HTML tags with attributes. Our context-free grammar does not require that opening and closing tags must match. However, we will see that such context-sensitive features can be maintained in the derived input fragments, and thus in the generated inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Grammars import is_valid_grammar, srange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XML_TOKENS = {\"<id>\",\"<text>\"}\n",
    "\n",
    "XML_GRAMMAR = {\n",
    "    \"<start>\": [\"<xml-tree>\"],\n",
    "    \"<xml-tree>\": [\"<text>\",\n",
    "                   \"<xml-open-tag><xml-tree><xml-close-tag>\", \n",
    "                   \"<xml-openclose-tag>\", \n",
    "                   \"<xml-tree><xml-tree>\"],\n",
    "    \"<xml-open-tag>\":      [\"<<id>>\", \"<<id> <xml-attribute>>\"],\n",
    "    \"<xml-openclose-tag>\": [\"<<id>/>\", \"<<id> <xml-attribute>/>\"],\n",
    "    \"<xml-close-tag>\":     [\"</<id>>\"],\n",
    "    \"<xml-attribute>\" :    [\"<id>=<id>\", \"<xml-attribute> <xml-attribute>\"],\n",
    "    \"<id>\":                [\"<letter>\", \"<id><letter>\"],\n",
    "    \"<text>\" :             [\"<text><letter_space>\",\"<letter_space>\"],\n",
    "    \"<letter>\":            srange(string.ascii_letters + string.digits +\"\\\"\"+\"'\"+\".\"),\n",
    "    \"<letter_space>\":      srange(string.ascii_letters + string.digits +\"\\\"\"+\"'\"+\" \"+\"\\t\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(XML_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to parse an input into a derivation tree, we use the [Earley parser](Parser.ipynb#Parsing-Context-Free-Grammars)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser import EarleyParser\n",
    "from GrammarFuzzer import display_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run the parser on a simple HTML input and display all possible parse trees. A *parse tree* represents the input structure according to the given grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS)\n",
    "\n",
    "for tree in parser.parse(\"<html>Text</html>\"):\n",
    "    display_tree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the input starts with an opening tag, contains some text, and ends with a closing tag. Excellent. This is a structure that we can work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the Fragment Pool\n",
    "We are now ready to implement our first input-structure-aware mutator. Let's initialize the mutator with the dictionary `fragments` representing the empty fragment pool. It contains a key for each symbol in the grammar (and the empty set as value)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(Mutator):\n",
    "    def __init__(self, parser):\n",
    "        \"\"\"Initialize empty fragment pool and add parser\"\"\"\n",
    "        self.parser = parser\n",
    "        self.fragments = {k: [] for k in self.parser.cgrammar}\n",
    "        super().__init__()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `FragmentMutator` adds fragments recursively. A *fragment* is a subtree in the parse tree and consists of the symbol of the current node and child nodes (i.e., descendant fragments). We can exclude fragments starting with symbols that are tokens, terminals, or not part of the grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Parser import terminals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(FragmentMutator):\n",
    "    def add_fragment(self, fragment):\n",
    "        \"\"\"Recursively adds fragments to the fragment pool\"\"\"\n",
    "        (symbol, children) = fragment\n",
    "        if not self.is_excluded(symbol):\n",
    "            self.fragments[symbol].append(fragment)\n",
    "            for subfragment in children:\n",
    "                self.add_fragment(subfragment)\n",
    "        \n",
    "    def is_excluded(self, symbol):\n",
    "        \"\"\"Returns true if a fragment starting with a specific\n",
    "           symbol and all its decendents can be excluded\"\"\"\n",
    "        return ((not symbol in self.parser.grammar()) or\n",
    "                symbol in self.parser.tokens or\n",
    "                symbol in terminals(self.parser.grammar()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parsing can take a long time, particularly if there is too much ambiguity during the parsing. In order to maintain the efficiency of mutational fuzzing, we will limit the parsing time to 200ms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import signal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Timeout(Exception): pass\n",
    "def timeout(signum, frame): \n",
    "    raise Timeout()\n",
    "\n",
    "# Register timeout() as handler for signal 'SIGALRM'\"\n",
    "signal.signal(signal.SIGALRM, timeout);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function `add_to_fragment_pool()` parses a seed (no longer than 200ms) and adds all its fragments to the fragment pool. If the parsing of the `seed` was successful, the attribute `seed.has_structure` is set to `True`. Otherwise, it is set to `False`.\n",
    "\n",
    "<!-- \\todo{Convert this to `ExpectTimeout` (or make ExpectTimeout more efficient)} -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(FragmentMutator):\n",
    "    def add_to_fragment_pool(self, seed):\n",
    "        \"\"\"Adds all fragments of a seed to the fragment pool\"\"\"\n",
    "        try: # only allow quick parsing of 200ms max\n",
    "            signal.setitimer(signal.ITIMER_REAL, 0.2)\n",
    "            seed.structure = next(self.parser.parse(seed.data))\n",
    "            signal.setitimer(signal.ITIMER_REAL, 0)\n",
    "            \n",
    "            self.add_fragment(seed.structure)\n",
    "            seed.has_structure = True\n",
    "        except (SyntaxError, Timeout):\n",
    "            seed.has_structure = False\n",
    "            signal.setitimer(signal.ITIMER_REAL, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how `FragmentMutator` fills the fragment pool for a simple HTML seed input. We initialize mutator with the `EarleyParser` which itself is initialized with our `XML_GRAMMAR`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from GrammarFuzzer import tree_to_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seed = Seed(\"<html><header><title>Hello</title></header><body>World<br/></body></html>\")\n",
    "fragment_mutator = FragmentMutator(EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS))\n",
    "fragment_mutator.add_to_fragment_pool(valid_seed)\n",
    "\n",
    "for key in fragment_mutator.fragments:\n",
    "    print(key)\n",
    "    for f in fragment_mutator.fragments[key]:\n",
    "        print(\"|-%s\" % tree_to_string(f))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For many symbols in the grammar, we have collected a number of fragments. There are several open and closing tags and several interesting fragments starting with the `xml-tree` symbol.\n",
    "\n",
    "***Summary***. For each interesting symbol in the grammar, the `FragmentMutator` has a set of fragments. These fragments are extracted by first parsing the inputs to be mutated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragment-Based Mutation\n",
    "\n",
    "We can use the fragments in the fragment pool to generate new inputs. Every seed that is being mutated is disassembled into fragments, and memoized – i.e., disassembled only the first time around."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(FragmentMutator):\n",
    "    def __init__(self, parser):\n",
    "        \"\"\"Initialize mutators\"\"\"\n",
    "        super().__init__(parser)\n",
    "        self.seen_seeds = []\n",
    "\n",
    "    def mutate(self, seed):\n",
    "        \"\"\"Implement structure-aware mutation. Memoize seeds.\"\"\"\n",
    "        if not seed in self.seen_seeds:\n",
    "            self.seen_seeds.append(seed)\n",
    "            self.add_to_fragment_pool(seed)\n",
    "        return super().mutate(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our first structural mutation operator is `swap_fragments()`, which choses a random fragment in the given seed and substitutes it with a random fragment from the pool. We make sure that both fragments start with the same symbol. For instance, we may swap a closing tag in the seed HTML by another closing tag from the fragment pool.\n",
    "\n",
    "In order to choose a random fragment, the mutator counts all fragments (`n_count`) below the root fragment associated with the start-symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(FragmentMutator):\n",
    "    def count_nodes(self, fragment):\n",
    "        \"\"\"Returns the number of nodes in the fragment\"\"\"\n",
    "        symbol, children = fragment\n",
    "        if self.is_excluded(symbol):\n",
    "            return 0\n",
    "        return 1 + sum(map(self.count_nodes, children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to swap the chosen fragment – identified using the \"global\" variable `self.to_swap` – the seed's parse tree is traversed recursively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(FragmentMutator):\n",
    "    def recursive_swap(self, fragment):\n",
    "        \"\"\"Recursively finds the fragment to swap.\"\"\"\n",
    "        symbol, children = fragment\n",
    "        if self.is_excluded(symbol):\n",
    "            return symbol, children\n",
    "\n",
    "        self.to_swap -= 1\n",
    "        if self.to_swap == 0: \n",
    "            return random.choice(list(self.fragments[symbol]))\n",
    "        return symbol, list(map(self.recursive_swap, children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our structural mutator chooses a random number between 2 (i.e., excluding the `start` symbol) and the total number of fragments (`n_count`) and uses the recursive swapping to generate the new fragment. The new fragment is serialized as string and returned as new seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(FragmentMutator):\n",
    "    def __init__(self, parser):\n",
    "        super().__init__(parser)\n",
    "        self.mutators = [self.swap_fragment]\n",
    "          \n",
    "    def swap_fragment(self, seed):\n",
    "        \"\"\"Substitutes a random fragment with another with the same symbol\"\"\"\n",
    "        if seed.has_structure:\n",
    "            n_nodes = self.count_nodes(seed.structure)\n",
    "            self.to_swap = random.randint(2, n_nodes)\n",
    "            new_structure = self.recursive_swap(seed.structure)\n",
    "            \n",
    "            new_seed = Seed(tree_to_string(new_structure))\n",
    "            new_seed.has_structure = True\n",
    "            new_seed.structure = new_structure\n",
    "            return new_seed\n",
    "        return seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_seed = Seed(\"<html><header><title>Hello</title></header><body>World<br/></body></html>\")\n",
    "lf_mutator = FragmentMutator(parser)\n",
    "print(valid_seed)\n",
    "lf_mutator.mutate(valid_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, one fragment has been substituted by another. \n",
    "\n",
    "We can use a similar recursive traversal to *remove* a random fragment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(FragmentMutator):\n",
    "    def recursive_delete(self, fragment):\n",
    "        \"\"\"Recursively finds the fragment to delete\"\"\"\n",
    "        symbol, children = fragment\n",
    "        if self.is_excluded(symbol):\n",
    "            return symbol, children\n",
    "\n",
    "        self.to_delete -= 1\n",
    "        if self.to_delete == 0: \n",
    "            return symbol, []\n",
    "        return symbol, list(map(self.recursive_delete, children))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We should also define the corresponding structural deletion operator, as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FragmentMutator(FragmentMutator):\n",
    "    def __init__(self, parser):\n",
    "        super().__init__(parser)\n",
    "        self.mutators.append(self.delete_fragment)\n",
    "    \n",
    "    def delete_fragment(self, seed):\n",
    "        \"\"\"Deletes a random fragment\"\"\"\n",
    "        if seed.has_structure:\n",
    "            n_nodes = self.count_nodes(seed.structure)\n",
    "            self.to_delete = random.randint(2, n_nodes)\n",
    "            new_structure = self.recursive_delete(seed.structure)\n",
    "            \n",
    "            new_seed = Seed(tree_to_string(new_structure))\n",
    "            new_seed.has_structure = True\n",
    "            new_seed.structure = new_structure\n",
    "            # do not return an empty new_seed\n",
    "            if not new_seed.data: return seed\n",
    "            else: return new_seed\n",
    "        return seed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary***. We now have all ingredients for structure-aware fuzzing. Our mutator disassembles all seeds into fragments, which are then added to the fragment pool. Our mutator swaps random fragments in a given seed with fragments of the same type. And our mutator deletes random fragments in a given seed. This allows to maintain a high degree of validity for the generated inputs w.r.t. the given grammar.\n",
    "\n",
    "***Try it***. Try adding other structural mutation operators. How would an *add-operator* know the position in a given seed file, where it is okay to add a fragment starting with a certain symbol?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fragment-Based Fuzzing\n",
    "\n",
    "We can now define a input-structure aware fuzzer as pioneered in LangFuzzer. To implement LangFuzz, we modify our [blackbox mutational fuzzer](GreyboxFuzzer.ipynb#Blackbox-Mutation-based-Fuzzer) to stack up to four structural mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LangFuzzer(MutationFuzzer):\n",
    "    def create_candidate(self):\n",
    "        \"\"\"Returns an input generated by fuzzing a seed in the population\"\"\"\n",
    "        candidate = self.schedule.choose(self.population)\n",
    "        trials = random.randint(1,4)\n",
    "        for i in range(trials):\n",
    "            candidate = self.mutator.mutate(candidate)\n",
    "        return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Okay, let's take our first input-structure aware fuzzer for a spin. Being careful, we set n=300 for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 300\n",
    "runner = FunctionCoverageRunner(my_parser)\n",
    "mutator = FragmentMutator(EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS))\n",
    "schedule = PowerSchedule()\n",
    "\n",
    "langFuzzer = LangFuzzer([valid_seed.data], mutator, schedule)\n",
    "\n",
    "start = time.time()\n",
    "langFuzzer.runs(runner, trials = n)\n",
    "end = time.time()\n",
    "\n",
    "\"It took LangFuzzer %0.2f seconds to generate and execute %d inputs.\" % (end - start, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We observe that structural mutation is *sooo very slow*. This is despite our time budget of 200ms for parsing. In contrast, our blackbox fuzzer alone can generate about 10k inputs per second!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = FunctionCoverageRunner(my_parser)\n",
    "mutator = Mutator()\n",
    "schedule = PowerSchedule()\n",
    "\n",
    "blackFuzzer = MutationFuzzer([valid_seed.data], mutator, schedule)\n",
    "\n",
    "start = time.time()\n",
    "blackFuzzer.runs(runner, trials = n)\n",
    "end = time.time()\n",
    "\n",
    "\"It took a blackbox fuzzer %0.2f seconds to generate and execute %d inputs.\" % (end - start, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Indeed, our blackbox fuzzer is done in the blink of an eye.\n",
    "\n",
    "***Try it***. We can deal with this overhead using [deferred parsing](https://arxiv.org/abs/1811.09447). Instead of wasting time in the beginning of the fuzzing campaign when a byte-level mutator would make efficient progress, deferred parsing suggests to invest time in structural mutation only later in the fuzzing campaign when it becomes viable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blackbox_coverage = len(runner.coverage())\n",
    "\"During this fuzzing campaign, the blackbox fuzzer covered %d statements.\" % blackbox_coverage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's print some stats for our fuzzing campaigns. Since we'll need to print stats more often later, we should wrap this into a function. In order to measure coverage, we import the [population_coverage](Coverage.ipynb#Coverage-of-Basic-Fuzzing) function. It takes a set of inputs and a Python function, executes the inputs on that function and collects coverage information. Specifically, it returns a tuple `(all_coverage, cumulative_coverage)` where `all_coverage` is the set of statements covered by all inputs, and `cumulative_coverage` is the number of statements covered as the number of executed inputs increases. We are just interested in the latter to plot coverage over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Coverage import population_coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_stats(fuzzer, parser):\n",
    "    coverage, _ = population_coverage(fuzzer.inputs, my_parser)\n",
    "    \n",
    "    has_structure = 0\n",
    "    for seed in fuzzer.inputs:\n",
    "        # reuse memoized information\n",
    "        if hasattr(seed, \"has_structure\"):\n",
    "            if seed.has_structure: \n",
    "                has_structure += 1\n",
    "        else:\n",
    "            if isinstance(seed, str):\n",
    "                seed = Seed(seed)\n",
    "            try:\n",
    "                signal.setitimer(signal.ITIMER_REAL, 0.2)\n",
    "                next(parser.parse(seed.data))\n",
    "                signal.setitimer(signal.ITIMER_REAL, 0)\n",
    "                has_structure += 1\n",
    "            except (SyntaxError, Timeout):\n",
    "                signal.setitimer(signal.ITIMER_REAL, 0)\n",
    "        \n",
    "    print(\"From the %d generated inputs, %d (%0.2f%%) can be parsed.\\n\"\n",
    "          \"In total, %d statements are covered.\" % (\n",
    "        len(fuzzer.inputs),\n",
    "        has_structure,\n",
    "        100 * has_structure / len(fuzzer.inputs),\n",
    "        len(coverage)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For LangFuzzer, let's see how many of the inputs generated by LangFuzz are valid (i.e., parsable) and how many statements were covered."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(langFuzzer, EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the stats for the mutational fuzzer that uses only byte-level mutation (and no grammars)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(blackFuzzer, EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary***. Our fragment-level blackbox fuzzer (LangFuzzer) generates *more valid inputs* but achieves *less code coverage* than a fuzzer with our byte-level fuzzer. So, there is some value in generating inputs that do not stick to the provided grammar. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integration with Greybox Fuzzing\n",
    "\n",
    "In the following we integrate fragment-level blackbox fuzzing (LangFuzz-style) with [byte-level greybox fuzzing](GreyboxFuzzer.ipynb#Greybox-Mutation-based-Fuzzer) (AFL-style). The additional coverage-feedback might allow us to increase code coverage more quickly.\n",
    "\n",
    "A [greybox fuzzer](GreyboxFuzzer.ipynb#Greybox-Mutation-based-Fuzzer) adds to the seed population all generated inputs which increase code coverage. Inputs are generated in two stages, stacking up to four structural mutations and up to 32 byte-level mutations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreyboxGrammarFuzzer(GreyboxFuzzer):\n",
    "    def __init__(self, seeds, byte_mutator, tree_mutator, schedule):\n",
    "        super().__init__(seeds, byte_mutator, schedule)\n",
    "        self.tree_mutator = tree_mutator\n",
    "    \n",
    "    def create_candidate(self):\n",
    "        \"\"\"Returns an input generated by structural mutation of a seed in the population\"\"\"\n",
    "        seed = self.schedule.choose(self.population)\n",
    "        \n",
    "        # Structural mutation\n",
    "        trials = random.randint(0,4)\n",
    "        for i in range(trials):\n",
    "            seed = self.tree_mutator.mutate(seed)\n",
    "        \n",
    "        # Byte-level mutation\n",
    "        candidate = seed.data\n",
    "        if trials == 0 or not seed.has_structure or 1 == random.randint(0, 1):\n",
    "            dumb_trials = min(len(seed.data), 1 << random.randint(1,5))\n",
    "            for i in range(dumb_trials):\n",
    "                candidate = self.mutator.mutate(candidate)\n",
    "        return candidate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run our integrated fuzzer with the [standard byte-level mutator](GreyboxFuzzer.ipynb#Mutator-and-Seed) and our [fragment-based structural mutator](#Fragment-based-Mutation) that was introduced above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = FunctionCoverageRunner(my_parser)\n",
    "byte_mutator = Mutator()\n",
    "tree_mutator = FragmentMutator(EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS))\n",
    "schedule = PowerSchedule()\n",
    "\n",
    "gg_fuzzer = GreyboxGrammarFuzzer([valid_seed.data], byte_mutator, tree_mutator, schedule)\n",
    "\n",
    "start = time.time()\n",
    "gg_fuzzer.runs(runner, trials = n)\n",
    "end = time.time()\n",
    "\n",
    "\"It took the greybox grammar fuzzer %0.2f seconds to generate and execute %d inputs.\" % (end - start, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_stats(gg_fuzzer, EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary***. Our structural greybox fuzzer \n",
    "* runs faster than the fragment-based LangFuzzer,\n",
    "* achieves more coverage than both the fragment-based LangFuzzer and the vanilla blackbox mutational fuzzer, and\n",
    "* generates fewer valid inputs than even the vanilla blackbox mutational fuzzer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc-hr-collapsed": false
   },
   "source": [
    "## Mutating Invalid Seeds\n",
    "\n",
    "In the previous section, we have seen that most inputs that are added as seeds are *invalid* w.r.t. our given grammar. Yet, in order to apply our fragment-based mutators, we need it to parse the seed successfully. Otherwise, the entire fragment-based approach becomes useless.  The question arises: *How can we derive structure from (invalid) seeds that cannot be parsed successfully?*\n",
    "\n",
    "To this end, we introduce the idea of _region-based mutation_, first explored with the [AFLSmart](https://github.com/aflsmart/aflsmart) structural greybox fuzzer \\cite{Pham2018aflsmart}. AFLSmart implements byte-level, fragment-based, and region-based mutation as well as validity-based power schedules. We define *region-based mutators*, where a *region* is a consecutive sequence of bytes in the input that can be associated with a symbol in the grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Determining Symbol Regions\n",
    "The function `chart_parse` of the [Earley parser](Parser.ipynb#The-Parsing-Algorithm) produces a parse table for a string. For each letter in the string, this table gives the potential symbol and a *region* of neighboring letters that might belong to the same symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "invalid_seed = Seed(\"<html><body><i>World</i><br/>>/body></html>\")\n",
    "parser = EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS)\n",
    "table = parser.chart_parse(invalid_seed.data, parser.start_symbol())\n",
    "for column in table:\n",
    "    print(column)\n",
    "    print(\"---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of columns in this table that are associated with potential symbols correspond to the number of letters that could be parsed successfully. In other words, we can use this table to compute the longest parsable substring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = [col for col in table if col.states]\n",
    "parsable = invalid_seed.data[:len(cols)-1]\n",
    "\n",
    "print(\"'%s'\" % invalid_seed)\n",
    "parsable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this, we can compute the *degree of validity* for an input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validity = 100 * len(parsable) / len(invalid_seed.data)\n",
    "\n",
    "\"%0.1f%% of the string can be parsed successfully.\" % validity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary***. Unlike input fragments, input regions can be derived even if the parser fails to generate the entire parse tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-based Mutation\n",
    "To fuzz invalid seeds, the region-based mutator associates symbols from the grammar with regions (i.e., indexed substrings) in the seed. The [overridden](#Building-the-Fragment-Pool) method `add_to_fragment_pool()` first tries to mine the fragments from the seed. If this fails, the region mutator uses [Earley parser](Parser.ipynb#The-Parsing-Algorithm) to derive the parse table. For each column (i.e., letter), it extracts the symbols and corresponding regions. This allows the mutator to store the set of regions with each symbol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionMutator(FragmentMutator):\n",
    "    def add_to_fragment_pool(self, seed):\n",
    "        \"\"\"Mark fragments and regions in a seed file\"\"\"\n",
    "        super().add_to_fragment_pool(seed)\n",
    "        if not seed.has_structure:\n",
    "            try:\n",
    "                signal.setitimer(signal.ITIMER_REAL, 0.2) # set 200ms timeout\n",
    "                seed.regions = {k: set() for k in self.parser.cgrammar}\n",
    "                for column in self.parser.chart_parse(seed.data, self.parser.start_symbol()):\n",
    "                    for state in column.states:\n",
    "                        if (not self.is_excluded(state.name) and\n",
    "                                state.e_col.index - state.s_col.index > 1 and\n",
    "                                state.finished()):\n",
    "                            seed.regions[state.name].add((state.s_col.index, state.e_col.index))\n",
    "                signal.setitimer(signal.ITIMER_REAL, 0) # cancel timeout\n",
    "                seed.has_regions = True\n",
    "            except Timeout:\n",
    "                seed.has_regions = False\n",
    "        else:\n",
    "            seed.has_regions = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is how these regions look like for our invalid seed. A region consists of a start and end index in the seed string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mutator = RegionMutator(parser)\n",
    "mutator.add_to_fragment_pool(invalid_seed)\n",
    "for symbol in invalid_seed.regions:\n",
    "    print(symbol)\n",
    "    for (s, e) in invalid_seed.regions[symbol]:\n",
    "        print(\"|-(%d,%d) : %s\" % (s, e, invalid_seed.data[s:e]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know which regions in the seed belong to which symbol, we can define region-based swap and delete operators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionMutator(RegionMutator):\n",
    "    def swap_fragment(self, seed):\n",
    "        \"\"\"Chooses a random region and swaps it with a fragment\n",
    "           that starts with the same symbol\"\"\"\n",
    "        if not seed.has_structure and seed.has_regions:\n",
    "            regions = [r for r in seed.regions\n",
    "                         if (len(seed.regions[r]) > 0 and\n",
    "                            len(self.fragments[r]) > 0)]\n",
    "            if len(regions) == 0: return seed\n",
    "                \n",
    "            key = random.choice(list(regions))\n",
    "            s, e = random.choice(list(seed.regions[key]))\n",
    "            swap_structure = random.choice(self.fragments[key])\n",
    "            swap_string = tree_to_string(swap_structure)\n",
    "            new_seed = Seed(seed.data[:s] + swap_string + seed.data[e:])\n",
    "            new_seed.has_structure = False\n",
    "            new_seed.has_regions = False\n",
    "            return new_seed\n",
    "        else:\n",
    "            return super().swap_fragment(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegionMutator(RegionMutator):\n",
    "    def delete_fragment(self, seed):\n",
    "        \"\"\"Deletes a random region\"\"\"\n",
    "        if not seed.has_structure and seed.has_regions:\n",
    "            regions = [r for r in seed.regions\n",
    "                         if len(seed.regions[r]) > 0]\n",
    "            if len(regions) == 0: return seed\n",
    "\n",
    "            key = random.choice(list(regions))\n",
    "            s, e = (0, 0)\n",
    "            while (e - s < 2):\n",
    "                s, e = random.choice(list(seed.regions[key]))\n",
    "            new_seed = Seed(seed.data[:s] + seed.data[e:])\n",
    "            new_seed.has_structure = False\n",
    "            new_seed.has_regions = False\n",
    "            return new_seed\n",
    "        else:\n",
    "            return super().delete_fragment(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our new region-based mutator. We add a simple, valid seed to the fragment pool and attempt to mutate the invalid seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_seed = Seed(\"<b>Text</b>\")\n",
    "mutator = RegionMutator(parser)\n",
    "mutator.add_to_fragment_pool(simple_seed)\n",
    "\n",
    "print(invalid_seed)\n",
    "mutator.mutate(invalid_seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary***. We can use the Earley parser to generate a parse table and assign regions in the input to symbols in the grammar. Our region mutators can substitute these region with fragments from the fragment pool that start with the same symbol, or delete these regions entirely.\n",
    "\n",
    "***Try it***. Implement a region pool (similar to the fragment pool) and a `swap_region()` mutator.\n",
    "You can execute your own code by opening this chapter as Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Region-Based Fuzzing\n",
    "\n",
    "Let's try our shiny new region mutator by integrating it with our [structure-aware greybox fuzzer](#Integration-with-Greybox-Fuzzing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = FunctionCoverageRunner(my_parser)\n",
    "byte_mutator = Mutator()\n",
    "tree_mutator = RegionMutator(EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS))\n",
    "schedule = PowerSchedule()\n",
    "\n",
    "regionFuzzer = GreyboxGrammarFuzzer([valid_seed.data], byte_mutator, tree_mutator, schedule)\n",
    "\n",
    "start = time.time()\n",
    "regionFuzzer.runs(runner, trials = n)\n",
    "end = time.time()\n",
    "\n",
    "\"It took the structural greybox fuzzer with region mutator\\\n",
    " %0.2f seconds to generate and execute %d inputs.\" % (end - start, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the structural greybox fuzzer with region-based mutator is slower than the [fragment-based mutator alone](#Fragment-based-Fuzzing). This is because region-based structural mutation is applicable for *all seeds*. In contrast, fragment-based mutators were applicable only for tiny number of parsable seeds. Otherwise, only (very efficient) byte-level mutators were applied.\n",
    "\n",
    "Let's also print the average degree of validity for the seeds in the population."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_more_stats(fuzzer, parser):\n",
    "    print_stats(fuzzer, parser)\n",
    "    validity = 0\n",
    "    total = 0\n",
    "    for seed in fuzzer.population:\n",
    "        if not seed.data: continue\n",
    "        table = parser.chart_parse(seed.data, parser.start_symbol())\n",
    "        cols = [col for col in table if col.states]\n",
    "        parsable = invalid_seed.data[:len(cols)-1]\n",
    "        validity += len(parsable) / len(seed.data)\n",
    "        total += 1\n",
    "    print(\"On average, %0.1f%% of a seed in the population can be successfully parsed.\" % (100 * validity / total))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_more_stats(regionFuzzer, parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary***. Compared to fragment-based mutation, a greybox fuzzer with region-based mutation achieves *higher coverage* but generates a *smaller number of valid inputs*. The higher coverage is explained by leveraging at least *some* structure for seeds that cannot be parsed successfully.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Focusing on Valid Seeds\n",
    "\n",
    "In the previous section, we have a problem: The low (degree of) validity.  To address this problem, a _validity-based power schedule_ assigns more [energy](GreyboxFuzzer.ipynb#Power-Schedules) to seeds that have a higher degree of validity.  In other words, the fuzzer _spends more time fuzzing seeds that are more valid_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AFLSmartSchedule(PowerSchedule):\n",
    "    \n",
    "    def __init__(self, parser, exponent):\n",
    "        self.parser = parser\n",
    "        self.exponent = exponent\n",
    "    \n",
    "    def parsable(self, seed):\n",
    "        \"\"\"Returns the substring that is parsable\"\"\"\n",
    "        table = self.parser.chart_parse(seed.data, self.parser.start_symbol())\n",
    "        cols = [col for col in table if col.states]\n",
    "        return seed.data[:len(cols)-1]\n",
    "    \n",
    "    def degree_of_validity(self, seed):\n",
    "        \"\"\"Returns the proportion of a seed that is parsable\"\"\"\n",
    "        if hasattr(seed, \"validity\"): return seed.validity\n",
    "        seed.validity = (len(self.parsable(seed)) / len(seed.data)\n",
    "                         if len(seed.data) > 0 else 0)\n",
    "        return seed.validity\n",
    "    \n",
    "    def assignEnergy(self, population):\n",
    "        \"\"\"Assign exponential energy proportional to degree of validity\"\"\"\n",
    "        for seed in population:\n",
    "            seed.energy = ((self.degree_of_validity(seed) / math.log(len(seed.data))) ** self.exponent\n",
    "                           if len(seed.data) > 1 else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's play with the degree of validity by passing in a valid seed ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smart_schedule = AFLSmartSchedule(parser, 1)\n",
    "print(\"%11s: %s\" % (\"Entire seed\", simple_seed))\n",
    "print(\"%11s: %s\" % (\"Parsable\", smart_schedule.parsable(simple_seed)))\n",
    "\n",
    "\"Degree of validity: %0.2f%%\" % (100 * smart_schedule.degree_of_validity(simple_seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "... and an invalid seed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%11s: %s\" % (\"Entire seed\", invalid_seed))\n",
    "print(\"%11s: %s\" % (\"Parsable\", smart_schedule.parsable(invalid_seed)))\n",
    "\n",
    "\"Degree of validity: %0.2f%%\" % (100 * smart_schedule.degree_of_validity(invalid_seed))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. We can compute the degree of validity as the proportion of the string that can be parsed. \n",
    "\n",
    "Let's plug the validity-based power schedule into the structure-aware greybox fuzzer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runner = FunctionCoverageRunner(my_parser)\n",
    "byte_mutator = Mutator()\n",
    "tree_mutator = RegionMutator(EarleyParser(XML_GRAMMAR, tokens=XML_TOKENS))\n",
    "schedule = AFLSmartSchedule(parser, 1)\n",
    "\n",
    "aflsmart = GreyboxGrammarFuzzer([valid_seed.data], byte_mutator, tree_mutator, schedule)\n",
    "\n",
    "start = time.time()\n",
    "aflsmart.runs(runner, trials = n)\n",
    "end = time.time()\n",
    "\n",
    "\"It took AFLSmart %0.2f seconds to generate and execute %d inputs.\" % (end - start, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_more_stats(aflsmart, parser)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Summary***. Indeed, by spending more time fuzzing seeds with a higher degree of validity, we also generate inputs with a higher degree of validity. More inputs are entirely valid w.r.t. the given grammar.\n",
    "\n",
    "***Read up***. Learn more about region-based fuzzing, deferred parsing, and validity-based schedules in the original AFLSmart paper: \"[Smart Greybox Fuzzing](https://arxiv.org/abs/1811.09447)\" by Pham et al.. Download and improve AFLSmart: [https://github.com/aflsmart/aflsmart](https://github.com/aflsmart/aflsmart)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mining Seeds\n",
    "\n",
    "By now, it should have become clear that the _choice of seeds_ can very much influence the success of fuzzing.  One aspect is _variability_ – our seeds should cover as many different features as possible in order to increase coverage.  Another aspect, however, is the _likelihood of a seed to induce errors_ – that is, if a seed was involved in causing a failure before, then a mutation of this very seed may be likely to induce failures again.  This is because fixes for past failures typically are successful in letting the concrete failure no longer occur, but sometimes may fail to capture all conditions under which a failure may occur.  Hence, even if the original failure is fixed, the likelihood of an error in the _surroundings_ of the original failure-inducing input is still higher.  It thus pays off to use as seeds _inputs that are known to have caused failures before_.\n",
    "\n",
    "To put things in context, Holler's _LangFuzz_ fuzzer used as seeds JavaScript inputs from CVE reports.  These were published as failure-inducing inputs at a time when the error already had been fixed; thus they could do no harm anymore.  Yet, by using such inputs as seeds, LangFuzz would create plenty of mutations and recombinations of all their features, many of which would (and do) find errors again and again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* A **dictionary** is useful to inject important keywords into the generated inputs.\n",
    "\n",
    "* **Fragment-based mutation** first disassembles seeds into fragments, and reassembles these fragments to generate new inputs. A *fragment* is a subtree in the seed's parse tree. However, fragment-based mutation requires that the seeds can be parsed successfully, which may not be true for seeds discovered by a coverage-based greybox fuzzer.\n",
    "\n",
    "* **Region-based mutation** marks regions in the input as belonging to a certain symbol in the grammar. For instance, it may identify a substring '</a>' as closing tag. These regions can then be deleted or substituted by fragments or regions belonging to the same symbol. Unlike fragment-based mutation, region-based mutation is applicable to *all* seeds - even those that can be parsed only partially. However, the degree of validity is still quite low for the generated inputs.\n",
    "\n",
    "* A **validity-based power schedule** invests more energy into seeds with a higher degree of validity. The inputs that are generated also have a higher degree of validity.\n",
    "\n",
    "* **Mining seeds** from repositories of previous failure-inducing inputs results in input fragments associated with past failures, raising the likelihood to find more failures in the vicinity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": false,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Next Steps\n",
    "\n",
    "This chapter closes our discussion of syntactic fuzzing techniques.\n",
    "\n",
    "* In the [next chapter](Reducer.ipynb), we discuss how to _reduce failure-inducing inputs_ after a failure, keeping only those portions of the input that are necessary for reproducing the failure.\n",
    "* The [next part](04_Semantical_Fuzzing.ipynb) will go from syntactical to _semantical_ fuzzing, considering code semantics for targeted test generation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Background\n",
    "\n",
    "This chapter builds on the following two works:\n",
    "\n",
    "* The _LangFuzz_ fuzzer \\cite{Holler2012} is an efficient (and effective!) grammar-based fuzzer for (mostly) JavaScript.  It uses the grammar for parsing seeds and recombining their inputs with generated parts and found 2,600 bugs in JavaScript interpreters to date.\n",
    "\n",
    "* Smart greybox fuzzing ([AFLSmart](https://github.com/aflsmart/aflsmart)) brings together coverage-based fuzzing and grammar-based (structural) fuzzing, as described in \\cite{Pham2018aflsmart}.  The resulting AFLSMART tool has discovered 42 zero-day vulnerabilities in widely-used, well-tested tools and libraries; so far 17 CVEs were assigned.\n",
    "\n",
    "Recent fuzzing work also brings together grammar-based fuzzing and coverage.\n",
    "\n",
    "* _Superion_ \\cite{Wang2019superion} is equivalent to our section \"Integration with Greybox Fuzzing\", as above – that is, a combination of LangFuzz and Greybox Fuzzing, but no AFL-style byte-level mutation.  Superion can improve the code coverage (i.e., 16.7% and 8.8% in line and function coverage) and bug-finding capability over AFL and jsfunfuzz.  According to the authors, they found 30 new bugs, among which they discovered 21 new vulnerabilities with 16 CVEs assigned and 3.2K USD bug bounty rewards received.\n",
    "\n",
    "* _Nautilus_ \\cite{Aschermann2019nautilus} also combines grammar-based fuzzing with coverage feedback.  It maintains the parse tree for all seeds and generated inputs. To allow AFL-style byte-level mutations, it \"collapses\" subtrees back to byte-level representations.  This has the advantage of not having to re-parse generated seeds; however, over time, Nautilus de-generates to structure-unaware greybox fuzzing because it does not re-parse collapsed subtrees to reconstitute input structure for later seeds where most of the parse tree is collapsed.  Nautilus identified bugs in mruby, PHP, ChakraCore, and in Lua; reporting these bugs was awarded with a sum of 2600 USD and 6 CVEs were assigned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "button": false,
    "new_sheet": true,
    "run_control": {
     "read_only": false
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Exercises\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden",
    "solution2_first": true
   },
   "source": [
    "### Exercise 1: The Big Greybox Fuzzer Shoot-Out\n",
    "\n",
    "Use our implementations of greybox techniques and evaluate them on a benchmark.  Which technique (and which sub-technique) has which impact and why?  Also take into account the specific approaches of Superion \\cite{Wang2019superion} and Nautilus \\cite{Aschermann2019nautilus}, possibly even on the benchmarks used by these approaches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    },
    "solution2": "hidden"
   },
   "source": [
    "**Solution.**  To be added by Summer 2019."
   ]
  }
 ],
 "metadata": {
  "ipub": {
   "bibliography": "fuzzingbook.bib",
   "toc": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "toc-autonumbering": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
