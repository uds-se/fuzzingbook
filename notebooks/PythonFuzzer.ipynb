{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Testing Compilers\n",
    "\n",
    "In this chapter, we will make use of [grammars and grammar-based testing](Grammars.ipynb) to systematically generate _program code_ – for instance, to test a compiler or an interpreter. Not very surprisingly, we use _Python_ and the _Python interpreter_ as our domain.\n",
    "\n",
    "We chose Python not only because the rest of the book is also based on Python.\n",
    "Most importantly, Python brings lots of built-in infrastructure we can leverage, especially\n",
    "\n",
    "* _parsers_ that convert Python code into an abstract syntax tree (AST) representation and \n",
    "* _unparsers_ that take an AST and convert it back into Python code.\n",
    "\n",
    "This allows us to leverage grammars that operate on ASTs rather than concrete syntax, greatly reducing complexity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import YouTubeVideo\n",
    "YouTubeVideo('Nr1xbKj_WRQ')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2",
   "metadata": {},
   "source": [
    "**Prerequisites**\n",
    "\n",
    "* You must read the chapter on [Fuzzing with Grammars](Grammars.ipynb) to understand how grammars and grammar-based testing work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "if sys.version_info < (3, 10):\n",
    "    print(\"This code requires Python 3.10 or later\")\n",
    "    sys.exit(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "## Synopsis\n",
    "<!-- Automatically generated. Do not edit. -->\n",
    "\n",
    "To [use the code provided in this chapter](Importing.ipynb), write\n",
    "\n",
    "```python\n",
    ">>> from fuzzingbook.PythonFuzzer import <identifier>\n",
    "```\n",
    "\n",
    "and then make use of the following features.\n",
    "\n",
    "\n",
    "This chapter provides a `PythonFuzzer` class which allows producing arbitrary Python code elements:\n",
    "\n",
    "```python\n",
    ">>> fuzzer = PythonFuzzer()\n",
    ">>> print(fuzzer.fuzz())\n",
    "def U() -> *set():\n",
    "    break\n",
    "    del set()\n",
    "\n",
    "```\n",
    "By default, `PythonFuzzer` produces a _function definition_ – that is, a list of statements as above.\n",
    "You can pass a `start_symbol` argument to state which Python element you'd like to have:\n",
    "\n",
    "```python\n",
    ">>> fuzzer = PythonFuzzer('<While>')\n",
    ">>> print(fuzzer.fuzz())\n",
    "while []:\n",
    "    continue\n",
    "else:\n",
    "    if D:\n",
    "        pass\n",
    "        return\n",
    "    with :\n",
    "        break\n",
    "    set()\n",
    "    return\n",
    "    return\n",
    "\n",
    "```\n",
    "Here is a list of all possible start symbols. Their names reflect the nonterminals from the [Python `ast` module documentation](https://docs.python.org/3/library/ast.html).\n",
    "\n",
    "```python\n",
    ">>> sorted(list(PYTHON_AST_GRAMMAR.keys()))\n",
    "['<Assert>',\n",
    " '<Assign>',\n",
    " '<Attribute>',\n",
    " '<AugAssign>',\n",
    " '<BinOp>',\n",
    " '<BoolOp>',\n",
    " '<Break>',\n",
    " '<Call>',\n",
    " '<Compare>',\n",
    " '<Constant>',\n",
    " '<Continue>',\n",
    " '<Delete>',\n",
    " '<Dict>',\n",
    " '<EmptySet>',\n",
    " '<Expr>',\n",
    " '<For>',\n",
    " '<FunctionDef>',\n",
    " '<If>',\n",
    " '<List>',\n",
    " '<Module>',\n",
    " '<Name>',\n",
    " '<Pass>',\n",
    " '<Return>',\n",
    " '<Set>',\n",
    " '<Slice>',\n",
    " '<Starred>',\n",
    " '<Subscript>',\n",
    " '<Tuple>',\n",
    " '<UnaryOp>',\n",
    " '<While>',\n",
    " '<With>',\n",
    " '<arg>',\n",
    " '<arg_list>',\n",
    " '<args>',\n",
    " '<arguments>',\n",
    " '<bool>',\n",
    " '<boolop>',\n",
    " '<cmpop>',\n",
    " '<cmpop_list>',\n",
    " '<cmpops>',\n",
    " '<digit>',\n",
    " '<digits>',\n",
    " '<expr>',\n",
    " '<expr_list>',\n",
    " '<exprs>',\n",
    " '<float>',\n",
    " '<func>',\n",
    " '<id>',\n",
    " '<id_continue>',\n",
    " '<id_start>',\n",
    " '<identifier>',\n",
    " '<integer>',\n",
    " '<keyword>',\n",
    " '<keyword_list>',\n",
    " '<keywords>',\n",
    " '<kwarg>',\n",
    " '<lhs_Attribute>',\n",
    " '<lhs_List>',\n",
    " '<lhs_Name>',\n",
    " '<lhs_Starred>',\n",
    " '<lhs_Subscript>',\n",
    " '<lhs_Tuple>',\n",
    " '<lhs_expr>',\n",
    " '<lhs_exprs>',\n",
    " '<literal>',\n",
    " '<mod>',\n",
    " '<none>',\n",
    " '<nonempty_expr_list>',\n",
    " '<nonempty_lhs_expr_list>',\n",
    " '<nonempty_stmt_list>',\n",
    " '<nonzerodigit>',\n",
    " '<not_double_quotes>',\n",
    " '<not_single_quotes>',\n",
    " '<operator>',\n",
    " '<returns>',\n",
    " '<start>',\n",
    " '<stmt>',\n",
    " '<stmt_list>',\n",
    " '<stmts>',\n",
    " '<string>',\n",
    " '<type_comment>',\n",
    " '<type_ignore>',\n",
    " '<type_ignore_list>',\n",
    " '<type_ignores>',\n",
    " '<unaryop>',\n",
    " '<vararg>',\n",
    " '<withitem>',\n",
    " '<withitem_list>',\n",
    " '<withitems>']\n",
    "```\n",
    "If you'd like more control over Python code generation, here is what is happening behind the scenes.\n",
    "The EBNF grammar `PYTHON_AST_GRAMMAR` can parse and produce _abstract syntax trees_ for Python.\n",
    "To produce a Python module without `PythonFuzzer`, you would take these steps:\n",
    "\n",
    "**Step 1:** Create a non-EBNF grammar suitable for `ISLaSolver` (or any other grammar fuzzer):\n",
    "\n",
    "```python\n",
    ">>> python_ast_grammar = convert_ebnf_grammar(PYTHON_AST_GRAMMAR)\n",
    "```\n",
    "**Step 2:**  Feed the resulting grammar into a grammar fuzzer such as ISLa:\n",
    "\n",
    "```python\n",
    ">>> solver = ISLaSolver(python_ast_grammar, start_symbol='<FunctionDef>')\n",
    "```\n",
    "**Step 3:**  Have the grammar fuzzer produce a string. This string represents an AST.\n",
    "\n",
    "```python\n",
    ">>> ast_string = str(solver.solve())\n",
    ">>> ast_string\n",
    "\"FunctionDef(name='W', args=arguments(posonlyargs=[], args=[], kwonlyargs=[], kw_defaults=[], defaults=[]), body=[Break()], decorator_list=[])\"\n",
    "```\n",
    "**Step 4:**  Convert the AST into an actual Python AST data structure.\n",
    "\n",
    "```python\n",
    ">>> from ast import *\n",
    ">>> abstract_syntax_tree = eval(ast_string)\n",
    "```\n",
    "**Step 5:** Finally, convert the AST structure back into readable Python code:\n",
    "\n",
    "```python\n",
    ">>> ast.fix_missing_locations(abstract_syntax_tree)\n",
    ">>> print(ast.unparse(abstract_syntax_tree))\n",
    "def W():\n",
    "    break\n",
    "\n",
    "```\n",
    "The chapter has many more applications, including parsing and mutating Python code, evolutionary fuzzing, and more.\n",
    "\n",
    "Here are the details on the `PythonFuzzer` constructor:\n",
    "\n",
    "<p><code>PythonFuzzer(self, start_symbol=None, *, grammar=None, constraint=None, **kw_params) -&gt; None</code></p>\n",
    "<p>Produce Python code. Parameters are:</p>\n",
    "<ul>\n",
    "<li><code>start_symbol</code>: The grammatical entity to be generated (default: <code>&lt;FunctionDef&gt;</code>)</li>\n",
    "<li><code>grammar</code>: The EBNF grammar to be used (default: <code>PYTHON_PYTHON_AST_GRAMMAR</code>); and</li>\n",
    "<li><code>constraint</code> an ISLa constraint (if any).</li>\n",
    "</ul>\n",
    "<p>Additional keyword parameters are passed to the <code>ISLaSolver</code> superclass.</p>\n",
    "\n",
    "![](PICS/PythonFuzzer-synopsis-1.svg)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6",
   "metadata": {},
   "source": [
    "## A Grammar for Concrete Code\n",
    "\n",
    "To _produce_ code, it is fairly easy to write a grammar with _concrete_ syntax. If we want to produce, say, arithmetic expressions, we can easily create a concrete grammar which does precisely that.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import bookutils.setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Grammars import Grammar\n",
    "from Grammars import is_valid_grammar, convert_ebnf_grammar, extend_grammar, trim_grammar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "We use the [Fuzzingbook format for grammars](https://www.fuzzingbook.org/html/Grammars.html), in which grammars are represented as dictionaries from symbols to lists of expansion alternatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPR_GRAMMAR: Grammar = {\n",
    "    \"<start>\":\n",
    "        [\"<expr>\"],\n",
    "\n",
    "    \"<expr>\":\n",
    "        [\"<term> + <expr>\", \"<term> - <expr>\", \"<term>\"],\n",
    "\n",
    "    \"<term>\":\n",
    "        [\"<factor> * <term>\", \"<factor> / <term>\", \"<factor>\"],\n",
    "\n",
    "    \"<factor>\":\n",
    "        [\"+<factor>\",\n",
    "         \"-<factor>\",\n",
    "         \"(<expr>)\",\n",
    "         \"<integer>.<integer>\",\n",
    "         \"<integer>\"],\n",
    "\n",
    "    \"<integer>\":\n",
    "        [\"<digit><integer>\", \"<digit>\"],\n",
    "\n",
    "    \"<digit>\":\n",
    "        [\"0\", \"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(EXPR_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13",
   "metadata": {},
   "source": [
    "We can use this grammar to produce syntactically valid arithmetic expressions.\n",
    "We use the [ISLa solver](FuzzingWithConstraints.ipynb) as our generator, as it is the most powerful; but we could also use any other of our grammar fuzzers such as [GrammarFuzzer](GrammarFuzzer.ipynb) at this point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from isla.solver import ISLaSolver  # type: ignore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Here are some concrete inputs produced from the grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_solver = ISLaSolver(EXPR_GRAMMAR)\n",
    "for _ in range(10):\n",
    "    print(expr_solver.solve())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We could extend the grammar further to also produce assignments and other statements, and piece by piece cover the entire syntax of the programming language. However, this would be a not-so-great idea. Why?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18",
   "metadata": {},
   "source": [
    "The problem is that when testing _compilers_, you not only want to be able to _produce_ code, but also to _parse_ code, such that you can mutate and manipulate it at will. And this is where our \"concrete\" syntax will give us problems. While we can easily parse code (or expressions) that exactly adheres to the syntax..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_solver.check('2 + 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "... a single space will already suffice to make it fail..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_solver.check('2 +  2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "... as does the absence of spaces:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_solver.check('2+2')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24",
   "metadata": {},
   "source": [
    "Indeed, spaces are optional in most programming languages. We _could_ update our grammar such that it can handle optional spaces at all times (introducing a `<space>` nonterminal). But then, there are other features like _comments_..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_solver.check('2 + 2    # should be 4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26",
   "metadata": {},
   "source": [
    "... or _continuation lines_ ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_solver.check('2 + \\\\\\n2')  # An expression split over two lines"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28",
   "metadata": {},
   "source": [
    "that our grammar would have to cover.\n",
    "\n",
    "On top, there are language features that cannot be even represented properly in a context-free grammar:\n",
    "\n",
    "* In the C programming language, for instance, the parser needs to know whether an identifier has been defined as a _type_\n",
    "* In Python, _indentation_ levels cannot be represented by a context-free grammar."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "For this reason, it is often a good idea to make use of a dedicated _parser_ (or _preprocessor_) to turn input into a more _abstract_ representation - typically a _tree_ structure. In programming languages, such a tree is called an _abstract syntax tree_ (AST); it is the data structure that compilers operate on."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "## Abstract Syntax Trees\n",
    "\n",
    "Abstract Syntax Trees (ASTs) that represent program code are among the most complex data structures in the world (if not _the_ most complex data structures) - notably because they reflect all the complexity of the programming language and its features.\n",
    "The good news is that in Python, working with ASTs is particularly easy - one can work with them using standard language features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "Let us illustrate ASTs using an example. Here is a piece of code that we'd like to work with:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    print(\"Hello, world!\")  # A simple example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "Let us obtain the source code of this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_source = inspect.getsource(main)\n",
    "print(main_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37",
   "metadata": {},
   "source": [
    "We make use of the [Python AST module](https://docs.python.org/3/library/ast.html) to convert this code string to an AST and back."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ast"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39",
   "metadata": {},
   "source": [
    "With `ast.parse()`, we can parse the `main()` source into an AST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "main_tree = ast.parse(main_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41",
   "metadata": {},
   "source": [
    "This is what this tree looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import show_ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ast(main_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44",
   "metadata": {},
   "source": [
    "We see how the function definition has become a `FunctionDef` node, whose third child is an `Expr` node, which in turn becomes a `Call` – of the `\"print\"` function with an argument of `\"Hello, world!\"`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "Each of these AST nodes comes as a _constructor_ – that is, we can invoke `FunctionDef()` to obtain a function definition node, or `Call()` to obtain a call node.\n",
    "These constructors take the AST _children_ as arguments, but also lots of _optional_ arguments (which we did not use so far). The _dump_ of the AST into a string reveals all the arguments for each constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.dump(main_tree, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47",
   "metadata": {},
   "source": [
    "The [Python ast documentation](https://docs.python.org/3/library/ast.html) lists all these constructors, which make up the abstract syntax. There are more than 100 individual constructors! (We said that ASTs are complex, right?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48",
   "metadata": {},
   "source": [
    "The nice thing about the above string representation is that we can take it _as is_ and turn it into a tree again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_main_tree = Module(\n",
    "    body=[\n",
    "        FunctionDef(\n",
    "            name='main',\n",
    "            args=arguments(\n",
    "                posonlyargs=[],\n",
    "                args=[],\n",
    "                kwonlyargs=[],\n",
    "                kw_defaults=[],\n",
    "                defaults=[]),\n",
    "            body=[\n",
    "                Expr(\n",
    "                    value=Call(\n",
    "                        func=Name(id='print', ctx=Load()),\n",
    "                        args=[\n",
    "                            Constant(value='Hello, world!')],\n",
    "                        keywords=[]))],\n",
    "            decorator_list=[])],\n",
    "    type_ignores=[])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51",
   "metadata": {},
   "source": [
    "We can take this tree and compile it into executable code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_main_tree = fix_missing_locations(my_main_tree)  # required for trees built from constructors\n",
    "my_main_code = compile(my_main_tree, filename='<unknown>', mode='exec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53",
   "metadata": {},
   "outputs": [],
   "source": [
    "del main  # This deletes the definition of main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54",
   "metadata": {},
   "outputs": [],
   "source": [
    "exec(my_main_code)  # This defines main() again from `code`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55",
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56",
   "metadata": {},
   "source": [
    "We can also _unparse_ the tree (= turn it into source code again). (Note how the comment got lost during parsing.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.unparse(my_main_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58",
   "metadata": {},
   "source": [
    "Hence, we can\n",
    "\n",
    "1. _Parse_ concrete code into ASTs (with `ast.parse()`)\n",
    "2. _Generate_ new ASTs and _mutate_ existing ones\n",
    "3. _Unparse_ ASTs to obtain concrete code again (with `ast.unparse()`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59",
   "metadata": {},
   "source": [
    "To _generate_ and _mutate_ ASTs (step #2, above), we need means to produce _correct_ ASTs, invoking all constructors with the correct arguments.\n",
    "The plan is thus to have a _grammar_ for ASTs, which produces (and parses) ASTs as we like."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60",
   "metadata": {},
   "source": [
    "## A Grammar for ASTs\n",
    "\n",
    "Programming language grammars are among the most complicated formal grammars around, and ASTs reflect much of this complexity. We will use the [abstract AST grammar](https://docs.python.org/3/library/ast.html) as specified in the Python documentation as base, and build a formal context-free grammar step by step."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61",
   "metadata": {},
   "source": [
    "### Constants\n",
    "\n",
    "We will start with simple constants – strings and integers. Again, we use the `fuzzingbook` syntax for grammars, as it allows for easier extension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANYTHING_BUT_DOUBLE_QUOTES_AND_BACKSLASH = (string.digits + string.ascii_letters + string.punctuation + ' ').replace('\"', '').replace('\\\\', '')\n",
    "ANYTHING_BUT_SINGLE_QUOTES_AND_BACKSLASH = (string.digits + string.ascii_letters + string.punctuation + ' ').replace(\"'\", '').replace('\\\\', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANYTHING_BUT_DOUBLE_QUOTES_AND_BACKSLASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65",
   "metadata": {},
   "outputs": [],
   "source": [
    "ANYTHING_BUT_SINGLE_QUOTES_AND_BACKSLASH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_CONSTANTS_GRAMMAR: Grammar = {\n",
    "    '<start>': [ '<expr>' ],\n",
    "\n",
    "    # Expressions\n",
    "    '<expr>': [ '<Constant>', '<Expr>' ],\n",
    "    '<Expr>': [ 'Expr(value=<expr>)' ],\n",
    "\n",
    "    # Constants\n",
    "    '<Constant>': [ 'Constant(value=<literal>)' ],\n",
    "    '<literal>': [ '<string>', '<integer>', '<float>', '<bool>', '<none>' ],\n",
    "\n",
    "    # Strings\n",
    "    '<string>': [ '\"<not_double_quotes>*\"', \"'<not_single_quotes>*'\" ],\n",
    "    '<not_double_quotes>': list(ANYTHING_BUT_DOUBLE_QUOTES_AND_BACKSLASH),\n",
    "    '<not_single_quotes>': list(ANYTHING_BUT_SINGLE_QUOTES_AND_BACKSLASH),\n",
    "    # FIXME: The actual rules for Python strings are also more complex:\n",
    "    # https://docs.python.org/3/reference/lexical_analysis.html#numeric-literals\n",
    "\n",
    "    # Numbers\n",
    "    '<integer>': [ '<digit>', '<nonzerodigit><digits>' ],\n",
    "    '<float>': [ '<integer>.<integer>' ],\n",
    "    '<nonzerodigit>': ['1', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
    "    '<digits>': [ '<digit><digits>', '<digit>' ],\n",
    "    '<digit>': ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9'],\n",
    "    # FIXME: There are _many_ more ways to express numbers in Python; see\n",
    "    # https://docs.python.org/3/reference/lexical_analysis.html#numeric-literals\n",
    "\n",
    "    # More\n",
    "    '<bool>': [ 'True', 'False' ],\n",
    "    '<none>': [ 'None' ],\n",
    "\n",
    "    # FIXME: Not supported: bytes, format strings, regex strings...\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67",
   "metadata": {},
   "source": [
    "Note that we use _extended Backus-Naur form_ in our grammars (here: `<string>`):\n",
    "\n",
    "* `<elem>+` stands for one or more instances of `<elem>`;\n",
    "* `<elem>*` stands for zero or more instances of `<elem>`;\n",
    "* `<elem>?` stands for one or zero instances of `<elem>`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68",
   "metadata": {},
   "source": [
    "A call to `is_valid_grammar()` ensures our grammar is free of common mistakes. Don't write grammars without it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_CONSTANTS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70",
   "metadata": {},
   "outputs": [],
   "source": [
    "constants_grammar = convert_ebnf_grammar(PYTHON_AST_CONSTANTS_GRAMMAR)\n",
    "constants_solver = ISLaSolver(constants_grammar)\n",
    "constants_tree_str = str(constants_solver.solve())\n",
    "print(constants_tree_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71",
   "metadata": {},
   "source": [
    "We can create an AST from this expression and turn it into Python code (well, a literal):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72",
   "metadata": {},
   "outputs": [],
   "source": [
    "constants_tree = eval(constants_tree_str)\n",
    "ast.unparse(constants_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73",
   "metadata": {},
   "source": [
    "Let's do this a number of times:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_samples(grammar: Grammar, iterations: int = 10, start_symbol = None, log: bool = True):\n",
    "    g = convert_ebnf_grammar(grammar)\n",
    "    solver = ISLaSolver(g, start_symbol=start_symbol, max_number_free_instantiations=iterations)\n",
    "    for i in range(iterations):\n",
    "        tree_str = str(solver.solve())\n",
    "        tree = eval(tree_str)\n",
    "        ast.fix_missing_locations(tree)\n",
    "        if log:\n",
    "            code = ast.unparse(tree)\n",
    "            print(f'{code:40} # {tree_str}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_samples(PYTHON_AST_CONSTANTS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76",
   "metadata": {},
   "source": [
    "Our grammar can also _parse_ ASTs obtained from concrete code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_constant_code = \"4711\"\n",
    "sample_constant_ast = ast.parse(sample_constant_code).body[0]  # get the `Expr` node\n",
    "sample_constant_ast_str = ast.dump(sample_constant_ast)\n",
    "print(sample_constant_ast_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78",
   "metadata": {},
   "outputs": [],
   "source": [
    "constant_solver = ISLaSolver(constants_grammar)\n",
    "constant_solver.check(sample_constant_ast_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79",
   "metadata": {},
   "source": [
    "Let us now come up with a quiz question: _Does our grammar support negative numbers?_\n",
    "For this, let's first find out if the `Constant()` constructor also take a _negative_ number as an argument? It turns out it can:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.unparse(Constant(value=-1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81",
   "metadata": {},
   "source": [
    "But what happens if we parse a negative number, say `-1`? One might assume that this simply results in a `Constant(-1)`, right? Try it out yourself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bookutils import quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "quiz(\"If we parse a negative number, do we obtain \",\n",
    "    [\n",
    "        \"a `Constant()` with a negative value, or\",\n",
    "        \"a unary `-` operator applied to a positive value?\"\n",
    "    ], 1 ** 0 + 1 ** 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84",
   "metadata": {},
   "source": [
    "The answer is that parsing `-1` yields a unary minus `USub()` applied to a positive value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.dump(ast.parse('-1')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86",
   "metadata": {},
   "source": [
    "As unary operators are not part of our grammar (yet), it cannot handle negative numbers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_constant_code = \"-1\"\n",
    "sample_constant_ast = ast.parse(sample_constant_code).body[0]  # get the `Expr` node\n",
    "sample_constant_ast_str = ast.dump(sample_constant_ast)\n",
    "constant_solver = ISLaSolver(constants_grammar)\n",
    "constant_solver.check(sample_constant_ast_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88",
   "metadata": {},
   "source": [
    "In the next sections, we will gradually expand our grammar with more and more Python features, eventually covering (almost) the entire language."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89",
   "metadata": {},
   "source": [
    "### Excursion: Composites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90",
   "metadata": {},
   "source": [
    "Let us add composite constants – lists, dictionaries, tuples, etc. Here is how these are represented in an AST:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.dump(ast.parse(\"{ 'a': set() }\"), indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92",
   "metadata": {},
   "source": [
    "Let us encode these into a grammar, again using the definitions from the [abstract AST grammar](https://docs.python.org/3/library/ast.html).\n",
    "All these structures also take _contexts_ in which identifiers are used – `Load()` if they are used for evaluation, `Store()` if they appear on the left-hand side of an assignment (yes, in Python, you can have a tuple on the left-hand side of an assignment, say `(x, y) = (1, 2)`), and `Del()` if they are used as operands in a `del` statement. Right now, we only use `Load()` and `Del()` interchangeably."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_COMPOSITES_GRAMMAR: Grammar = extend_grammar(\n",
    "    PYTHON_AST_CONSTANTS_GRAMMAR, {\n",
    "    '<expr>': PYTHON_AST_CONSTANTS_GRAMMAR['<expr>'] + [\n",
    "        '<Dict>', '<Set>', '<List>', '<Tuple>'\n",
    "    ],\n",
    "\n",
    "    '<Dict>': [ 'Dict(keys=<expr_list>, values=<expr_list>)' ],\n",
    "    '<Set>': [ 'Set(elts=<nonempty_expr_list>)', '<EmptySet>' ],\n",
    "    '<EmptySet>': [ 'Call(func=Name(id=\"set\", ctx=Load()), args=[], keywords=[])' ],\n",
    "    '<List>': [\n",
    "        'List(elts=<expr_list>, ctx=Load())',\n",
    "        'List(elts=<expr_list>, ctx=Del())',\n",
    "    ],\n",
    "    '<Tuple>': [\n",
    "        'Tuple(elts=<expr_list>, ctx=Load())',\n",
    "        'Tuple(elts=<expr_list>, ctx=Del())',\n",
    "    ],\n",
    "\n",
    "    # Lists of expressions\n",
    "    '<expr_list>': [ '[<exprs>?]' ],\n",
    "    '<nonempty_expr_list>': [ '[<exprs>]' ],\n",
    "    '<exprs>': [ '<expr>', '<exprs>, <expr>' ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_COMPOSITES_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in [ '<Constant>', '<Dict>', '<Set>', '<List>', '<Tuple>' ]:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_COMPOSITES_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96",
   "metadata": {},
   "source": [
    "You may encounter a number of uncommon expressions here. For instance:\n",
    "\n",
    "1. `()` is an empty tuple.\n",
    "2. `(1,)` is a tuple with one element.\n",
    "3. `{}` is an empty dictionary; `{1}` is a set with one element.\n",
    "4. An empty set is denoted by `set()`.\n",
    "\n",
    "The fact that we use `set()` to represent empty sets is actually a feature of our `PYTHON_AST_COMPOSITES_GRAMMAR` grammar.\n",
    "If we invoke the `Set()` AST constructor without any elements, we obtain this beautiful expression..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.unparse(Set(elts=[])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98",
   "metadata": {},
   "source": [
    "... which indeed evaluates into an empty set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99",
   "metadata": {},
   "outputs": [],
   "source": [
    "{*()}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100",
   "metadata": {},
   "source": [
    "Technically speaking, all of this is correct, but we'd like to stick to (somewhat) more readable code. If you want to confuse your programmer friends, always use `{*()}` instead of `set()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "101",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "102",
   "metadata": {},
   "source": [
    "### Excursion: Expressions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103",
   "metadata": {},
   "source": [
    "Let us extend our grammar with _expressions_. The Python parser already takes care of precedence rules, so we can treat all unary and binary operators in a similar fashion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "104",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.dump(ast.parse(\"2 + 2 is not False\"), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_EXPRS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_COMPOSITES_GRAMMAR, {\n",
    "    '<expr>': PYTHON_AST_COMPOSITES_GRAMMAR['<expr>'] + [\n",
    "        '<BoolOp>', '<BinOp>', '<UnaryOp>', '<Compare>',\n",
    "    ],\n",
    "\n",
    "    # Booleans: and or\n",
    "    '<BoolOp>': [ 'BoolOp(op=<boolop>, values=<expr_list>)' ],\n",
    "    '<boolop>': [ 'And()', 'Or()' ],\n",
    "\n",
    "    # Binary operators: + - * ...\n",
    "    '<BinOp>': [ 'BinOp(left=<expr>, op=<operator>, right=<expr>)' ],\n",
    "    '<operator>': [ 'Add()', 'Sub()', 'Mult()', 'MatMult()',\n",
    "                   'Div()', 'Mod()', 'Pow()',\n",
    "                   'LShift()', 'RShift()', 'BitOr()', 'BitXor()', 'BitAnd()',\n",
    "                   'FloorDiv()' ],\n",
    "\n",
    "    # Unary operators: not + - ...\n",
    "    '<UnaryOp>': [ 'UnaryOp(op=<unaryop>, operand=<expr>)'],\n",
    "    '<unaryop>': [ 'Invert()', 'Not()', 'UAdd()', 'USub()' ],\n",
    "\n",
    "    # Comparisons: == != < <= > >= is in ...\n",
    "    '<Compare>': [ 'Compare(left=<expr>, ops=<cmpop_list>, comparators=<expr_list>)'],\n",
    "    '<cmpop_list>': [ '[<cmpops>?]' ],\n",
    "    '<cmpops>': [ '<cmpop>', '<cmpop>, <cmpops>' ],\n",
    "    '<cmpop>': [ 'Eq()', 'NotEq()', 'Lt()', 'LtE()', 'Gt()', 'GtE()',\n",
    "                 'Is()', 'IsNot()', 'In()', 'NotIn()' ],\n",
    "\n",
    "    # FIXME: There's a few more expressions: GeneratorExp, Await, YieldFrom, ...\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_EXPRS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "107",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in [ '<BoolOp>', '<BinOp>', '<UnaryOp>', '<Compare>' ]:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_EXPRS_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108",
   "metadata": {},
   "source": [
    "Not all of these expressions are _type-correct_. For instance, `set() * set()` raises a type error at runtime. They _can_ be properly parsed, though."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109",
   "metadata": {},
   "source": [
    "How good is our grammar at this point? Let us create 20 expressions and check how many of these\n",
    "1. parse without `SyntaxError`\n",
    "2. evaluate without `TypeError`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110",
   "metadata": {},
   "outputs": [],
   "source": [
    "expr_iterations = 20\n",
    "bad_syntax = 0\n",
    "bad_type = 0\n",
    "ast_exprs_grammar = convert_ebnf_grammar(PYTHON_AST_EXPRS_GRAMMAR)\n",
    "expr_solver = ISLaSolver(ast_exprs_grammar, max_number_free_instantiations=expr_iterations)\n",
    "for i in range(expr_iterations):\n",
    "    expr_tree = eval(str(expr_solver.solve()))\n",
    "    expr_tree = fix_missing_locations(expr_tree)\n",
    "    expr_str = ast.unparse(expr_tree)\n",
    "    print(i, expr_str)\n",
    "    try:\n",
    "        ...  # insert parsing code here\n",
    "    except SyntaxError:\n",
    "        bad_syntax += 1\n",
    "    except TypeError:\n",
    "        bad_type += 1\n",
    "\n",
    "    try:\n",
    "        ...  # <-- insert evaluation code here\n",
    "    except TypeError:\n",
    "        bad_type += 1\n",
    "    except SyntaxError:\n",
    "        bad_syntax += 1\n",
    "\n",
    "print(f\"Bad syntax: {bad_syntax}/{expr_iterations}\")\n",
    "print(f\"Bad type: {bad_type}/{expr_iterations}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111",
   "metadata": {},
   "source": [
    "We're not doing too bad here.\n",
    "It is possible, in principle, to use ISLa constraints such that the resulting code will be properly typed - but this would take hundreds to thousands of rules. We will leave this exercise to the reader."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112",
   "metadata": {},
   "source": [
    "Note that you should _not_ repeat this experiment once _identifiers_ come into play. There is a remote chance that the fuzzer synthesizes a call like `os.remove(\"/\")` – and away goes your file system!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "114",
   "metadata": {},
   "source": [
    "### Excursion: Names and Function Calls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "115",
   "metadata": {},
   "source": [
    "\n",
    "Let us add some _identifiers_ such that we can call _functions_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "116",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_START = string.ascii_letters + '_'\n",
    "ID_CONTINUE = ID_START + string.digits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117",
   "metadata": {},
   "outputs": [],
   "source": [
    "ID_CONTINUE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "118",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.dump(ast.parse(\"xyzzy(a, b=c)\"), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_IDS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_EXPRS_GRAMMAR, {\n",
    "    '<expr>': PYTHON_AST_EXPRS_GRAMMAR['<expr>'] + [\n",
    "        '<Name>', '<Call>'\n",
    "    ],\n",
    "\n",
    "    # Identifiers\n",
    "    '<Name>': [\n",
    "        'Name(id=<identifier>, ctx=Load())',\n",
    "        'Name(id=<identifier>, ctx=Del())'\n",
    "    ],\n",
    "    '<identifier>': [ \"'<id>'\" ],\n",
    "    '<id>': [ '<id_start><id_continue>*' ],\n",
    "    '<id_start>': list(ID_START),\n",
    "    '<id_continue>': list(ID_CONTINUE),\n",
    "    # FIXME: Actual rules are a bit more complex; see\n",
    "    # https://docs.python.org/3/reference/lexical_analysis.html#identifiers\n",
    "\n",
    "    # Function Calls\n",
    "    '<Call>': [ 'Call(func=<func><args_param><keywords_param>)' ],\n",
    "    '<args_param>': [ ', args=<expr_list>' ],\n",
    "    '<keywords_param>': [ ', keywords=<keyword_list>' ],\n",
    "    '<func>': [ '<expr>' ],  # Actually <Expr>, but this is more readable and parses 90%\n",
    "    '<keyword_list>': [ '[<keywords>?]' ],\n",
    "    '<keywords>': [ '<keyword>', '<keyword>, <keywords>' ],\n",
    "    '<keyword>': [ 'keyword(arg=<identifier>, value=<expr>)' ]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do import this unconditionally\n",
    "if sys.version_info >= (3, 13):\n",
    "    PYTHON_AST_IDS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_IDS_GRAMMAR, {\n",
    "        # As of 3.13, args and keywords parameters are optional\n",
    "        '<Call>': [ 'Call(func=<func><args_param>?<keywords_param>?)' ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "121",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_IDS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "122",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in [ '<Name>', '<Call>' ]:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_IDS_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_ids_grammar = convert_ebnf_grammar(PYTHON_AST_IDS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124",
   "metadata": {},
   "outputs": [],
   "source": [
    "id_solver = ISLaSolver(ast_ids_grammar, start_symbol='<id>')\n",
    "assert id_solver.check('open')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_solver = ISLaSolver(ast_ids_grammar)\n",
    "assert name_solver.check(\"Name(id='open', ctx=Load())\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_solver = ISLaSolver(ast_ids_grammar, start_symbol='<keyword_list>')\n",
    "assert call_solver.check('[]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "127",
   "metadata": {},
   "outputs": [],
   "source": [
    "call_str = ast.dump(ast.parse('open(\"foo.txt\", \"r\")').body[0].value)  # type: ignore\n",
    "print(call_str)\n",
    "call_solver = ISLaSolver(ast_ids_grammar)\n",
    "assert call_solver.check(call_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "128",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "129",
   "metadata": {},
   "source": [
    "### Excursion: Attributes and Subscripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "130",
   "metadata": {},
   "source": [
    "\n",
    "Let us add attributes and subscripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "131",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.dump(ast.parse(\"a[b].c\"), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "132",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_ATTRS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_IDS_GRAMMAR, {\n",
    "    '<expr>': PYTHON_AST_IDS_GRAMMAR['<expr>'] + [\n",
    "        '<Attribute>', '<Subscript>', '<Starred>',\n",
    "    ],\n",
    "\n",
    "    # Attributes\n",
    "    '<Attribute>': [\n",
    "        'Attribute(value=<expr>, attr=<identifier>, ctx=Load())',\n",
    "        'Attribute(value=<expr>, attr=<identifier>, ctx=Del())',\n",
    "    ],\n",
    "\n",
    "    # Subscripts\n",
    "    '<Subscript>': [\n",
    "        'Subscript(value=<expr>, slice=<Slice>, ctx=Load())',\n",
    "        'Subscript(value=<expr>, slice=<Slice>, ctx=Del())',\n",
    "    ],\n",
    "    '<Slice>': [\n",
    "        'Slice()',\n",
    "        'Slice(<expr>)',\n",
    "        'Slice(<expr>, <expr>)',\n",
    "        'Slice(<expr>, <expr>, <expr>)',\n",
    "    ],\n",
    "\n",
    "    # Starred\n",
    "    '<Starred>': [\n",
    "        'Starred(value=<expr>, ctx=Load())',\n",
    "        'Starred(value=<expr>, ctx=Del())',\n",
    "    ],\n",
    "\n",
    "    # We're extending the set of callers a bit\n",
    "    '<func>': [ '<Name>', '<Attribute>', '<Subscript>' ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "133",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_ATTRS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in [ '<Attribute>', '<Subscript>', '<Starred>' ]:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_ATTRS_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136",
   "metadata": {},
   "source": [
    "### Excursion: Variable Assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137",
   "metadata": {},
   "source": [
    "\n",
    "Now for variable assignments. These make things more complex, as we have a restricted set of expressions on the left hand side of an assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "138",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_ASSIGNMENTS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_ATTRS_GRAMMAR, {\n",
    "    '<start>': [ '<stmt>' ],\n",
    "\n",
    "    '<stmt>': [\n",
    "        '<Assign>', '<AugAssign>',\n",
    "        '<Expr>'\n",
    "    ],\n",
    "\n",
    "    # Assignments\n",
    "    '<Assign>': [\n",
    "        'Assign(targets=<nonempty_lhs_expr_list>, value=<expr><type_comment>?)',\n",
    "    ],\n",
    "    '<type_comment>': [ ', type_comment=<string>' ],\n",
    "    '<AugAssign>': [\n",
    "        'AugAssign(target=<lhs_expr>, op=<operator>, value=<expr>)',\n",
    "    ],\n",
    "\n",
    "    # Lists of left-hand side expressions\n",
    "    # '<lhs_expr_list>': [ '[<lhs_exprs>?]' ],\n",
    "    '<nonempty_lhs_expr_list>': [ '[<lhs_exprs>]' ],\n",
    "    '<lhs_exprs>': [ '<lhs_expr>', '<lhs_exprs>, <lhs_expr>' ],\n",
    "\n",
    "    # On the left-hand side of assignments, we allow a number of structures\n",
    "    '<lhs_expr>': [\n",
    "        '<lhs_Name>',  # Most common\n",
    "        '<lhs_List>', '<lhs_Tuple>',\n",
    "        '<lhs_Attribute>',\n",
    "        '<lhs_Subscript>',\n",
    "        '<lhs_Starred>',\n",
    "    ],\n",
    "\n",
    "    '<lhs_Name>': [ 'Name(id=<identifier>, ctx=Store())', ],\n",
    "\n",
    "    '<lhs_List>': [\n",
    "        'List(elts=<nonempty_lhs_expr_list>, ctx=Store())',\n",
    "    ],\n",
    "    '<lhs_Tuple>': [\n",
    "        'Tuple(elts=<nonempty_lhs_expr_list>, ctx=Store())',\n",
    "    ],\n",
    "    '<lhs_Attribute>': [\n",
    "        'Attribute(value=<lhs_expr>, attr=<identifier>, ctx=Store())',\n",
    "    ],\n",
    "    '<lhs_Subscript>': [\n",
    "        'Subscript(value=<lhs_expr>, slice=<Slice>, ctx=Store())',\n",
    "    ],\n",
    "    '<lhs_Starred>': [\n",
    "        'Starred(value=<lhs_expr>, ctx=Store())',\n",
    "    ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_ASSIGNMENTS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in ['<Assign>', '<AugAssign>']:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_ASSIGNMENTS_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "141",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "142",
   "metadata": {},
   "source": [
    "### Excursion: Statements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "143",
   "metadata": {},
   "source": [
    "\n",
    "Now for statements. There's quite a lot of these."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_STMTS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_ASSIGNMENTS_GRAMMAR, {\n",
    "    '<start>': [ '<stmt>' ],\n",
    "\n",
    "    '<stmt>': PYTHON_AST_ASSIGNMENTS_GRAMMAR['<stmt>'] + [\n",
    "        '<For>', '<While>', '<If>',\n",
    "        '<Return>', '<Delete>', '<Assert>',\n",
    "        '<Pass>', '<Break>', '<Continue>',\n",
    "        '<With>'\n",
    "    ],\n",
    "\n",
    "    # Control structures\n",
    "    '<For>': [\n",
    "        'For(target=<lhs_expr>, iter=<expr>, body=<nonempty_stmt_list>, orelse=<stmt_list><type_comment>)'\n",
    "    ],\n",
    "    '<stmt_list>': [ '[<stmts>?]' ],\n",
    "    '<nonempty_stmt_list>': [ '[<stmts>]' ],\n",
    "    '<stmts>': [ '<stmt>', '<stmt>, <stmts>' ],\n",
    "\n",
    "    '<While>': [\n",
    "        'While(test=<expr>, body=<nonempty_stmt_list>, orelse=<stmt_list>)'\n",
    "    ],\n",
    "\n",
    "    '<If>': [\n",
    "        'If(test=<expr>, body=<nonempty_stmt_list><orelse_param>)'\n",
    "    ],\n",
    "    '<orelse_param>': [\n",
    "        ', orelse=<stmt_list>'\n",
    "    ],\n",
    "\n",
    "    '<With>': [\n",
    "        'With(items=<withitem_list>, body=<nonempty_stmt_list><type_comment>?)'\n",
    "    ],\n",
    "    '<withitem_list>': [ '[<withitems>?]' ],\n",
    "    '<withitems>': [ '<withitem>', '<withitems>, <withitem>' ],\n",
    "    '<withitem>': [\n",
    "        'withitem(context_expr=<expr>)',\n",
    "        'withitem(context_expr=<expr>, optional_vars=<lhs_expr>)',\n",
    "    ],\n",
    "\n",
    "    # Other statements\n",
    "    '<Return>': [\n",
    "        'Return()',\n",
    "        'Return(value=<expr>)'\n",
    "    ],\n",
    "    '<Delete>': [\n",
    "        'Delete(targets=<expr_list>)'\n",
    "    ],\n",
    "    '<Assert>': [\n",
    "        'Assert(test=<expr>)',\n",
    "        'Assert(test=<expr>, msg=<expr>)'\n",
    "    ],\n",
    "    '<Pass>': [ 'Pass()'],\n",
    "    '<Break>': [ 'Break()' ],\n",
    "    '<Continue>': [ 'Continue()']\n",
    "\n",
    "    # FIXME: A few more: AsyncFor, AsyncWith, Match, Try, TryStar\n",
    "    # Import, ImportFrom, Global, Nonlocal...\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "145",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do import this unconditionally\n",
    "if sys.version_info >= (3, 13):\n",
    "    PYTHON_AST_STMTS_GRAMMAR: Grammar = \\\n",
    "        extend_grammar(PYTHON_AST_STMTS_GRAMMAR, {\n",
    "        # As of 3.13, orelse is optional\n",
    "        '<If>': [\n",
    "            'If(test=<expr>, body=<nonempty_stmt_list><orelse_param>?)'\n",
    "        ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "146",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_STMTS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "147",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in PYTHON_AST_STMTS_GRAMMAR['<stmt>']:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_STMTS_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "148",
   "metadata": {},
   "source": [
    "Let us see if we can also _parse_ code properly. Here is a sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149",
   "metadata": {},
   "outputs": [],
   "source": [
    "with_tree = ast.parse(\"\"\"\n",
    "with open('foo.txt') as myfile:\n",
    "    content = myfile.readlines()\n",
    "    if content is not None:\n",
    "        print(content)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_ast_stmts_grammar = convert_ebnf_grammar(PYTHON_AST_STMTS_GRAMMAR)\n",
    "with_tree_str = ast.dump(with_tree.body[0])  # get the `With(...)` subtree\n",
    "print(with_tree_str)\n",
    "with_solver = ISLaSolver(python_ast_stmts_grammar)\n",
    "assert with_solver.check(with_tree_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "151",
   "metadata": {},
   "source": [
    "It seems our grammar can also parse non-trivial code properly. We are doing well!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153",
   "metadata": {},
   "source": [
    "### Excursion: Function Definitions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "154",
   "metadata": {},
   "source": [
    "Now for function definitions.\n",
    "Not too many surprises here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.dump(ast.parse(\"\"\"\n",
    "def f(a, b=1):\n",
    "    pass\n",
    "\"\"\"\n",
    "), indent=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "156",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_DEFS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_STMTS_GRAMMAR, {\n",
    "    '<stmt>': PYTHON_AST_STMTS_GRAMMAR['<stmt>'] + [ '<FunctionDef>' ],\n",
    "\n",
    "    '<FunctionDef>': [\n",
    "        'FunctionDef(name=<identifier>, args=<arguments>, body=<nonempty_stmt_list><decorator_list_param><returns>?<type_comment>?)'\n",
    "    ],\n",
    "    '<decorator_list_param>': [\n",
    "        ', decorator_list=<expr_list>'\n",
    "    ],\n",
    "\n",
    "    '<arguments>': [\n",
    "        'arguments(<posonlyargs_param>args=<arg_list><vararg>?<kwonlyargs_param><kw_defaults_param><kwarg>?<defaults_param>)'\n",
    "    ],\n",
    "    '<posonlyargs_param>': [\n",
    "        'posonlyargs=<arg_list>, '\n",
    "    ],\n",
    "    '<kwonlyargs_param>': [\n",
    "        ', kwonlyargs=<arg_list>'\n",
    "    ],\n",
    "    '<kw_defaults_param>': [\n",
    "        ', kw_defaults=<expr_list>'\n",
    "    ],\n",
    "    '<defaults_param>': [\n",
    "        ', defaults=<expr_list>'\n",
    "    ],\n",
    "\n",
    "\n",
    "    '<arg_list>': [ '[<args>?]' ],\n",
    "    '<args>': [ '<arg>', '<arg>, <arg>' ],\n",
    "    '<arg>': [ 'arg(arg=<identifier>)' ],\n",
    "\n",
    "    '<vararg>': [ ', vararg=<arg>' ],\n",
    "    '<kwarg>': [ ', kwarg=<arg>' ],\n",
    "    '<returns>': [ ', returns=<expr>' ],\n",
    "\n",
    "    # FIXME: Not handled: AsyncFunctionDef, ClassDef\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157",
   "metadata": {},
   "source": [
    "In Python 3.12 and later, function definitions also have a `type_param` field:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do import this unconditionally\n",
    "if sys.version_info >= (3, 12):\n",
    "    PYTHON_AST_DEFS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_DEFS_GRAMMAR, {\n",
    "    '<FunctionDef>': [\n",
    "        'FunctionDef(name=<identifier>, args=<arguments>, body=<nonempty_stmt_list><decorator_list_param><returns>?<type_comment>?<type_params>?)'\n",
    "    ],\n",
    "    '<type_params>': [\n",
    "        ', type_params=<type_param_list>',\n",
    "    ],\n",
    "    '<type_param_list>': [ '[<type_param>?]' ],\n",
    "    '<type_param>': [ '<TypeVar>', '<ParamSpec>', '<TypeVarTuple>' ],\n",
    "    '<TypeVar>': [\n",
    "        'TypeVar(name=<identifier>(, bound=<expr>)?)'\n",
    "    ],\n",
    "    '<ParamSpec>': [\n",
    "        'ParamSpec(name=<identifier>)'\n",
    "    ],\n",
    "    '<TypeVarTuple>': [\n",
    "        'TypeVarTuple(name=<identifier>)'\n",
    "    ]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "159",
   "metadata": {},
   "source": [
    "In Python 3.13 and later, several `<FunctionDef>` and `<arguments>` attributes are optional:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do import this unconditionally\n",
    "if sys.version_info >= (3, 13):\n",
    "    PYTHON_AST_DEFS_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_DEFS_GRAMMAR, {\n",
    "    '<FunctionDef>': [\n",
    "        'FunctionDef(name=<identifier>, args=<arguments>, body=<nonempty_stmt_list><decorator_list_param>?<returns>?<type_comment>?<type_params>?)'\n",
    "    ],\n",
    "    '<arguments>': [\n",
    "        'arguments(<posonlyargs_param>?args=<arg_list><vararg>?<kwonlyargs_param>?<kw_defaults_param>?<kwarg>?<defaults_param>?)'\n",
    "    ],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_DEFS_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "162",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in [ '<arguments>', '<FunctionDef>' ]:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_DEFS_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "164",
   "metadata": {},
   "source": [
    "### Excursion: Modules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "165",
   "metadata": {},
   "source": [
    "We close with _modules_ – sequences of definitions.\n",
    "After all the other definitions, this is now fairly straightforward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_MODULE_GRAMMAR: Grammar = extend_grammar(PYTHON_AST_DEFS_GRAMMAR, {\n",
    "    '<start>': [ '<mod>' ],\n",
    "    '<mod>': [ '<Module>' ],\n",
    "    '<Module>': [ 'Module(body=<nonempty_stmt_list><type_ignore_param>)'],\n",
    "\n",
    "    '<type_ignore_param>': [ ', type_ignores=<type_ignore_list>' ],\n",
    "    '<type_ignore_list>': [ '[<type_ignores>?]' ],\n",
    "    '<type_ignores>': [ '<type_ignore>', '<type_ignore>, <type_ignore>' ],\n",
    "    '<type_ignore>': [ 'TypeIgnore(lineno=<integer>, tag=<string>)' ],\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do import this unconditionally\n",
    "if sys.version_info >= (3, 13):\n",
    "    PYTHON_AST_MODULE_GRAMMAR: Grammar = \\\n",
    "        extend_grammar(PYTHON_AST_MODULE_GRAMMAR, {\n",
    "        # As of 3.13, the type_ignore parameter is optional\n",
    "        '<Module>': [ 'Module(body=<nonempty_stmt_list><type_ignore_param>?)'],\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "168",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(PYTHON_AST_MODULE_GRAMMAR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "169",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in [ '<Module>' ]:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_MODULE_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "170",
   "metadata": {},
   "source": [
    "### End of Excursion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171",
   "metadata": {},
   "source": [
    "At this point, we have covered (almost) all AST elements of Python.\n",
    "There would be a few more Python elements to consider (marked as `FIXME`, above), but we'll leave these to the reader.\n",
    "Let us define `PYTHON_AST_GRAMMAR` as the official grammar coming out of this chapter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "172",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_GRAMMAR = PYTHON_AST_MODULE_GRAMMAR\n",
    "python_ast_grammar = convert_ebnf_grammar(PYTHON_AST_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "173",
   "metadata": {},
   "source": [
    "Here are a few (very weird) examples of Python functions we can produce.\n",
    "All of these are valid, but only _syntactically_ – very few of the code samples produced this way will actually result in something meaningful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "174",
   "metadata": {},
   "outputs": [],
   "source": [
    "for elt in [ '<FunctionDef>' ]:\n",
    "    print(elt)\n",
    "    test_samples(PYTHON_AST_GRAMMAR, start_symbol=elt)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175",
   "metadata": {},
   "source": [
    "## A Class for Fuzzing Python\n",
    "\n",
    "For convenience, let us introduce a class `PythonFuzzer` that makes use of the above grammar in order to produce Python code. This will be fairly easy to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PythonFuzzer(ISLaSolver):\n",
    "    \"\"\"Produce Python code.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 start_symbol: Optional[str] = None, *,\n",
    "                 grammar: Optional[Grammar] = None,\n",
    "                 constraint: Optional[str] =None,\n",
    "                 **kw_params) -> None:\n",
    "        \"\"\"Produce Python code. Parameters are:\n",
    "\n",
    "        * `start_symbol`: The grammatical entity to be generated (default: `<FunctionDef>`)\n",
    "        * `grammar`: The EBNF grammar to be used (default: `PYTHON__AST_GRAMMAR`); and\n",
    "        * `constraint` an ISLa constraint (if any).\n",
    "\n",
    "        Additional keyword parameters are passed to the `ISLaSolver` superclass.\n",
    "        \"\"\"\n",
    "        if start_symbol is None:\n",
    "            start_symbol = '<FunctionDef>'\n",
    "        if grammar is None:\n",
    "            grammar = PYTHON_AST_GRAMMAR\n",
    "        assert start_symbol in grammar\n",
    "\n",
    "        g = convert_ebnf_grammar(grammar)\n",
    "        if constraint is None:\n",
    "            super().__init__(g, start_symbol=start_symbol, **kw_params)\n",
    "        else:\n",
    "            super().__init__(g, constraint, start_symbol=start_symbol, **kw_params)\n",
    "\n",
    "    def fuzz(self) -> str:\n",
    "        \"\"\"Produce a Python code string.\"\"\"\n",
    "        abstract_syntax_tree = eval(str(self.solve()))\n",
    "        ast.fix_missing_locations(abstract_syntax_tree)\n",
    "        return ast.unparse(abstract_syntax_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "177",
   "metadata": {},
   "source": [
    "By default, the `PythonFuzzer` will produce a _function definition_ - that is, a function header and body."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer = PythonFuzzer()\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179",
   "metadata": {},
   "source": [
    "By passing a start symbol as parameter, you can have `PythonFuzzer` produce arbitrary Python elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer = PythonFuzzer('<While>')\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "181",
   "metadata": {},
   "source": [
    "Here is a list of all possible start symbols:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(PYTHON_AST_GRAMMAR.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183",
   "metadata": {},
   "source": [
    "## Customizing the Python Fuzzer\n",
    "\n",
    "When fuzzing, you may be interested in _specific_ properties of the produced output. How can we influence the code that `PythonFuzzer` produces? We explore two ways:\n",
    "\n",
    "* By adjusting the _grammar_ to our needs\n",
    "* By adding _constraints_ that customize the output for us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184",
   "metadata": {},
   "source": [
    "### Adjusting the Grammar\n",
    "\n",
    "A simple way to adjust output generation is to _adapt the grammar_.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185",
   "metadata": {},
   "source": [
    "Let us assume you'd like to have function definitions without decorators.\n",
    "To achieve this, you can _alter the rule that produces function definitions_:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186",
   "metadata": {},
   "outputs": [],
   "source": [
    "PYTHON_AST_GRAMMAR['<FunctionDef>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187",
   "metadata": {},
   "source": [
    "As any AST rule, it comes in _abstract syntax_, so we first have to identify the element we'd like to adjust.\n",
    "In our case, this is `decorator_list`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188",
   "metadata": {},
   "source": [
    "Since decorator_list is a list, we can alter the rule to produce empty lists only.\n",
    "To create a new adapted grammar, we do not alter the existing `PYTHON_AST_GRAMMAR`.\n",
    "Instead, we use the `extend_grammar()` function to create a new grammar with a new, adapted rule for `<FunctionDef>`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "189",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_ast_grammar_without_decorators: Grammar = extend_grammar(PYTHON_AST_GRAMMAR,\n",
    "{\n",
    "    '<FunctionDef>' :\n",
    "        ['FunctionDef(name=<identifier>, args=<arguments>, body=<nonempty_stmt_list>, decorator_list=[])']\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "190",
   "metadata": {},
   "source": [
    "However, we're not done yet.\n",
    "We also need to ensure that our grammar is _valid_, as any misspelled nonterminal identifier will result in problems during production.\n",
    "For this, we use the `is_valid_grammar()` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ExpectError import ExpectError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192",
   "metadata": {},
   "outputs": [],
   "source": [
    "with ExpectError():\n",
    "    assert is_valid_grammar(python_ast_grammar_without_decorators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "193",
   "metadata": {},
   "source": [
    "We see that with our change, our grammar has an _orphaned rule_: The `<returns>` rule is no longer used.\n",
    "This is because `<returns>` is part of the `<type_annotation>` we just have deleted.\n",
    "(`<type_annotation>` is still used when defining types for variables.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "194",
   "metadata": {},
   "source": [
    "To fix this, we need to delete the `<returns>` rule from our grammar.\n",
    "Fortunately, we have a function `trim_grammar()`, which deletes all orphaned rules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "195",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_ast_grammar_without_decorators = trim_grammar(python_ast_grammar_without_decorators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196",
   "metadata": {},
   "source": [
    "With this, our grammar becomes valid..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert is_valid_grammar(python_ast_grammar_without_decorators)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "198",
   "metadata": {},
   "source": [
    "... and we can use it for fuzzing - now without decorators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "199",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer = PythonFuzzer(grammar=python_ast_grammar_without_decorators)\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200",
   "metadata": {},
   "source": [
    "Adjusting the grammar is straightforward once you understood the grammar structure, but the AST grammar is complex; also, your changes and extensions tie you closely to the grammar structure.\n",
    "Carefully study how the individual rules are defined, above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201",
   "metadata": {},
   "source": [
    "### Using Constraints for Customizing\n",
    "\n",
    "A more elegant alternative to altering the grammar is to make use of _constraints_ that tune the grammar to your needs.\n",
    "Since `PythonFuzzer` is derived from `ISLaSolver`, we can pass a `constraint` argument constraining the grammar, as discussed in the chapter on [fuzzing with constraints](FuzzingWithConstraints.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "If we want to have a function definition with 10 characters in each identifier, we make use of an ISLa constraint:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer = PythonFuzzer(constraint='str.len(<id>) = 10')\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "204",
   "metadata": {},
   "source": [
    "We can also constrain individual children – say, the actual identifier of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Also works (the <identifier> has quotes)\n",
    "fuzzer = PythonFuzzer(constraint='<FunctionDef>.<identifier> = \"\\'my_favorite_function\\'\"')\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206",
   "metadata": {},
   "source": [
    "Assume we want to test how the compiler handles large numbers. Let us define a constraint such that the function body (`<nonempty_stmt_list>`) contains at least one integer (`<integer>`) with a value of at least 1000:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer = PythonFuzzer(constraint=\n",
    "\"\"\"\n",
    "    exists <integer> x:\n",
    "        (inside(x, <nonempty_stmt_list>) and str.to.int(x) > 1000)\n",
    "\"\"\")\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208",
   "metadata": {},
   "source": [
    "Assume we'd like to test compilers with non-trivial functions. Here's how to define a constraint such that the function body has exactly _three_ statements (`<stmt>`). Note that this can take more than a minute to resolve, but the result definitely is a nontrivial function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "209",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will not work with ISLa 2\n",
    "fuzzer = PythonFuzzer(constraint=\"\"\"\n",
    "    forall <FunctionDef> def: count(def, \"<stmt>\", \"3\")\n",
    "\"\"\")\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "210",
   "metadata": {},
   "source": [
    "And finally, if we want the decorator list to be empty, as in our grammar-altering example, we can constrain the decorator list to be empty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "211",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "# with ExpectError(mute=True):\n",
    "#     # Triggers an ISLa error (AssertionError)\n",
    "#     fuzzer = PythonFuzzer(constraint='''\n",
    "#         str.contains(<FunctionDef>, \"decorator_list=[]\")\n",
    "#     ''')\n",
    "#     print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "212",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "# with ExpectError(mute=True):\n",
    "#     # Triggers an ISLa error (AssertionError)\n",
    "#     fuzzer = PythonFuzzer(constraint='<FunctionDef>.<expr_list> = \"[]\"')\n",
    "#     print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "213",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer = PythonFuzzer(constraint='<FunctionDef>..<expr_list> = \"[]\"')\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "214",
   "metadata": {},
   "source": [
    "## Mutating Code\n",
    "\n",
    "When producing code for compilers (or actually, producing inputs in general), it is often a good idea to not just create _everything_ from scratch, but rather to _mutate_ existing inputs. This way, one can achieve a better balance between _common_ inputs (the ones to mutate) and _uncommon inputs_ (the new parts added via mutation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "215",
   "metadata": {},
   "source": [
    "### Parsing Inputs\n",
    "\n",
    "To _mutate_ inputs, we first need to be able to _parse_ them. This is where a grammar is really put to test - can it really parse all possible code? This is why relying on an _existing_ parser that is tried and proven (in our case the Python parser) and operating on an _abstraction_ (in our case the AST) is really handy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "216",
   "metadata": {},
   "source": [
    "We already have seen how to parse code into an AST, using `ast.parse()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sum(a, b):    # A simple example\n",
    "    the_sum = a + b\n",
    "    return the_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "218",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_source = inspect.getsource(sum)\n",
    "sum_tree = ast.parse(sum_source)\n",
    "print(ast.unparse(sum_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_str = ast.dump(sum_tree)\n",
    "sum_str"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "220",
   "metadata": {},
   "source": [
    "Our grammar is able to parse this (non_trivial) string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "221",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ISLaSolver(python_ast_grammar)\n",
    "assert solver.check(sum_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222",
   "metadata": {},
   "source": [
    "To mutate the input, we first have to parse it into a _derivation tree_ structure. This is (again) a tree representation of the code, but this time, using the elements of _our_ grammar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "223",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_tree = solver.parse(sum_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "224",
   "metadata": {},
   "source": [
    "Let us inspect what a derivation tree looks like. Alas, the string representation is very long and not that useful:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "225",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(repr(sum_tree))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226",
   "metadata": {},
   "outputs": [],
   "source": [
    "repr(sum_tree)[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "227",
   "metadata": {},
   "source": [
    "However, we can _visualize_ the derivation tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "228",
   "metadata": {},
   "outputs": [],
   "source": [
    "from GrammarFuzzer import display_tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "229",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "display_tree(sum_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230",
   "metadata": {},
   "source": [
    "We see that a derivation tree consists of _nonterminal_ nodes whose children make up an _expansion_ from the grammar.\n",
    "For instance, at the very top, we see that a `<start>` nonterminal expands into a `<mod>` nonterminal, which again expands into a `<Module>` nonterminal.\n",
    "This comes right from the grammar rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_ast_grammar['<start>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "232",
   "metadata": {},
   "source": [
    "and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "233",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_ast_grammar['<mod>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234",
   "metadata": {},
   "source": [
    "The child of `<mod>` is a `<Module>`, which expands into the nodes\n",
    "\n",
    "* `(body=`\n",
    "* `<nonempty_stmt_list>`\n",
    "* `, type_ignores=`\n",
    "* `<type_ignore_list>`\n",
    "* `)`\n",
    "\n",
    "Here, nodes like `(body=` or `, type_ignores=` are called _terminal_ nodes (because they have no more elements to expand).\n",
    "The nonterminals like `<nonempty_stmt_list>` get expanded further below – notably, `<nonempty_stmt_list>` expands into a `<FunctionDef>` node that represents the `sum()` definition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "235",
   "metadata": {},
   "source": [
    "Again, the structure exactly follows the `<Module>` definition in our grammar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_ast_grammar['<Module>']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237",
   "metadata": {},
   "source": [
    "If we traverse the tree depth-first, left to right, and only collect the terminal symbols, we obtain the original string we parsed.\n",
    "Applying the `str()` function to the derivation tree gets us exactly that string:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "238",
   "metadata": {},
   "outputs": [],
   "source": [
    "str(sum_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "239",
   "metadata": {},
   "source": [
    "And again, we can convert this string into an AST and thus obtain our original function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_ast = ast.fix_missing_locations(eval(str(sum_tree)))\n",
    "print(ast.unparse(sum_ast))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "241",
   "metadata": {},
   "source": [
    "### Mutating Inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "242",
   "metadata": {},
   "source": [
    "With derivation trees, we can have a _structured_ representation of our input. In our case, we already have that with ASTs, so why bother introducing a new one? The answer is simple: Derivation trees also allow us to _synthesize_ new inputs, because we have a _grammar_ that describes their structure."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "243",
   "metadata": {},
   "source": [
    "Most notably, we can mutate inputs as follows:\n",
    "\n",
    "1. Parse the input into a derivation tree, as shown above.\n",
    "2. Randomly choose some node `<symbol>` in the derivation tree to be mutated.\n",
    "3. Use the grammar to produce a new expansion for `<symbol>`.\n",
    "4. Replace the children of `<symbol>` by the expansion just generated.\n",
    "5. Repeat the process as often as needed.\n",
    "\n",
    "This is a decent programming task, and if you'd like a blueprint, have a look at the `FragmentMutator` in this tutorial on [greybox fuzzing with grammars](https://www.fuzzingbook.org/html/GreyboxGrammarFuzzer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244",
   "metadata": {},
   "source": [
    "Fortunately, ISLa already provides us with functionality that does exactly this.\n",
    "The `ISLaSolver.mutate()` method takes an input and mutates it according to the rules in the grammar.\n",
    "The input to mutate can be given as a derivation tree, or as a string; its output is a derivation tree (which can again be converted into a string)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "245",
   "metadata": {},
   "source": [
    "Let us apply `mutate()` on our `sum()` function. The `min_mutations` and `max_mutations` parameters define how many mutation steps should be performed; we set both to 1 in order to have exactly one mutation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "246",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mutated_tree = solver.mutate(sum_str, min_mutations=1, max_mutations=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mutated_ast = ast.fix_missing_locations(eval(str(sum_mutated_tree)))\n",
    "print(ast.unparse(sum_mutated_ast))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "248",
   "metadata": {},
   "source": [
    "Toy with the above to see the effect of a mutation.\n",
    "Note if one of the top-level nodes (like `<FunctionDef>` or `<Module>`) is selected for mutation, then `sum()` will be replaced by something entirely different. Otherwise, though, the code will still be pretty similar to the original `sum()` code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249",
   "metadata": {},
   "source": [
    "Of course, the more we increase the number of mutations, the more different the code will look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "250",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mutated_tree = solver.mutate(sum_str, min_mutations=10, max_mutations=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_mutated_ast = ast.fix_missing_locations(eval(str(sum_mutated_tree)))\n",
    "print(ast.unparse(sum_mutated_ast))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "252",
   "metadata": {},
   "source": [
    "By toying with the `mutate()` parameters, we can control how _common_ and how _uncommon_ our input should be."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253",
   "metadata": {},
   "source": [
    "### How Effective is Mutation?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254",
   "metadata": {},
   "source": [
    "Does mutating existing code help us in finding bugs?\n",
    "Let us assume we have a buggy compiler that generates bad code for an expression of the form `<elem> * (<elem> + <elem>)`.\n",
    "The code in `has_distributive_law()` checks an AST for the presence of this bug:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_distributive_law(tree) -> bool:\n",
    "    for node in walk(tree):  # iterate over all nodes in `tree`\n",
    "        # print(node)\n",
    "        if isinstance(node, ast.BinOp):\n",
    "            if isinstance(node.op, ast.Mult):\n",
    "                if isinstance(node.right, ast.BinOp):\n",
    "                    if isinstance(node.right.op, ast.Add):\n",
    "                        return True\n",
    "\n",
    "                if isinstance(node.left, ast.BinOp):\n",
    "                    if isinstance(node.left.op, ast.Add):\n",
    "                        return True\n",
    "\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256",
   "metadata": {},
   "source": [
    "To understand how this works, a visualization of the AST comes in handy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "257",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_ast(ast.parse(\"1 + (2 * 3)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "258",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_distributive_law(ast.parse(\"1 * (2 + 3)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_distributive_law(ast.parse(\"(1 + 2) * 3\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "260",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_distributive_law(ast.parse(\"1 + (2 * 3)\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "261",
   "metadata": {},
   "outputs": [],
   "source": [
    "has_distributive_law(ast.parse(\"def f(a, b):\\n    return a * (b + 10)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262",
   "metadata": {},
   "source": [
    "How many attempts does it take for each until we find a mutation that triggers the bug in `has_distributive_law()`? \n",
    "Let us write a function that computes this number."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "263",
   "metadata": {},
   "outputs": [],
   "source": [
    "def how_many_mutations(code: str) -> int:\n",
    "    solver = ISLaSolver(python_ast_grammar)\n",
    "\n",
    "    code_ast = ast.parse(code)\n",
    "    code_ast = ast.fix_missing_locations(code_ast)\n",
    "    code_ast_str = ast.dump(code_ast)\n",
    "    code_derivation_tree = solver.parse(code_ast_str)\n",
    "    mutations = 0\n",
    "    mutated_code_ast = code_ast\n",
    "\n",
    "    while not has_distributive_law(mutated_code_ast):\n",
    "        mutations += 1\n",
    "        if mutations % 100 == 0:\n",
    "            print(f'{mutations}...', end='')\n",
    "\n",
    "        mutated_code_str = str(solver.mutate(code_derivation_tree))\n",
    "        mutated_code_ast = eval(mutated_code_str)\n",
    "        # mutated_code_ast = ast.fix_missing_locations(mutated_code_ast)\n",
    "        # print(ast.dump(mutated_code_ast))\n",
    "        # print(ast.unparse(mutated_code_ast))\n",
    "\n",
    "    return mutations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "264",
   "metadata": {},
   "source": [
    "If we pass an input that already exhibits the bug, we do not need any mutation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert how_many_mutations('1 * (2 + 3)') == 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266",
   "metadata": {},
   "source": [
    "However, the further we are away from the bug, the more mutations (and the more time) it takes to find it.\n",
    "Notably, mutating `2 + 2` until we have a distributive law still is much faster than mutating `2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_mutations('2 + 2')    # <-- Note: this can take a minute"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268",
   "metadata": {},
   "outputs": [],
   "source": [
    "how_many_mutations('2')  # <-- Note: this can take several minutes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "269",
   "metadata": {},
   "source": [
    "We conclude that mutating existing code can indeed be helpful, especially if it is syntactically _close to inputs that trigger bugs_.\n",
    "If you want to have a good chance in finding bugs, focus on _inputs that have triggered bugs before_ – sometimes a simple mutation of these already helps finding a new bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270",
   "metadata": {},
   "source": [
    "## Evolutionary Fuzzing\n",
    "\n",
    "One interesting application of mutating inputs is to use mutations for _evolutionary fuzzing_.\n",
    "The idea is to have a population of inputs, to apply _mutations_ on them, and to check whether they improve on a particular goal (mostly code coverage).\n",
    "Those inputs that _do_ improve are being retained (\"survival of the fittest\") as the next generation, and evolved further.\n",
    "By repeating this process often enough, we may obtain inputs that cover large parts of code and thus improve chances to uncover bugs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271",
   "metadata": {},
   "source": [
    "Let us assume we have a buggy compiler that generates bad code for an expression of the form `<elem> * (<elem> + <elem>)`.\n",
    "The function `has_distributive_law()`, above, checks an AST for the presence of this bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272",
   "metadata": {},
   "source": [
    "Our aim is to detect this bug via fuzzing. But if we simply generate random inputs from scratch, it may take a long time until we generate the exact copmbination of operators that triggers the bug."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273",
   "metadata": {},
   "source": [
    "### Getting Coverage\n",
    "\n",
    "To have our fuzzers guided by coverage, we first need to _measure_ code coverage.\n",
    "We make use of the [Coverage module from the Fuzzing Book](https://www.fuzzingbook.org/html/Coverage.html), which is particularly easy to use.\n",
    "It simply uses a `with` clause to obtain coverage from the code in the `with` body.\n",
    "Here is how to obtain coverage for our `has_distributive_law()` code, above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Coverage import Coverage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "275",
   "metadata": {},
   "outputs": [],
   "source": [
    "mult_ast = ast.parse(\"1 * 2\")\n",
    "with Coverage() as cov:\n",
    "    has_distributive_law(mult_ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276",
   "metadata": {},
   "source": [
    "The `coverage()` method tells us which lines in the code actually have been reached.\n",
    "This includes lines from `has_distributive_law()`, but also lines from other functions called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "277",
   "metadata": {},
   "outputs": [],
   "source": [
    "cov.coverage()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "278",
   "metadata": {},
   "source": [
    "Which are the lines executed? \n",
    "With a bit of code inspection, we can easily visualize the covered lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_coverage(cov, fun):\n",
    "    fun_lines, fun_start = inspect.getsourcelines(fun)\n",
    "    fun_name = fun.__name__\n",
    "    coverage = cov.coverage()\n",
    "    for line in range(len(fun_lines)):\n",
    "        if (fun_name, line + fun_start) in coverage:\n",
    "            print('# ', end='')  # covered lines\n",
    "        else:\n",
    "            print('  ', end='')  # uncovered lines\n",
    "        print(line + fun_start, fun_lines[line], end='')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_coverage(cov, has_distributive_law)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "281",
   "metadata": {},
   "source": [
    "In this listing, a `#` indicates that the code has been executed (covered).\n",
    "We see that our input \"1 * 2\" satisfies the conditions in Lines 4 and 5, but does not satisfy the conditions in later lines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282",
   "metadata": {},
   "source": [
    "### Fitness\n",
    "\n",
    "Let us now use coverage as a _fitness function_ to guide evolution.\n",
    "The higher the fitness (the coverage), the higher the chances of an input to be retained for further evolution.\n",
    "Our `ast_fitness()` function simply counts the number of lines covered in `has_distributive_law()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "283",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ast_fitness(code_ast) -> int:\n",
    "    with Coverage() as cov:\n",
    "        has_distributive_law(code_ast)\n",
    "    lines = set()\n",
    "    for (name, line) in cov.coverage():\n",
    "        if name == has_distributive_law.__name__:\n",
    "            lines.add(line)\n",
    "    return len(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284",
   "metadata": {},
   "source": [
    "Here is the fitness of a number of given inputs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "285",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_fitness(ast.parse(\"1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "286",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_fitness(ast.parse(\"1 + 1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "287",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_fitness(ast.parse(\"1 * 2\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_fitness(ast.parse(\"1 * (2 + 3)\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "289",
   "metadata": {},
   "source": [
    "Now, let's set up a fitness function that takes derivation trees.\n",
    "Essentially, our `tree_fitness()` function is based on the `ast_fitness()` function, above;\n",
    "however, we also add a small component `1 / len(code_str)` to give extra fitness to shorter inputs.\n",
    "Otherwise, our inputs may grow and keep on growing, making mutations inefficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tree_fitness(tree) -> float:\n",
    "    code_str = str(tree)\n",
    "    code_ast = ast.fix_missing_locations(eval(code_str))\n",
    "    fitness = ast_fitness(code_ast)\n",
    "    # print(ast.unparse(code_ast), f\"\\n=> Fitness = {fitness}\\n\")\n",
    "    return fitness + 1 / len(code_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291",
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_fitness(sum_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "292",
   "metadata": {},
   "source": [
    "### Evolving Inputs\n",
    "\n",
    "Let us now make use of our fitness function to implement a simple evolutionary fuzzing algorithm.\n",
    "We start with _evolution_ – that is, taking a population and adding offspring via mutations.\n",
    "Our initial population consists of a single candidate – in our case, `sum_tree` reflecting the `sum()` function, above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initial_population(tree):\n",
    "    return [ (tree, tree_fitness(tree)) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_population = initial_population(sum_tree)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "295",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(sum_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "296",
   "metadata": {},
   "source": [
    "Our `evolve()` function adds two new children to each population member."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297",
   "metadata": {},
   "outputs": [],
   "source": [
    "OFFSPRING = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "298",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evolve(population, min_fitness=-1):\n",
    "    solver = ISLaSolver(python_ast_grammar)\n",
    "\n",
    "    for (candidate, _) in list(population):\n",
    "        for i in range(OFFSPRING):\n",
    "            child = solver.mutate(candidate, min_mutations=1, max_mutations=1)\n",
    "            child_fitness = tree_fitness(child)\n",
    "            if child_fitness > min_fitness:\n",
    "                population.append((child, child_fitness))\n",
    "    return population"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "299",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_population = evolve(sum_population)\n",
    "len(sum_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "300",
   "metadata": {},
   "source": [
    "As we can evolve all these, too, we get an exponential growth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_population = evolve(sum_population)\n",
    "len(sum_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "302",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_population = evolve(sum_population)\n",
    "len(sum_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "303",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_population = evolve(sum_population)\n",
    "len(sum_population)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "304",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_population = evolve(sum_population)\n",
    "len(sum_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305",
   "metadata": {},
   "source": [
    "### Survival of the Fittest\n",
    "\n",
    "No population can expand forever and still survive.\n",
    "Let us thus limit the population to a certain size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306",
   "metadata": {},
   "outputs": [],
   "source": [
    "POPULATION_SIZE = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "307",
   "metadata": {},
   "source": [
    "The `select()` function implements survival of the fittest: It limits the population to at most `POPULATION_SIZE` elements, sorting them by their fitness (highest to lowest).\n",
    "Members with low fitness beyond `POPULATION_SIZE` do not survive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_fitness(elem):\n",
    "    (candidate, fitness) = elem\n",
    "    return fitness\n",
    "\n",
    "def select(population):\n",
    "    population = sorted(population, key=get_fitness, reverse=True)\n",
    "    population = population[:POPULATION_SIZE]\n",
    "    return population"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "309",
   "metadata": {},
   "source": [
    "We can use the following call to trim our `sum_population` to the fittest members:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310",
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_population = select(sum_population)\n",
    "len(sum_population)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "311",
   "metadata": {},
   "source": [
    "### Evolution\n",
    "\n",
    "We now have everything in place:\n",
    "\n",
    "* We have a _population_ (say, `sum_population`)\n",
    "* We can evolve the population (using `evolve()`)\n",
    "* We can have only the fittest survive (using `select()`)\n",
    "\n",
    "Let us repeat this process over several generations.\n",
    "We track whenever we have found a new \"best\" candidate and log them.\n",
    "If we find a candidate that triggers the bug, we stop.\n",
    "Note that this may take a long time, and not necessarily yield a perfect result."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "312",
   "metadata": {},
   "source": [
    "As common in search-based approaches, we stop and restart the search if we have not found a sufficient solution after a number of generations (here: `GENERATIONS`).\n",
    "Other than that, we keep searching until we have a solution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "313",
   "metadata": {},
   "outputs": [],
   "source": [
    "GENERATIONS = 100  # Upper bound"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314",
   "metadata": {},
   "outputs": [],
   "source": [
    "trial = 1\n",
    "found = False\n",
    "\n",
    "while not found:\n",
    "    sum_population = initial_population(sum_tree)\n",
    "    prev_best_fitness = -1\n",
    "\n",
    "    for generation in range(GENERATIONS):\n",
    "        sum_population = evolve(sum_population, min_fitness=prev_best_fitness)\n",
    "        sum_population = select(sum_population)\n",
    "        best_candidate, best_fitness = sum_population[0]\n",
    "        if best_fitness > prev_best_fitness:\n",
    "            print(f\"Generation {generation}: found new best candidate (fitness={best_fitness}):\")\n",
    "            best_ast = ast.fix_missing_locations(eval(str(best_candidate)))\n",
    "            print(ast.unparse(best_ast))\n",
    "            prev_best_fitness = best_fitness\n",
    "\n",
    "            if has_distributive_law(best_ast):\n",
    "                print(\"Done!\")\n",
    "                found = True\n",
    "                break\n",
    "\n",
    "    trial = trial + 1\n",
    "    print(f\"\\n\\nRestarting; trial #{trial}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "315",
   "metadata": {},
   "source": [
    "Success! We found a piece of code that triggers the bug. Check for occurrences of the distributive law."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ast.unparse(best_ast))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert has_distributive_law(best_ast)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318",
   "metadata": {},
   "source": [
    "\n",
    "You may note that not all of the code is required to trigger the bug.\n",
    "We could run our evolutionary fuzzer a bit longer to see whether it can be further reduced,\n",
    "or use a dedicated input reduction technique such as [Delta Debugging](https://www.fuzzingbook.org/html/Reducer.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "319",
   "metadata": {},
   "source": [
    "### Chances of Evolutionary Fuzzing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320",
   "metadata": {},
   "source": [
    "Could the bug in `distributive_law()` have been found without evolutionary guidance - i.e., simply by applying one mutation to `sum()`?\n",
    "\n",
    "When producing an expression (`<expr>`), we calculate how big the chances are to\n",
    "\n",
    "* Produce a binary operator, and\n",
    "* Produce a `*`, and\n",
    "* Produce another binary operator as one child, and\n",
    "* Produce a `+`\n",
    "\n",
    "Let's do a few queries on our grammar to compute the chances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert '<BinOp>' in python_ast_grammar['<expr>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(python_ast_grammar['<expr>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "323",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert 'Add()' in python_ast_grammar['<operator>']\n",
    "assert 'Mult()' in python_ast_grammar['<operator>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(python_ast_grammar['<operator>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "325",
   "metadata": {},
   "outputs": [],
   "source": [
    "(len(python_ast_grammar['<expr>'])       # chances of choosing a `BinOp`\n",
    "* len(python_ast_grammar['<operator>'])  # chances of choosing a `*`\n",
    "* len(python_ast_grammar['<expr>'])      # chances of choosing a `BinOp` as a child\n",
    "* len(python_ast_grammar['<operator>'])  # chances of choosing a `+`\n",
    "/ 2)   # two chances - one for the left child, one for the right"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "326",
   "metadata": {},
   "source": [
    "On average, it would take about 19000 (non-evolutionary) runs until we have an expression that triggers the distributive law.\n",
    "So it is definitely better to make use of additional information (say, coverage) in order to guide mutations towards a goal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "327",
   "metadata": {},
   "source": [
    "## Synopsis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328",
   "metadata": {},
   "source": [
    "This chapter provides a `PythonFuzzer` class that allows producing arbitrary Python code elements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer = PythonFuzzer()\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330",
   "metadata": {},
   "source": [
    "By default, `PythonFuzzer` produces a _function definition_ – that is, a list of statements as above.\n",
    "You can pass a `start_symbol` argument to state which Python element you'd like to have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331",
   "metadata": {},
   "outputs": [],
   "source": [
    "fuzzer = PythonFuzzer('<While>')\n",
    "print(fuzzer.fuzz())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332",
   "metadata": {},
   "source": [
    "Here is a list of all possible start symbols. Their names reflect the nonterminals from the [Python `ast` module documentation](https://docs.python.org/3/library/ast.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted(list(PYTHON_AST_GRAMMAR.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334",
   "metadata": {},
   "source": [
    "If you'd like more control over Python code generation, here is what is happening behind the scenes.\n",
    "The EBNF grammar `PYTHON_AST_GRAMMAR` can parse and produce _abstract syntax trees_ for Python.\n",
    "To produce a Python module without `PythonFuzzer`, you would take these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335",
   "metadata": {},
   "source": [
    "**Step 1:** Create a non-EBNF grammar suitable for `ISLaSolver` (or any other grammar fuzzer):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "336",
   "metadata": {},
   "outputs": [],
   "source": [
    "python_ast_grammar = convert_ebnf_grammar(PYTHON_AST_GRAMMAR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "337",
   "metadata": {},
   "source": [
    "**Step 2:**  Feed the resulting grammar into a grammar fuzzer such as ISLa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "338",
   "metadata": {},
   "outputs": [],
   "source": [
    "solver = ISLaSolver(python_ast_grammar, start_symbol='<FunctionDef>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339",
   "metadata": {},
   "source": [
    "**Step 3:**  Have the grammar fuzzer produce a string. This string represents an AST."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast_string = str(solver.solve())\n",
    "ast_string"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341",
   "metadata": {},
   "source": [
    "**Step 4:**  Convert the AST into an actual Python AST data structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ast import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343",
   "metadata": {},
   "outputs": [],
   "source": [
    "abstract_syntax_tree = eval(ast_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344",
   "metadata": {},
   "source": [
    "**Step 5:** Finally, convert the AST structure back into readable Python code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "345",
   "metadata": {},
   "outputs": [],
   "source": [
    "ast.fix_missing_locations(abstract_syntax_tree)\n",
    "print(ast.unparse(abstract_syntax_tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "346",
   "metadata": {},
   "source": [
    "The chapter has many more applications, including parsing and mutating Python code, evolutionary fuzzing, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "347",
   "metadata": {},
   "source": [
    "Here are the details on the `PythonFuzzer` constructor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "import inspect\n",
    "import markdown\n",
    "from bookutils import HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "sig = inspect.signature(PythonFuzzer.__init__)\n",
    "sig_str = str(sig) if sig else \"\"\n",
    "doc = inspect.getdoc(PythonFuzzer.__init__) or \"\"\n",
    "HTML(markdown.markdown('`PythonFuzzer' + sig_str + '`\\n\\n' + doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "from ClassDiagram import display_class_hierarchy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "351",
   "metadata": {
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "# ignore\n",
    "display_class_hierarchy([PythonFuzzer],\n",
    "                        public_methods=[\n",
    "                            PythonFuzzer.__init__,\n",
    "                            PythonFuzzer.fuzz,\n",
    "                            ISLaSolver.__init__\n",
    "                        ],\n",
    "                        project='fuzzingbook')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352",
   "metadata": {},
   "source": [
    "## Lessons Learned\n",
    "\n",
    "* When creating and processing complex inputs such as program code,\n",
    "  * try to rely on existing infrastructure to _parse_ inputs into some _abstract syntax_, and then\n",
    "  * have your grammars _process that abstract syntax_ rather than the concrete syntax.\n",
    "* Specifically, program code is normally converted into _abstract syntax trees_ before being compiled or interpreted, and you can (and should) make use of such conversions.\n",
    "* Once program code is turned into an AST, it is fairly easy to generate, mutate, and evolve despite its complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "353",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "The seminal work on compiler testing is _Csmith_ \\cite{Yang2011}, a generator of C programs.\n",
    "Csmith has been used to thoroughly test compilers such as Clang or GCC; beyond producing code that is syntactically correct, it also aims at _semantic_ correctness as well as avoiding undefined and unspecified behaviors.\n",
    "This is a must read for anyone in the field in compiler testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
